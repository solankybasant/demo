{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solankybasant/demo/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqf24JewXnU9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from IPython.display import Image, display\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import seaborn as sn\n",
        "\n",
        "mpl.rcParams[\"figure.figsize\"] = (18,6)\n",
        "mpl.rcParams[\"axes.grid\"] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRwrxAbvXnU_"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/Sentiment_Analysis_Combined.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CCgjBc79XnVA",
        "outputId": "f04041b8-1c78-44f1-ad6e-a2872ff1bc85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date     Adj Close  Label          Open          High           Low  \\\n",
              "0  2008-08-08  11734.320312      1  11432.089844  11759.959961  11388.040039   \n",
              "1  2008-08-11  11782.349609      0  11729.669922  11867.110352  11675.530273   \n",
              "2  2008-08-12  11642.469727      0  11781.700195  11782.349609  11601.519531   \n",
              "3  2008-08-13  11532.959961      1  11632.809570  11633.780273  11453.339844   \n",
              "4  2008-08-14  11615.929688      1  11532.070312  11718.280273  11450.889648   \n",
              "\n",
              "      Volume  Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0  212830000      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1  183190000      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2  173590000      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3  182550000      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4  159790000      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccb1a9f3-6f49-41a5-945c-ca14cfcf1039\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-08-12</td>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-08-13</td>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-08-14</td>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccb1a9f3-6f49-41a5-945c-ca14cfcf1039')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ccb1a9f3-6f49-41a5-945c-ca14cfcf1039 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ccb1a9f3-6f49-41a5-945c-ca14cfcf1039');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hHyfP5j4XnVA",
        "outputId": "7bc8c94c-8ecd-447d-f1a4-6dc5d4bb6035"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date     Adj Close  Label          Open          High           Low  \\\n",
              "0 2008-08-08  11734.320312      1  11432.089844  11759.959961  11388.040039   \n",
              "1 2008-08-11  11782.349609      0  11729.669922  11867.110352  11675.530273   \n",
              "2 2008-08-12  11642.469727      0  11781.700195  11782.349609  11601.519531   \n",
              "3 2008-08-13  11532.959961      1  11632.809570  11633.780273  11453.339844   \n",
              "4 2008-08-14  11615.929688      1  11532.070312  11718.280273  11450.889648   \n",
              "\n",
              "      Volume  Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0  212830000      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1  183190000      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2  173590000      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3  182550000      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4  159790000      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5fbfcf9e-b752-4d36-a696-692385be4e41\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-08-12</td>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-08-13</td>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-08-14</td>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5fbfcf9e-b752-4d36-a696-692385be4e41')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5fbfcf9e-b752-4d36-a696-692385be4e41 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5fbfcf9e-b752-4d36-a696-692385be4e41');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# df = data.drop(columns = [\"Label\"])\n",
        "df = data\n",
        "df.Date = pd.to_datetime(df.Date, infer_datetime_format=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcsUfCVkXnVB",
        "outputId": "1217f763-33db-4f2d-dbec-119dc6d2f9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1989 entries, 0 to 1988\n",
            "Data columns (total 13 columns):\n",
            " #   Column        Non-Null Count  Dtype         \n",
            "---  ------        --------------  -----         \n",
            " 0   Date          1989 non-null   datetime64[ns]\n",
            " 1   Adj Close     1989 non-null   float64       \n",
            " 2   Label         1989 non-null   int64         \n",
            " 3   Open          1989 non-null   float64       \n",
            " 4   High          1989 non-null   float64       \n",
            " 5   Low           1989 non-null   float64       \n",
            " 6   Volume        1989 non-null   int64         \n",
            " 7   Subjectivity  1989 non-null   float64       \n",
            " 8   Polarity      1989 non-null   float64       \n",
            " 9   compound      1989 non-null   float64       \n",
            " 10  neg           1989 non-null   float64       \n",
            " 11  pos           1989 non-null   float64       \n",
            " 12  neu           1989 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(10), int64(2)\n",
            "memory usage: 202.1 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "YwZ-q6UjXnVB",
        "outputId": "53dd543c-30aa-4486-c004-e82cf406b617"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f70434fe390>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df.set_index('Date')[\"Adj Close\"].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "2f-0JzVUXnVC",
        "outputId": "57a8a613-851a-444b-8273-660fe32ffa20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f7043395c90>,\n",
              "       <matplotlib.axes._subplots.AxesSubplot object at 0x7f7042f1b950>,\n",
              "       <matplotlib.axes._subplots.AxesSubplot object at 0x7f7043568e90>],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df.set_index('Date')[[\"Open\", \"High\", \"Low\"]].plot(subplots=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "i0gCcPwEXnVC",
        "outputId": "7b4dc0bf-ba8b-4fad-82d3-083ea543cf61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Adj Close  Label          Open          High           Low     Volume  \\\n",
              "0  11734.320312      1  11432.089844  11759.959961  11388.040039  212830000   \n",
              "1  11782.349609      0  11729.669922  11867.110352  11675.530273  183190000   \n",
              "2  11642.469727      0  11781.700195  11782.349609  11601.519531  173590000   \n",
              "3  11532.959961      1  11632.809570  11633.780273  11453.339844  182550000   \n",
              "4  11615.929688      1  11532.070312  11718.280273  11450.889648  159790000   \n",
              "\n",
              "   Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-392d58cf-af39-4ed2-8acb-ac66183f0c9e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-392d58cf-af39-4ed2-8acb-ac66183f0c9e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-392d58cf-af39-4ed2-8acb-ac66183f0c9e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-392d58cf-af39-4ed2-8acb-ac66183f0c9e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_input = df.drop(columns=[\"Date\"])\n",
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "U2EVdO5yXnVD",
        "outputId": "46d443bb-aee1-440d-f841-4b9968de5ac3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Adj Close        Label          Open          High           Low  \\\n",
              "count   1989.000000  1989.000000   1989.000000   1989.000000   1989.000000   \n",
              "mean   13463.032255     0.535445  13459.116048  13541.303173  13372.931728   \n",
              "std     3144.006996     0.498867   3143.281634   3136.271725   3150.420934   \n",
              "min     6547.049805     0.000000   6547.009766   6709.609863   6469.950195   \n",
              "25%    10913.379883     0.000000  10907.339844  11000.980469  10824.759766   \n",
              "50%    13025.580078     1.000000  13022.049805  13088.110352  12953.129883   \n",
              "75%    16478.410156     1.000000  16477.699219  16550.070312  16392.769531   \n",
              "max    18312.390625     1.000000  18315.060547  18351.359375  18272.560547   \n",
              "\n",
              "             Volume  Subjectivity     Polarity     compound          neg  \\\n",
              "count  1.989000e+03   1989.000000  1989.000000  1989.000000  1989.000000   \n",
              "mean   1.628110e+08      0.361426     0.022722    -0.957369     0.162315   \n",
              "std    9.392343e+07      0.060884     0.053687     0.199673     0.038575   \n",
              "min    8.410000e+06      0.161332    -0.225978    -0.999500     0.059000   \n",
              "25%    1.000000e+08      0.321410    -0.011461    -0.996400     0.135000   \n",
              "50%    1.351700e+08      0.361652     0.024870    -0.993200     0.159000   \n",
              "75%    1.926000e+08      0.400533     0.057980    -0.985500     0.188000   \n",
              "max    6.749200e+08      0.615242     0.195774     0.991700     0.316000   \n",
              "\n",
              "               pos          neu  \n",
              "count  1989.000000  1989.000000  \n",
              "mean      0.065675     0.772018  \n",
              "std       0.020968     0.041819  \n",
              "min       0.007000     0.588000  \n",
              "25%       0.051000     0.746000  \n",
              "50%       0.064000     0.773000  \n",
              "75%       0.079000     0.802000  \n",
              "max       0.153000     0.894000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dd54c01-35a9-4505-a52f-9d34885b10e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1.989000e+03</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13463.032255</td>\n",
              "      <td>0.535445</td>\n",
              "      <td>13459.116048</td>\n",
              "      <td>13541.303173</td>\n",
              "      <td>13372.931728</td>\n",
              "      <td>1.628110e+08</td>\n",
              "      <td>0.361426</td>\n",
              "      <td>0.022722</td>\n",
              "      <td>-0.957369</td>\n",
              "      <td>0.162315</td>\n",
              "      <td>0.065675</td>\n",
              "      <td>0.772018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3144.006996</td>\n",
              "      <td>0.498867</td>\n",
              "      <td>3143.281634</td>\n",
              "      <td>3136.271725</td>\n",
              "      <td>3150.420934</td>\n",
              "      <td>9.392343e+07</td>\n",
              "      <td>0.060884</td>\n",
              "      <td>0.053687</td>\n",
              "      <td>0.199673</td>\n",
              "      <td>0.038575</td>\n",
              "      <td>0.020968</td>\n",
              "      <td>0.041819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6547.049805</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6547.009766</td>\n",
              "      <td>6709.609863</td>\n",
              "      <td>6469.950195</td>\n",
              "      <td>8.410000e+06</td>\n",
              "      <td>0.161332</td>\n",
              "      <td>-0.225978</td>\n",
              "      <td>-0.999500</td>\n",
              "      <td>0.059000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>10913.379883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10907.339844</td>\n",
              "      <td>11000.980469</td>\n",
              "      <td>10824.759766</td>\n",
              "      <td>1.000000e+08</td>\n",
              "      <td>0.321410</td>\n",
              "      <td>-0.011461</td>\n",
              "      <td>-0.996400</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.051000</td>\n",
              "      <td>0.746000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13025.580078</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13022.049805</td>\n",
              "      <td>13088.110352</td>\n",
              "      <td>12953.129883</td>\n",
              "      <td>1.351700e+08</td>\n",
              "      <td>0.361652</td>\n",
              "      <td>0.024870</td>\n",
              "      <td>-0.993200</td>\n",
              "      <td>0.159000</td>\n",
              "      <td>0.064000</td>\n",
              "      <td>0.773000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>16478.410156</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16477.699219</td>\n",
              "      <td>16550.070312</td>\n",
              "      <td>16392.769531</td>\n",
              "      <td>1.926000e+08</td>\n",
              "      <td>0.400533</td>\n",
              "      <td>0.057980</td>\n",
              "      <td>-0.985500</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.079000</td>\n",
              "      <td>0.802000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>18312.390625</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18315.060547</td>\n",
              "      <td>18351.359375</td>\n",
              "      <td>18272.560547</td>\n",
              "      <td>6.749200e+08</td>\n",
              "      <td>0.615242</td>\n",
              "      <td>0.195774</td>\n",
              "      <td>0.991700</td>\n",
              "      <td>0.316000</td>\n",
              "      <td>0.153000</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dd54c01-35a9-4505-a52f-9d34885b10e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dd54c01-35a9-4505-a52f-9d34885b10e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dd54c01-35a9-4505-a52f-9d34885b10e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df_input.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4Ea8IFgXnVD"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nruXFUayXnVE"
      },
      "outputs": [],
      "source": [
        "features = df_scaled\n",
        "target = df_scaled[:,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f65h15kFXnVE"
      },
      "source": [
        "<center>features is a 2D list, target is a 1D list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6bEQjW6XnVF"
      },
      "outputs": [],
      "source": [
        "def plotHist(history : tf.keras.callbacks.History):\n",
        "    plt.plot(history.history[\"accuracy\"])\n",
        "    plt.plot(history.history[\"val_accuracy\"])\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['train', 'test'], loc='upper left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD1dPNqrXnVG",
        "outputId": "776f9f0d-241c-49fe-bd99-0c9fbc5d1ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1989, 12, 1989)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(features), len(features[0]), len(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "C9fbCyivXnVG",
        "outputId": "4295630d-d61b-4dde-98ad-abc59bf0d464"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Adj Close  Label          Open          High           Low     Volume  \\\n",
              "0  11734.320312      1  11432.089844  11759.959961  11388.040039  212830000   \n",
              "1  11782.349609      0  11729.669922  11867.110352  11675.530273  183190000   \n",
              "2  11642.469727      0  11781.700195  11782.349609  11601.519531  173590000   \n",
              "3  11532.959961      1  11632.809570  11633.780273  11453.339844  182550000   \n",
              "4  11615.929688      1  11532.070312  11718.280273  11450.889648  159790000   \n",
              "\n",
              "   Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b5df25f-db45-4d4b-b479-4a8c4471dc2a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b5df25f-db45-4d4b-b479-4a8c4471dc2a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b5df25f-db45-4d4b-b479-4a8c4471dc2a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b5df25f-db45-4d4b-b479-4a8c4471dc2a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcZS39ESXnVG",
        "outputId": "6d45ebae-d932-460a-bcae-cb8347fedaa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.40894198e-01, 1.00000000e+00, 4.15113783e-01, 4.33813671e-01,\n",
              "       4.16695095e-01, 3.06702075e-01, 2.34004623e-01, 4.20649400e-01,\n",
              "       6.52872640e-04, 6.84824903e-01, 2.32876712e-01, 4.44444444e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AM4p9K8XnVH",
        "outputId": "3f7fe8c4-4ff9-4b8d-9533-1952918547ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "target[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzFfx2TmXnVH"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzFaymZsXnVI",
        "outputId": "1a71f76f-f6e1-45d7-c0eb-ff70a1821172"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1491, 12), (498, 12), (1491,), (498,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gGhQj7YXnVI"
      },
      "outputs": [],
      "source": [
        "win_len = 30\n",
        "batch_size = 32\n",
        "num_features = 12\n",
        "train_generator = TimeseriesGenerator(x_train, y_train, length=win_len, sampling_rate=1, batch_size=batch_size)\n",
        "test_generator = TimeseriesGenerator(x_test, y_test, length=win_len, sampling_rate=1, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q91L1WuXnVI",
        "outputId": "afda0b09-50b9-4871-d7e3-893172643fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of batches in training set : 46\n",
            "number of batches in testing set : 15\n",
            "batch size : 32\n",
            "window size : 30\n",
            "number of features : 12\n"
          ]
        }
      ],
      "source": [
        "print(\"number of batches in training set :\", len(train_generator))\n",
        "print(\"number of batches in testing set :\", len(test_generator))\n",
        "print(\"batch size :\", len(train_generator[0][0]))\n",
        "print(\"window size :\", len(train_generator[0][0][0]))\n",
        "print(\"number of features :\", len(train_generator[0][0][0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpuK0YS4YcQ0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIEPqTwkYcTH"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK6XSj90YcU7"
      },
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QAVyDKgBwJw",
        "outputId": "fea9b921-4bc3-4ba7-f800-7c4d0e857aa1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 30, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_generator[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h3>Model 1\n"
      ],
      "metadata": {
        "id": "18rDy6Y-2fFb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HPsAdEgY5VV",
        "outputId": "ee0e3bfa-2e55-4686-dcbe-95619c715921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 30, 12)      24          ['input_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 30, 12)      52236       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 30, 12)       0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 30, 12)      0           ['dropout[0][0]',                \n",
            " da)                                                              'input_1[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 30, 4)        52          ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 30, 4)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 30, 12)       60          ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 30, 12)      0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 30, 12)      52236       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 30, 12)      0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 30, 4)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 30, 12)       60          ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 30, 12)      0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 30, 12)      52236       ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 30, 12)      0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 30, 4)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 30, 12)       60          ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 30, 12)      0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 30, 12)      52236       ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 30, 12)      0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 30, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 30, 12)       60          ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 30, 12)      0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 30)          0           ['tf.__operators__.add_7[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          3968        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 213,681\n",
            "Trainable params: 213,681\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "46/46 [==============================] - 14s 30ms/step - loss: 1.5611 - val_loss: 0.7137\n",
            "Epoch 2/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 1.1336 - val_loss: 0.6961\n",
            "Epoch 3/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.8684 - val_loss: 0.6953\n",
            "Epoch 4/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.8966 - val_loss: 0.6978\n",
            "Epoch 5/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.9463 - val_loss: 0.7053\n",
            "Epoch 6/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.8646 - val_loss: 0.7053\n",
            "Epoch 7/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 1.5413 - val_loss: 0.7101\n",
            "Epoch 8/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.8496 - val_loss: 0.7007\n",
            "Epoch 9/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.8170 - val_loss: 0.7098\n",
            "Epoch 10/150\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 0.7945 - val_loss: 0.7054\n",
            "Epoch 11/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7924 - val_loss: 0.6966\n",
            "Epoch 12/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7523 - val_loss: 0.6973\n",
            "Epoch 13/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7538 - val_loss: 0.7026\n",
            "Epoch 14/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7469 - val_loss: 0.6987\n",
            "Epoch 15/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7446 - val_loss: 0.6997\n",
            "Epoch 16/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7443 - val_loss: 0.7013\n",
            "Epoch 17/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7339 - val_loss: 0.7017\n",
            "Epoch 18/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7393 - val_loss: 0.7101\n",
            "Epoch 19/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7331 - val_loss: 0.7015\n",
            "Epoch 20/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7249 - val_loss: 0.7001\n",
            "Epoch 21/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7303 - val_loss: 0.7005\n",
            "Epoch 22/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7394 - val_loss: 0.7006\n",
            "Epoch 23/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7414 - val_loss: 0.7006\n",
            "Epoch 24/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7237 - val_loss: 0.7177\n",
            "Epoch 25/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7293 - val_loss: 0.7010\n",
            "Epoch 26/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7295 - val_loss: 0.7083\n",
            "Epoch 27/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7267 - val_loss: 0.7001\n",
            "Epoch 28/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7225 - val_loss: 0.7005\n",
            "Epoch 29/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7412 - val_loss: 0.7028\n",
            "Epoch 30/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7175 - val_loss: 0.7006\n",
            "Epoch 31/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7137 - val_loss: 0.7014\n",
            "Epoch 32/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7330 - val_loss: 0.7152\n",
            "Epoch 33/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7213 - val_loss: 0.7009\n",
            "Epoch 34/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7080 - val_loss: 0.7004\n",
            "Epoch 35/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7117 - val_loss: 0.7057\n",
            "Epoch 36/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7256 - val_loss: 0.7142\n",
            "Epoch 37/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7304 - val_loss: 0.7007\n",
            "Epoch 38/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7117 - val_loss: 0.7052\n",
            "Epoch 39/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7197 - val_loss: 0.7558\n",
            "Epoch 40/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7184 - val_loss: 0.7042\n",
            "Epoch 41/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7112 - val_loss: 0.7187\n",
            "Epoch 42/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7068 - val_loss: 0.7019\n",
            "Epoch 43/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7237 - val_loss: 0.7096\n",
            "Epoch 44/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7145 - val_loss: 0.7023\n",
            "Epoch 45/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7209 - val_loss: 0.7018\n",
            "Epoch 46/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7187 - val_loss: 0.7010\n",
            "Epoch 47/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7055 - val_loss: 0.7066\n",
            "Epoch 48/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7139 - val_loss: 0.7013\n",
            "Epoch 49/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7285 - val_loss: 0.7234\n",
            "Epoch 50/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7062 - val_loss: 0.7003\n",
            "Epoch 51/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7236 - val_loss: 0.6983\n",
            "Epoch 52/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7024 - val_loss: 0.7037\n",
            "Epoch 53/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7062 - val_loss: 0.7007\n",
            "Epoch 54/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7095 - val_loss: 0.6991\n",
            "Epoch 55/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7118 - val_loss: 0.7091\n",
            "Epoch 56/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7223 - val_loss: 0.7077\n",
            "Epoch 57/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7148 - val_loss: 0.7055\n",
            "Epoch 58/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7249 - val_loss: 0.7045\n",
            "Epoch 59/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7103 - val_loss: 0.6995\n",
            "Epoch 60/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7182 - val_loss: 0.6999\n",
            "Epoch 61/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7011 - val_loss: 0.7034\n",
            "Epoch 62/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7051 - val_loss: 0.7081\n",
            "Epoch 63/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7043 - val_loss: 0.7007\n",
            "Epoch 64/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7054 - val_loss: 0.6986\n",
            "Epoch 65/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7190 - val_loss: 0.7164\n",
            "Epoch 66/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7119 - val_loss: 0.7214\n",
            "Epoch 67/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7103 - val_loss: 0.7021\n",
            "Epoch 68/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7038 - val_loss: 0.7148\n",
            "Epoch 69/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7127 - val_loss: 0.7325\n",
            "Epoch 70/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7033 - val_loss: 0.6999\n",
            "Epoch 71/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7036 - val_loss: 0.7572\n",
            "Epoch 72/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7077 - val_loss: 0.7124\n",
            "Epoch 73/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7191 - val_loss: 0.7013\n",
            "Epoch 74/150\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.7127 - val_loss: 0.6988\n",
            "Epoch 75/150\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.7144 - val_loss: 0.7018\n",
            "Epoch 76/150\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 0.7120 - val_loss: 0.8076\n",
            "Epoch 77/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7162 - val_loss: 0.7130\n",
            "Epoch 78/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7123 - val_loss: 0.6960\n",
            "Epoch 79/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7099 - val_loss: 0.6992\n",
            "Epoch 80/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7032 - val_loss: 0.6966\n",
            "Epoch 81/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7125 - val_loss: 0.6962\n",
            "Epoch 82/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7101 - val_loss: 0.7013\n",
            "Epoch 83/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7072 - val_loss: 0.7056\n",
            "Epoch 84/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7075 - val_loss: 0.7021\n",
            "Epoch 85/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6997 - val_loss: 0.6980\n",
            "Epoch 86/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6963 - val_loss: 0.7031\n",
            "Epoch 87/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7050 - val_loss: 0.6980\n",
            "Epoch 88/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7095 - val_loss: 0.6996\n",
            "Epoch 89/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7074 - val_loss: 0.6994\n",
            "Epoch 90/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6967 - val_loss: 0.6972\n",
            "Epoch 91/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7034 - val_loss: 0.7427\n",
            "Epoch 92/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7029 - val_loss: 0.7000\n",
            "Epoch 93/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6993 - val_loss: 0.6977\n",
            "Epoch 94/150\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.6991 - val_loss: 0.6998\n",
            "Epoch 95/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7016 - val_loss: 0.6987\n",
            "Epoch 96/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7004 - val_loss: 0.6987\n",
            "Epoch 97/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7189 - val_loss: 0.7090\n",
            "Epoch 98/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7063 - val_loss: 0.7165\n",
            "Epoch 99/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7016 - val_loss: 0.6967\n",
            "Epoch 100/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7132 - val_loss: 0.6969\n",
            "Epoch 101/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7034 - val_loss: 0.6998\n",
            "Epoch 102/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7024 - val_loss: 0.6974\n",
            "Epoch 103/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6965 - val_loss: 0.6974\n",
            "Epoch 104/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7032 - val_loss: 0.7030\n",
            "Epoch 105/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7075 - val_loss: 0.6957\n",
            "Epoch 106/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7048 - val_loss: 0.6982\n",
            "Epoch 107/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7027 - val_loss: 0.6962\n",
            "Epoch 108/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7004 - val_loss: 0.7006\n",
            "Epoch 109/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7018 - val_loss: 0.6971\n",
            "Epoch 110/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7057 - val_loss: 0.6993\n",
            "Epoch 111/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6998 - val_loss: 0.6977\n",
            "Epoch 112/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6941 - val_loss: 0.7109\n",
            "Epoch 113/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6987 - val_loss: 0.6971\n",
            "Epoch 114/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6986 - val_loss: 0.7108\n",
            "Epoch 115/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7002 - val_loss: 0.6973\n",
            "Epoch 116/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7055 - val_loss: 0.6960\n",
            "Epoch 117/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6989 - val_loss: 0.7037\n",
            "Epoch 118/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7003 - val_loss: 0.6962\n",
            "Epoch 119/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6946 - val_loss: 0.6958\n",
            "Epoch 120/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7015 - val_loss: 0.6993\n",
            "Epoch 121/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7052 - val_loss: 0.7012\n",
            "Epoch 122/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7037 - val_loss: 0.6972\n",
            "Epoch 123/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6935 - val_loss: 0.6968\n",
            "Epoch 124/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6968 - val_loss: 0.6975\n",
            "Epoch 125/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7034 - val_loss: 0.6976\n",
            "Epoch 126/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7026 - val_loss: 0.6980\n",
            "Epoch 127/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7000 - val_loss: 0.7021\n",
            "Epoch 128/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6928 - val_loss: 0.7031\n",
            "Epoch 129/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7089 - val_loss: 0.6973\n",
            "Epoch 130/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6916 - val_loss: 0.6979\n",
            "Epoch 131/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6974 - val_loss: 0.7002\n",
            "Epoch 132/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6946 - val_loss: 0.6964\n",
            "Epoch 133/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7033 - val_loss: 0.6976\n",
            "Epoch 134/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6998 - val_loss: 0.7037\n",
            "Epoch 135/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6964 - val_loss: 0.6961\n",
            "Epoch 136/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6980 - val_loss: 0.6958\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6886 - val_loss: 0.6982\n",
            "Epoch 138/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6968 - val_loss: 0.6974\n",
            "Epoch 139/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7003 - val_loss: 0.6960\n",
            "Epoch 140/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6931 - val_loss: 0.6960\n",
            "Epoch 141/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7000 - val_loss: 0.6962\n",
            "Epoch 142/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7007 - val_loss: 0.6958\n",
            "Epoch 143/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6960 - val_loss: 0.6959\n",
            "Epoch 144/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6930 - val_loss: 0.6962\n",
            "Epoch 145/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6974 - val_loss: 0.6964\n",
            "Epoch 146/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6978 - val_loss: 0.6956\n",
            "Epoch 147/150\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6987 - val_loss: 0.6958\n",
            "Epoch 148/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6944 - val_loss: 0.7019\n",
            "Epoch 149/150\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6952 - val_loss: 0.6976\n",
            "Epoch 150/150\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6848 - val_loss: 0.6967\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6fdc457cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# input_shape = x_train.shape[1:]\n",
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model1 = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model1.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "model1.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, \\\n",
        "    restore_best_weights=True)]\n",
        "\n",
        "# model.fit(\n",
        "#     train_generator,\n",
        "#     y_train,\n",
        "#     validation_split=0.2,\n",
        "#     epochs=200,\n",
        "#     batch_size=64,\n",
        "#     callbacks=callbacks,\n",
        "# )\n",
        "\n",
        "model1.fit_generator(train_generator, epochs=150, validation_data=test_generator)\n",
        "\n",
        "# model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFD40LXDFwc9"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model1.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model1.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu9JbnpfQd_l",
        "outputId": "4acd92ec-472c-4b11-ec8b-f9cea57489ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7432 - accuracy: 0.5133\n",
            "Epoch 1: val_accuracy improved from -inf to 0.48504, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 5s 31ms/step - loss: 0.7432 - accuracy: 0.5133 - val_loss: 0.7392 - val_accuracy: 0.4850\n",
            "Epoch 2/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7709 - accuracy: 0.5189\n",
            "Epoch 2: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7680 - accuracy: 0.5195 - val_loss: 0.7829 - val_accuracy: 0.4850\n",
            "Epoch 3/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7337 - accuracy: 0.5106\n",
            "Epoch 3: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7337 - accuracy: 0.5106 - val_loss: 0.7439 - val_accuracy: 0.4850\n",
            "Epoch 4/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7252 - accuracy: 0.4949\n",
            "Epoch 4: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7243 - accuracy: 0.4962 - val_loss: 0.7228 - val_accuracy: 0.4850\n",
            "Epoch 5/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7188 - accuracy: 0.5050\n",
            "Epoch 5: val_accuracy improved from 0.48504 to 0.51496, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 0.7188 - accuracy: 0.5051 - val_loss: 0.7556 - val_accuracy: 0.5150\n",
            "Epoch 6/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7308 - accuracy: 0.4936\n",
            "Epoch 6: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7328 - accuracy: 0.4901 - val_loss: 0.7304 - val_accuracy: 0.4850\n",
            "Epoch 7/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7187 - accuracy: 0.5051\n",
            "Epoch 7: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7222 - accuracy: 0.4983 - val_loss: 0.7939 - val_accuracy: 0.4850\n",
            "Epoch 8/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7254 - accuracy: 0.5065\n",
            "Epoch 8: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7260 - accuracy: 0.5010 - val_loss: 0.6946 - val_accuracy: 0.5128\n",
            "Epoch 9/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7248 - accuracy: 0.4792\n",
            "Epoch 9: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7245 - accuracy: 0.4784 - val_loss: 0.6926 - val_accuracy: 0.5021\n",
            "Epoch 10/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7106 - accuracy: 0.5185\n",
            "Epoch 10: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.7105 - accuracy: 0.5195 - val_loss: 0.6931 - val_accuracy: 0.5128\n",
            "Epoch 11/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7178 - accuracy: 0.4815\n",
            "Epoch 11: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7164 - accuracy: 0.4839 - val_loss: 0.7023 - val_accuracy: 0.5150\n",
            "Epoch 12/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7009 - accuracy: 0.5145\n",
            "Epoch 12: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6997 - accuracy: 0.5188 - val_loss: 0.7076 - val_accuracy: 0.5150\n",
            "Epoch 13/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7109 - accuracy: 0.4942\n",
            "Epoch 13: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7109 - accuracy: 0.4942 - val_loss: 0.6954 - val_accuracy: 0.5150\n",
            "Epoch 14/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7037 - accuracy: 0.5342\n",
            "Epoch 14: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7039 - accuracy: 0.5346 - val_loss: 0.6953 - val_accuracy: 0.5150\n",
            "Epoch 15/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7049 - accuracy: 0.5276\n",
            "Epoch 15: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.7038 - accuracy: 0.5305 - val_loss: 0.6961 - val_accuracy: 0.5150\n",
            "Epoch 16/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6978 - accuracy: 0.5320\n",
            "Epoch 16: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6973 - accuracy: 0.5325 - val_loss: 0.6963 - val_accuracy: 0.5150\n",
            "Epoch 17/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7048 - accuracy: 0.5127\n",
            "Epoch 17: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.7048 - accuracy: 0.5127 - val_loss: 0.7001 - val_accuracy: 0.5150\n",
            "Epoch 18/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.5010\n",
            "Epoch 18: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7079 - accuracy: 0.5010 - val_loss: 0.6948 - val_accuracy: 0.5150\n",
            "Epoch 19/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7071 - accuracy: 0.5073\n",
            "Epoch 19: val_accuracy improved from 0.51496 to 0.51709, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7054 - accuracy: 0.5113 - val_loss: 0.6943 - val_accuracy: 0.5171\n",
            "Epoch 20/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7147 - accuracy: 0.4760\n",
            "Epoch 20: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7113 - accuracy: 0.4853 - val_loss: 0.6950 - val_accuracy: 0.5150\n",
            "Epoch 21/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7036 - accuracy: 0.4986\n",
            "Epoch 21: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7024 - accuracy: 0.5017 - val_loss: 0.7092 - val_accuracy: 0.5150\n",
            "Epoch 22/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7008 - accuracy: 0.5102\n",
            "Epoch 22: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7011 - accuracy: 0.5072 - val_loss: 0.7002 - val_accuracy: 0.5150\n",
            "Epoch 23/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5389\n",
            "Epoch 23: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6934 - accuracy: 0.5380 - val_loss: 0.6944 - val_accuracy: 0.5085\n",
            "Epoch 24/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7006 - accuracy: 0.5222\n",
            "Epoch 24: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6998 - accuracy: 0.5236 - val_loss: 0.6944 - val_accuracy: 0.5107\n",
            "Epoch 25/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7098 - accuracy: 0.5007\n",
            "Epoch 25: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7096 - accuracy: 0.4997 - val_loss: 0.6964 - val_accuracy: 0.5043\n",
            "Epoch 26/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7039 - accuracy: 0.5042\n",
            "Epoch 26: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7036 - accuracy: 0.5044 - val_loss: 0.6946 - val_accuracy: 0.4786\n",
            "Epoch 27/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7028 - accuracy: 0.4922\n",
            "Epoch 27: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7028 - accuracy: 0.4935 - val_loss: 0.6965 - val_accuracy: 0.4701\n",
            "Epoch 28/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6951 - accuracy: 0.5292\n",
            "Epoch 28: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6950 - accuracy: 0.5284 - val_loss: 0.6946 - val_accuracy: 0.4744\n",
            "Epoch 29/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7080 - accuracy: 0.4969\n",
            "Epoch 29: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7080 - accuracy: 0.4969 - val_loss: 0.6936 - val_accuracy: 0.5107\n",
            "Epoch 30/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7014 - accuracy: 0.5116\n",
            "Epoch 30: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7026 - accuracy: 0.5086 - val_loss: 0.6983 - val_accuracy: 0.5150\n",
            "Epoch 31/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6982 - accuracy: 0.5215\n",
            "Epoch 31: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6987 - accuracy: 0.5202 - val_loss: 0.6989 - val_accuracy: 0.4679\n",
            "Epoch 32/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6942 - accuracy: 0.5431\n",
            "Epoch 32: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6939 - accuracy: 0.5435 - val_loss: 0.7038 - val_accuracy: 0.5150\n",
            "Epoch 33/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6972 - accuracy: 0.5167\n",
            "Epoch 33: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6980 - accuracy: 0.5168 - val_loss: 0.6985 - val_accuracy: 0.4850\n",
            "Epoch 34/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6969 - accuracy: 0.5298\n",
            "Epoch 34: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6958 - accuracy: 0.5305 - val_loss: 0.6966 - val_accuracy: 0.5171\n",
            "Epoch 35/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6855 - accuracy: 0.5533\n",
            "Epoch 35: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6862 - accuracy: 0.5510 - val_loss: 0.6979 - val_accuracy: 0.4979\n",
            "Epoch 36/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5698\n",
            "Epoch 36: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6894 - accuracy: 0.5715 - val_loss: 0.7077 - val_accuracy: 0.5150\n",
            "Epoch 37/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7107 - accuracy: 0.5090\n",
            "Epoch 37: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7102 - accuracy: 0.5106 - val_loss: 0.7025 - val_accuracy: 0.5150\n",
            "Epoch 38/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.5414\n",
            "Epoch 38: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6882 - accuracy: 0.5414 - val_loss: 0.6997 - val_accuracy: 0.4979\n",
            "Epoch 39/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6867 - accuracy: 0.5486\n",
            "Epoch 39: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6862 - accuracy: 0.5496 - val_loss: 0.6992 - val_accuracy: 0.5107\n",
            "Epoch 40/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.5558\n",
            "Epoch 40: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6884 - accuracy: 0.5558 - val_loss: 0.7089 - val_accuracy: 0.5128\n",
            "Epoch 41/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6775 - accuracy: 0.5632\n",
            "Epoch 41: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.6780 - accuracy: 0.5613 - val_loss: 0.6976 - val_accuracy: 0.5171\n",
            "Epoch 42/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.5510\n",
            "Epoch 42: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6948 - accuracy: 0.5510 - val_loss: 0.7085 - val_accuracy: 0.5150\n",
            "Epoch 43/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6910 - accuracy: 0.5320\n",
            "Epoch 43: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6916 - accuracy: 0.5339 - val_loss: 0.7047 - val_accuracy: 0.5150\n",
            "Epoch 44/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6834 - accuracy: 0.5479\n",
            "Epoch 44: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6833 - accuracy: 0.5489 - val_loss: 0.6990 - val_accuracy: 0.5150\n",
            "Epoch 45/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6941 - accuracy: 0.5299\n",
            "Epoch 45: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6937 - accuracy: 0.5311 - val_loss: 0.6992 - val_accuracy: 0.5085\n",
            "Epoch 46/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5528\n",
            "Epoch 46: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6896 - accuracy: 0.5503 - val_loss: 0.7011 - val_accuracy: 0.4850\n",
            "Epoch 47/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.5455\n",
            "Epoch 47: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6877 - accuracy: 0.5455 - val_loss: 0.6988 - val_accuracy: 0.4829\n",
            "Epoch 48/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.5407\n",
            "Epoch 48: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6870 - accuracy: 0.5407 - val_loss: 0.6975 - val_accuracy: 0.5043\n",
            "Epoch 49/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.5455\n",
            "Epoch 49: val_accuracy improved from 0.51709 to 0.51923, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6960 - accuracy: 0.5455 - val_loss: 0.7098 - val_accuracy: 0.5192\n",
            "Epoch 50/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.5043\n",
            "Epoch 50: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6937 - accuracy: 0.5031 - val_loss: 0.7045 - val_accuracy: 0.4808\n",
            "Epoch 51/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.5493\n",
            "Epoch 51: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6832 - accuracy: 0.5503 - val_loss: 0.7016 - val_accuracy: 0.4979\n",
            "Epoch 52/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6834 - accuracy: 0.5625\n",
            "Epoch 52: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6835 - accuracy: 0.5619 - val_loss: 0.6976 - val_accuracy: 0.5107\n",
            "Epoch 53/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6772 - accuracy: 0.5743\n",
            "Epoch 53: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6773 - accuracy: 0.5756 - val_loss: 0.7010 - val_accuracy: 0.5150\n",
            "Epoch 54/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6747 - accuracy: 0.5689\n",
            "Epoch 54: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6751 - accuracy: 0.5702 - val_loss: 0.7032 - val_accuracy: 0.5128\n",
            "Epoch 55/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6563 - accuracy: 0.5778\n",
            "Epoch 55: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6601 - accuracy: 0.5695 - val_loss: 0.6955 - val_accuracy: 0.4893\n",
            "Epoch 56/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.5941\n",
            "Epoch 56: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6841 - accuracy: 0.5941 - val_loss: 0.7037 - val_accuracy: 0.5150\n",
            "Epoch 57/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6510 - accuracy: 0.6141\n",
            "Epoch 57: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6521 - accuracy: 0.6112 - val_loss: 0.7167 - val_accuracy: 0.5150\n",
            "Epoch 58/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6645 - accuracy: 0.5833\n",
            "Epoch 58: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6646 - accuracy: 0.5852 - val_loss: 0.7133 - val_accuracy: 0.5150\n",
            "Epoch 59/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6707 - accuracy: 0.5938\n",
            "Epoch 59: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6708 - accuracy: 0.5934 - val_loss: 0.7127 - val_accuracy: 0.5107\n",
            "Epoch 60/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6510 - accuracy: 0.6016\n",
            "Epoch 60: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6512 - accuracy: 0.6037 - val_loss: 0.7108 - val_accuracy: 0.5150\n",
            "Epoch 61/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6523 - accuracy: 0.5791\n",
            "Epoch 61: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6523 - accuracy: 0.5791 - val_loss: 0.6999 - val_accuracy: 0.4573\n",
            "Epoch 62/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6580 - accuracy: 0.6009\n",
            "Epoch 62: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6587 - accuracy: 0.6037 - val_loss: 0.7094 - val_accuracy: 0.5150\n",
            "Epoch 63/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6576 - accuracy: 0.6023\n",
            "Epoch 63: val_accuracy improved from 0.51923 to 0.52137, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6576 - accuracy: 0.6023 - val_loss: 0.7024 - val_accuracy: 0.5214\n",
            "Epoch 64/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6739 - accuracy: 0.6061\n",
            "Epoch 64: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6740 - accuracy: 0.6044 - val_loss: 0.7165 - val_accuracy: 0.5150\n",
            "Epoch 65/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5889\n",
            "Epoch 65: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6891 - accuracy: 0.5880 - val_loss: 0.6934 - val_accuracy: 0.5128\n",
            "Epoch 66/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.5975\n",
            "Epoch 66: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6553 - accuracy: 0.5975 - val_loss: 0.6978 - val_accuracy: 0.5150\n",
            "Epoch 67/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6590 - accuracy: 0.5858\n",
            "Epoch 67: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6616 - accuracy: 0.5825 - val_loss: 0.6952 - val_accuracy: 0.4637\n",
            "Epoch 68/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6485 - accuracy: 0.5764\n",
            "Epoch 68: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6490 - accuracy: 0.5763 - val_loss: 0.7087 - val_accuracy: 0.5107\n",
            "Epoch 69/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7672 - accuracy: 0.5486\n",
            "Epoch 69: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7672 - accuracy: 0.5476 - val_loss: 0.6969 - val_accuracy: 0.5128\n",
            "Epoch 70/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7204 - accuracy: 0.4927\n",
            "Epoch 70: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7196 - accuracy: 0.4928 - val_loss: 0.6955 - val_accuracy: 0.5150\n",
            "Epoch 71/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7083 - accuracy: 0.5099\n",
            "Epoch 71: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7083 - accuracy: 0.5099 - val_loss: 0.6947 - val_accuracy: 0.5150\n",
            "Epoch 72/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7030 - accuracy: 0.5121\n",
            "Epoch 72: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7028 - accuracy: 0.5113 - val_loss: 0.6944 - val_accuracy: 0.5085\n",
            "Epoch 73/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7023 - accuracy: 0.4965\n",
            "Epoch 73: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7022 - accuracy: 0.4976 - val_loss: 0.6946 - val_accuracy: 0.4658\n",
            "Epoch 74/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.5133\n",
            "Epoch 74: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7015 - accuracy: 0.5133 - val_loss: 0.6943 - val_accuracy: 0.4808\n",
            "Epoch 75/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6985 - accuracy: 0.5249\n",
            "Epoch 75: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6982 - accuracy: 0.5222 - val_loss: 0.6948 - val_accuracy: 0.5150\n",
            "Epoch 76/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7041 - accuracy: 0.5065\n",
            "Epoch 76: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7041 - accuracy: 0.5065 - val_loss: 0.6954 - val_accuracy: 0.4509\n",
            "Epoch 77/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7033 - accuracy: 0.4931\n",
            "Epoch 77: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7029 - accuracy: 0.4962 - val_loss: 0.6957 - val_accuracy: 0.5064\n",
            "Epoch 78/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7013 - accuracy: 0.4917\n",
            "Epoch 78: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7007 - accuracy: 0.4949 - val_loss: 0.6960 - val_accuracy: 0.5021\n",
            "Epoch 79/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7004 - accuracy: 0.4956\n",
            "Epoch 79: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7002 - accuracy: 0.4969 - val_loss: 0.6955 - val_accuracy: 0.4957\n",
            "Epoch 80/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6954 - accuracy: 0.5099\n",
            "Epoch 80: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6945 - accuracy: 0.5154 - val_loss: 0.6969 - val_accuracy: 0.5171\n",
            "Epoch 81/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.5278\n",
            "Epoch 81: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6937 - accuracy: 0.5284 - val_loss: 0.6986 - val_accuracy: 0.5000\n",
            "Epoch 82/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5407\n",
            "Epoch 82: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6939 - accuracy: 0.5407 - val_loss: 0.7063 - val_accuracy: 0.5128\n",
            "Epoch 83/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6862 - accuracy: 0.5526\n",
            "Epoch 83: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6841 - accuracy: 0.5578 - val_loss: 0.7081 - val_accuracy: 0.5150\n",
            "Epoch 84/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.5305\n",
            "Epoch 84: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6904 - accuracy: 0.5305 - val_loss: 0.6965 - val_accuracy: 0.5150\n",
            "Epoch 85/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.5763\n",
            "Epoch 85: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6868 - accuracy: 0.5763 - val_loss: 0.7026 - val_accuracy: 0.5150\n",
            "Epoch 86/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6631 - accuracy: 0.5702\n",
            "Epoch 86: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6631 - accuracy: 0.5702 - val_loss: 0.6976 - val_accuracy: 0.5128\n",
            "Epoch 87/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6695 - accuracy: 0.5589\n",
            "Epoch 87: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6714 - accuracy: 0.5619 - val_loss: 0.6974 - val_accuracy: 0.4402\n",
            "Epoch 88/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6724 - accuracy: 0.5974\n",
            "Epoch 88: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6729 - accuracy: 0.5941 - val_loss: 0.7026 - val_accuracy: 0.5150\n",
            "Epoch 89/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6580 - accuracy: 0.5909\n",
            "Epoch 89: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6587 - accuracy: 0.5900 - val_loss: 0.6995 - val_accuracy: 0.4701\n",
            "Epoch 90/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6982 - accuracy: 0.6141\n",
            "Epoch 90: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6969 - accuracy: 0.6071 - val_loss: 0.7043 - val_accuracy: 0.5150\n",
            "Epoch 91/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6243 - accuracy: 0.6083\n",
            "Epoch 91: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6252 - accuracy: 0.6099 - val_loss: 0.6978 - val_accuracy: 0.5192\n",
            "Epoch 92/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6534 - accuracy: 0.6112\n",
            "Epoch 92: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6534 - accuracy: 0.6112 - val_loss: 0.6966 - val_accuracy: 0.5064\n",
            "Epoch 93/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6228 - accuracy: 0.6125\n",
            "Epoch 93: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6235 - accuracy: 0.6126 - val_loss: 0.6984 - val_accuracy: 0.5171\n",
            "Epoch 94/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.6133\n",
            "Epoch 94: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6722 - accuracy: 0.6133 - val_loss: 0.7132 - val_accuracy: 0.5150\n",
            "Epoch 95/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6433 - accuracy: 0.6090\n",
            "Epoch 95: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6436 - accuracy: 0.6099 - val_loss: 0.7066 - val_accuracy: 0.5150\n",
            "Epoch 96/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6724 - accuracy: 0.6371\n",
            "Epoch 96: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6738 - accuracy: 0.6283 - val_loss: 0.6977 - val_accuracy: 0.4979\n",
            "Epoch 97/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6444 - accuracy: 0.6057\n",
            "Epoch 97: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6444 - accuracy: 0.6057 - val_loss: 0.6960 - val_accuracy: 0.4551\n",
            "Epoch 98/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6262 - accuracy: 0.5959\n",
            "Epoch 98: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6289 - accuracy: 0.5934 - val_loss: 0.6951 - val_accuracy: 0.5150\n",
            "Epoch 99/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6844 - accuracy: 0.5447\n",
            "Epoch 99: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6848 - accuracy: 0.5428 - val_loss: 0.6942 - val_accuracy: 0.5128\n",
            "Epoch 100/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6618 - accuracy: 0.5619\n",
            "Epoch 100: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6618 - accuracy: 0.5619 - val_loss: 0.6949 - val_accuracy: 0.4957\n",
            "Epoch 101/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7587 - accuracy: 0.5854\n",
            "Epoch 101: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7578 - accuracy: 0.5852 - val_loss: 0.7040 - val_accuracy: 0.5150\n",
            "Epoch 102/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6857 - accuracy: 0.5575\n",
            "Epoch 102: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6856 - accuracy: 0.5572 - val_loss: 0.6956 - val_accuracy: 0.5150\n",
            "Epoch 103/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7325 - accuracy: 0.5359\n",
            "Epoch 103: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7325 - accuracy: 0.5359 - val_loss: 0.6954 - val_accuracy: 0.4915\n",
            "Epoch 104/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7018 - accuracy: 0.5135\n",
            "Epoch 104: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7019 - accuracy: 0.5106 - val_loss: 0.6949 - val_accuracy: 0.4808\n",
            "Epoch 105/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6859 - accuracy: 0.5361\n",
            "Epoch 105: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6856 - accuracy: 0.5366 - val_loss: 0.6949 - val_accuracy: 0.5043\n",
            "Epoch 106/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6707 - accuracy: 0.5690\n",
            "Epoch 106: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6713 - accuracy: 0.5702 - val_loss: 0.6945 - val_accuracy: 0.5171\n",
            "Epoch 107/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6748 - accuracy: 0.5732\n",
            "Epoch 107: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6760 - accuracy: 0.5661 - val_loss: 0.6953 - val_accuracy: 0.4850\n",
            "Epoch 108/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6675 - accuracy: 0.5916\n",
            "Epoch 108: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6689 - accuracy: 0.5866 - val_loss: 0.6942 - val_accuracy: 0.4744\n",
            "Epoch 109/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5500\n",
            "Epoch 109: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6833 - accuracy: 0.5489 - val_loss: 0.6953 - val_accuracy: 0.5128\n",
            "Epoch 110/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6785 - accuracy: 0.5625\n",
            "Epoch 110: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6780 - accuracy: 0.5654 - val_loss: 0.6975 - val_accuracy: 0.5150\n",
            "Epoch 111/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7022 - accuracy: 0.5483\n",
            "Epoch 111: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7024 - accuracy: 0.5441 - val_loss: 0.6948 - val_accuracy: 0.4722\n",
            "Epoch 112/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6710 - accuracy: 0.5705\n",
            "Epoch 112: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6719 - accuracy: 0.5702 - val_loss: 0.6939 - val_accuracy: 0.5150\n",
            "Epoch 113/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6739 - accuracy: 0.5717\n",
            "Epoch 113: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6744 - accuracy: 0.5722 - val_loss: 0.6962 - val_accuracy: 0.5085\n",
            "Epoch 114/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6458 - accuracy: 0.5859\n",
            "Epoch 114: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6473 - accuracy: 0.5838 - val_loss: 0.6939 - val_accuracy: 0.4850\n",
            "Epoch 115/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6684 - accuracy: 0.5722\n",
            "Epoch 115: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6687 - accuracy: 0.5708 - val_loss: 0.6940 - val_accuracy: 0.4466\n",
            "Epoch 116/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7144 - accuracy: 0.5916\n",
            "Epoch 116: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.7135 - accuracy: 0.5859 - val_loss: 0.6935 - val_accuracy: 0.5128\n",
            "Epoch 117/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.5866\n",
            "Epoch 117: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7052 - accuracy: 0.5866 - val_loss: 0.6943 - val_accuracy: 0.5085\n",
            "Epoch 118/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6700 - accuracy: 0.5828\n",
            "Epoch 118: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6714 - accuracy: 0.5743 - val_loss: 0.6935 - val_accuracy: 0.5064\n",
            "Epoch 119/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6600 - accuracy: 0.5778\n",
            "Epoch 119: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6607 - accuracy: 0.5770 - val_loss: 0.6937 - val_accuracy: 0.5150\n",
            "Epoch 120/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7040 - accuracy: 0.5832\n",
            "Epoch 120: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7040 - accuracy: 0.5832 - val_loss: 0.6967 - val_accuracy: 0.5150\n",
            "Epoch 121/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6821 - accuracy: 0.5312\n",
            "Epoch 121: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6817 - accuracy: 0.5332 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
            "Epoch 122/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6724 - accuracy: 0.5810\n",
            "Epoch 122: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6727 - accuracy: 0.5797 - val_loss: 0.6936 - val_accuracy: 0.5150\n",
            "Epoch 123/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6615 - accuracy: 0.5681\n",
            "Epoch 123: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6614 - accuracy: 0.5702 - val_loss: 0.6935 - val_accuracy: 0.5150\n",
            "Epoch 124/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7012 - accuracy: 0.5791\n",
            "Epoch 124: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7012 - accuracy: 0.5791 - val_loss: 0.6924 - val_accuracy: 0.5150\n",
            "Epoch 125/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.5325\n",
            "Epoch 125: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6909 - accuracy: 0.5325 - val_loss: 0.6925 - val_accuracy: 0.5150\n",
            "Epoch 126/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6947 - accuracy: 0.5229\n",
            "Epoch 126: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6943 - accuracy: 0.5250 - val_loss: 0.6925 - val_accuracy: 0.5150\n",
            "Epoch 127/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5361\n",
            "Epoch 127: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6892 - accuracy: 0.5380 - val_loss: 0.6937 - val_accuracy: 0.5150\n",
            "Epoch 128/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6915 - accuracy: 0.5348\n",
            "Epoch 128: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6908 - accuracy: 0.5400 - val_loss: 0.6932 - val_accuracy: 0.5150\n",
            "Epoch 129/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.5421\n",
            "Epoch 129: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6897 - accuracy: 0.5421 - val_loss: 0.6933 - val_accuracy: 0.5150\n",
            "Epoch 130/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6901 - accuracy: 0.5361\n",
            "Epoch 130: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6901 - accuracy: 0.5380 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
            "Epoch 131/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6856 - accuracy: 0.5448\n",
            "Epoch 131: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6856 - accuracy: 0.5448 - val_loss: 0.6929 - val_accuracy: 0.5150\n",
            "Epoch 132/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6917 - accuracy: 0.5426\n",
            "Epoch 132: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6906 - accuracy: 0.5476 - val_loss: 0.6942 - val_accuracy: 0.5150\n",
            "Epoch 133/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6897 - accuracy: 0.5410\n",
            "Epoch 133: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6895 - accuracy: 0.5428 - val_loss: 0.6928 - val_accuracy: 0.5150\n",
            "Epoch 134/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.5496\n",
            "Epoch 134: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6866 - accuracy: 0.5496 - val_loss: 0.6931 - val_accuracy: 0.5150\n",
            "Epoch 135/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5528\n",
            "Epoch 135: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6864 - accuracy: 0.5537 - val_loss: 0.6929 - val_accuracy: 0.5150\n",
            "Epoch 136/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6812 - accuracy: 0.5599\n",
            "Epoch 136: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6812 - accuracy: 0.5599 - val_loss: 0.6944 - val_accuracy: 0.5150\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6753 - accuracy: 0.5695\n",
            "Epoch 137: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 0.6753 - accuracy: 0.5695 - val_loss: 0.6936 - val_accuracy: 0.5150\n",
            "Epoch 138/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6681 - accuracy: 0.5695\n",
            "Epoch 138: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6681 - accuracy: 0.5695 - val_loss: 0.6934 - val_accuracy: 0.5150\n",
            "Epoch 139/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6694 - accuracy: 0.5604\n",
            "Epoch 139: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6697 - accuracy: 0.5647 - val_loss: 0.6933 - val_accuracy: 0.5150\n",
            "Epoch 140/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6969 - accuracy: 0.4979\n",
            "Epoch 140: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6971 - accuracy: 0.4976 - val_loss: 0.6946 - val_accuracy: 0.4850\n",
            "Epoch 141/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4997\n",
            "Epoch 141: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6942 - accuracy: 0.4997 - val_loss: 0.6940 - val_accuracy: 0.4893\n",
            "Epoch 142/150\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.5182\n",
            "Epoch 142: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6934 - accuracy: 0.5175 - val_loss: 0.6938 - val_accuracy: 0.4423\n",
            "Epoch 143/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6937 - accuracy: 0.5092\n",
            "Epoch 143: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6936 - accuracy: 0.5106 - val_loss: 0.6939 - val_accuracy: 0.4957\n",
            "Epoch 144/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6962 - accuracy: 0.4854\n",
            "Epoch 144: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6962 - accuracy: 0.4860 - val_loss: 0.6940 - val_accuracy: 0.4850\n",
            "Epoch 145/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6949 - accuracy: 0.4929\n",
            "Epoch 145: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6948 - accuracy: 0.4935 - val_loss: 0.6937 - val_accuracy: 0.4979\n",
            "Epoch 146/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6912 - accuracy: 0.5355\n",
            "Epoch 146: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6908 - accuracy: 0.5380 - val_loss: 0.6935 - val_accuracy: 0.4786\n",
            "Epoch 147/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6941 - accuracy: 0.4910\n",
            "Epoch 147: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6941 - accuracy: 0.4894 - val_loss: 0.6937 - val_accuracy: 0.4850\n",
            "Epoch 148/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.4938\n",
            "Epoch 148: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6936 - accuracy: 0.4935 - val_loss: 0.6934 - val_accuracy: 0.4893\n",
            "Epoch 149/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6951 - accuracy: 0.4674\n",
            "Epoch 149: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6951 - accuracy: 0.4675 - val_loss: 0.6934 - val_accuracy: 0.4850\n",
            "Epoch 150/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6919 - accuracy: 0.5028\n",
            "Epoch 150: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6916 - accuracy: 0.5010 - val_loss: 0.6934 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "history = model1.fit(train_generator, epochs=150, validation_data=test_generator, shuffle=False, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh_8sTtbQQGm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "61724dc1-9509-4dd7-efd2-0705f048f2e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plotHist(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb4VHlWhQQKk"
      },
      "outputs": [],
      "source": [
        "model1 = tf.keras.models.load_model(\"/content/clas_logs\\model1.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfyohhFVQQMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104163fd-e70d-47c6-bd28-0435f85dca51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 1s 6ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model1.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRdSk7iWQQOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d637d3d-66f0-4208-a096-cc5e44fc36e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.55898005],\n",
              "       [0.55576694],\n",
              "       [0.55722904],\n",
              "       [0.56497806],\n",
              "       [0.56124616],\n",
              "       [0.54846686],\n",
              "       [0.5528972 ],\n",
              "       [0.55185914],\n",
              "       [0.5459508 ],\n",
              "       [0.55582273],\n",
              "       [0.54945374],\n",
              "       [0.5495547 ],\n",
              "       [0.5465469 ],\n",
              "       [0.5479709 ],\n",
              "       [0.560831  ],\n",
              "       [0.5550411 ],\n",
              "       [0.55641633],\n",
              "       [0.5574602 ],\n",
              "       [0.5595236 ],\n",
              "       [0.5468044 ],\n",
              "       [0.54597414],\n",
              "       [0.55339026],\n",
              "       [0.5544524 ],\n",
              "       [0.55281126],\n",
              "       [0.55561167],\n",
              "       [0.5495525 ],\n",
              "       [0.5496601 ],\n",
              "       [0.54751575],\n",
              "       [0.5529989 ],\n",
              "       [0.5439819 ],\n",
              "       [0.5668669 ],\n",
              "       [0.5602032 ],\n",
              "       [0.5600237 ],\n",
              "       [0.56336004],\n",
              "       [0.5528577 ],\n",
              "       [0.5517632 ],\n",
              "       [0.5492976 ],\n",
              "       [0.5720175 ],\n",
              "       [0.5360272 ],\n",
              "       [0.54002595],\n",
              "       [0.5384144 ],\n",
              "       [0.5425414 ],\n",
              "       [0.5631926 ],\n",
              "       [0.562437  ],\n",
              "       [0.5704126 ],\n",
              "       [0.5689704 ],\n",
              "       [0.5437416 ],\n",
              "       [0.5411199 ],\n",
              "       [0.5444777 ],\n",
              "       [0.5583447 ],\n",
              "       [0.54506654],\n",
              "       [0.5202925 ],\n",
              "       [0.5293627 ],\n",
              "       [0.52154684],\n",
              "       [0.5439342 ],\n",
              "       [0.49986917],\n",
              "       [0.5131401 ],\n",
              "       [0.53040934],\n",
              "       [0.51482457],\n",
              "       [0.5074994 ],\n",
              "       [0.5141269 ],\n",
              "       [0.5259067 ],\n",
              "       [0.51448554],\n",
              "       [0.5254721 ],\n",
              "       [0.5331046 ],\n",
              "       [0.5237028 ],\n",
              "       [0.5264135 ],\n",
              "       [0.5312602 ],\n",
              "       [0.5389687 ],\n",
              "       [0.55358267],\n",
              "       [0.52495486],\n",
              "       [0.5252693 ],\n",
              "       [0.51802343],\n",
              "       [0.5498637 ],\n",
              "       [0.52140254],\n",
              "       [0.52049094],\n",
              "       [0.527521  ],\n",
              "       [0.55625474],\n",
              "       [0.5671216 ],\n",
              "       [0.56769645],\n",
              "       [0.5805428 ],\n",
              "       [0.5743986 ],\n",
              "       [0.5695517 ],\n",
              "       [0.5831895 ],\n",
              "       [0.57680786],\n",
              "       [0.5844303 ],\n",
              "       [0.57839864],\n",
              "       [0.5736578 ],\n",
              "       [0.5692897 ],\n",
              "       [0.5621581 ],\n",
              "       [0.56865144],\n",
              "       [0.56442636],\n",
              "       [0.58130985],\n",
              "       [0.5629113 ],\n",
              "       [0.56955755],\n",
              "       [0.5597305 ],\n",
              "       [0.56214523],\n",
              "       [0.5761744 ],\n",
              "       [0.5664379 ],\n",
              "       [0.5746139 ],\n",
              "       [0.58041847],\n",
              "       [0.5814056 ],\n",
              "       [0.58401036],\n",
              "       [0.59370345],\n",
              "       [0.5970168 ],\n",
              "       [0.5730921 ],\n",
              "       [0.5806834 ],\n",
              "       [0.5684063 ],\n",
              "       [0.579679  ],\n",
              "       [0.57037205],\n",
              "       [0.5632871 ],\n",
              "       [0.5642695 ],\n",
              "       [0.5607229 ],\n",
              "       [0.55771637],\n",
              "       [0.5645187 ],\n",
              "       [0.5762466 ],\n",
              "       [0.5644249 ],\n",
              "       [0.568907  ],\n",
              "       [0.5553514 ],\n",
              "       [0.57229966],\n",
              "       [0.56458604],\n",
              "       [0.57092434],\n",
              "       [0.5668591 ],\n",
              "       [0.5610668 ],\n",
              "       [0.56712496],\n",
              "       [0.56773984],\n",
              "       [0.5823113 ],\n",
              "       [0.5743338 ],\n",
              "       [0.5653931 ],\n",
              "       [0.5587858 ],\n",
              "       [0.5653049 ],\n",
              "       [0.5601401 ],\n",
              "       [0.5707641 ],\n",
              "       [0.5769283 ],\n",
              "       [0.5679487 ],\n",
              "       [0.56362134],\n",
              "       [0.5628009 ],\n",
              "       [0.5773562 ],\n",
              "       [0.56653476],\n",
              "       [0.5846471 ],\n",
              "       [0.5835786 ],\n",
              "       [0.52678317],\n",
              "       [0.5430566 ],\n",
              "       [0.53270614],\n",
              "       [0.55727303],\n",
              "       [0.53361565],\n",
              "       [0.5494865 ],\n",
              "       [0.52133214],\n",
              "       [0.54000396],\n",
              "       [0.52659667],\n",
              "       [0.53473246],\n",
              "       [0.558347  ],\n",
              "       [0.5150009 ],\n",
              "       [0.54582673],\n",
              "       [0.516886  ],\n",
              "       [0.5421016 ],\n",
              "       [0.52280223],\n",
              "       [0.5455386 ],\n",
              "       [0.5263558 ],\n",
              "       [0.52405864],\n",
              "       [0.5336669 ],\n",
              "       [0.5138261 ],\n",
              "       [0.54675746],\n",
              "       [0.54461765],\n",
              "       [0.5256768 ],\n",
              "       [0.5233247 ],\n",
              "       [0.5217653 ],\n",
              "       [0.53529096],\n",
              "       [0.5218796 ],\n",
              "       [0.5621198 ],\n",
              "       [0.52330184],\n",
              "       [0.533609  ],\n",
              "       [0.53226876],\n",
              "       [0.56701267],\n",
              "       [0.57467115],\n",
              "       [0.5625347 ],\n",
              "       [0.5671188 ],\n",
              "       [0.56168807],\n",
              "       [0.5597937 ],\n",
              "       [0.56608725],\n",
              "       [0.5621131 ],\n",
              "       [0.5614453 ],\n",
              "       [0.56581235],\n",
              "       [0.5712515 ],\n",
              "       [0.56194097],\n",
              "       [0.569182  ],\n",
              "       [0.5736229 ],\n",
              "       [0.56563854],\n",
              "       [0.5619656 ],\n",
              "       [0.56504333],\n",
              "       [0.5643952 ],\n",
              "       [0.5756662 ],\n",
              "       [0.57223487],\n",
              "       [0.5731581 ],\n",
              "       [0.56477   ],\n",
              "       [0.56874627],\n",
              "       [0.5640731 ],\n",
              "       [0.5800372 ],\n",
              "       [0.5749271 ],\n",
              "       [0.5705099 ],\n",
              "       [0.57174563],\n",
              "       [0.57180005],\n",
              "       [0.58318883],\n",
              "       [0.58019775],\n",
              "       [0.5793432 ],\n",
              "       [0.5742003 ],\n",
              "       [0.58530605],\n",
              "       [0.57423496],\n",
              "       [0.58040583],\n",
              "       [0.57417685],\n",
              "       [0.57671255],\n",
              "       [0.5696228 ],\n",
              "       [0.5716032 ],\n",
              "       [0.57863253],\n",
              "       [0.58504975],\n",
              "       [0.57917446],\n",
              "       [0.56819004],\n",
              "       [0.5745415 ],\n",
              "       [0.5711233 ],\n",
              "       [0.56722456],\n",
              "       [0.5856466 ],\n",
              "       [0.57815   ],\n",
              "       [0.58157563],\n",
              "       [0.577184  ],\n",
              "       [0.5760636 ],\n",
              "       [0.5810344 ],\n",
              "       [0.5764656 ],\n",
              "       [0.58771646],\n",
              "       [0.5802834 ],\n",
              "       [0.5771078 ],\n",
              "       [0.56876963],\n",
              "       [0.56600475],\n",
              "       [0.55981565],\n",
              "       [0.5739225 ],\n",
              "       [0.5751891 ],\n",
              "       [0.57901186],\n",
              "       [0.5716889 ],\n",
              "       [0.56522536],\n",
              "       [0.56690305],\n",
              "       [0.57970786],\n",
              "       [0.5833342 ],\n",
              "       [0.5819479 ],\n",
              "       [0.5871406 ],\n",
              "       [0.58392364],\n",
              "       [0.57540596],\n",
              "       [0.58518296],\n",
              "       [0.5861788 ],\n",
              "       [0.6000842 ],\n",
              "       [0.58567154],\n",
              "       [0.5823905 ],\n",
              "       [0.57338166],\n",
              "       [0.5786548 ],\n",
              "       [0.59323084],\n",
              "       [0.5860364 ],\n",
              "       [0.5851413 ],\n",
              "       [0.5783342 ],\n",
              "       [0.57714814],\n",
              "       [0.55731833],\n",
              "       [0.57310724],\n",
              "       [0.56971395],\n",
              "       [0.5878265 ],\n",
              "       [0.57441986],\n",
              "       [0.57156897],\n",
              "       [0.57354903],\n",
              "       [0.5729032 ],\n",
              "       [0.56387746],\n",
              "       [0.5702244 ],\n",
              "       [0.58633935],\n",
              "       [0.57086873],\n",
              "       [0.55418766],\n",
              "       [0.55115193],\n",
              "       [0.5521423 ],\n",
              "       [0.5662999 ],\n",
              "       [0.5598006 ],\n",
              "       [0.54577696],\n",
              "       [0.5408205 ],\n",
              "       [0.5337376 ],\n",
              "       [0.56225324],\n",
              "       [0.536283  ],\n",
              "       [0.56009996],\n",
              "       [0.573382  ],\n",
              "       [0.54287267],\n",
              "       [0.5433116 ],\n",
              "       [0.55456775],\n",
              "       [0.58173424],\n",
              "       [0.5581274 ],\n",
              "       [0.5620717 ],\n",
              "       [0.54072326],\n",
              "       [0.540688  ],\n",
              "       [0.5478956 ],\n",
              "       [0.5647935 ],\n",
              "       [0.5693964 ],\n",
              "       [0.5512134 ],\n",
              "       [0.54480934],\n",
              "       [0.5360688 ],\n",
              "       [0.5429428 ],\n",
              "       [0.5495726 ],\n",
              "       [0.55100167],\n",
              "       [0.5489233 ],\n",
              "       [0.5468559 ],\n",
              "       [0.5468207 ],\n",
              "       [0.5381071 ],\n",
              "       [0.5580865 ],\n",
              "       [0.5545781 ],\n",
              "       [0.5657503 ],\n",
              "       [0.55201924],\n",
              "       [0.55072284],\n",
              "       [0.5583006 ],\n",
              "       [0.54957956],\n",
              "       [0.55704796],\n",
              "       [0.5677257 ],\n",
              "       [0.56858593],\n",
              "       [0.5585629 ],\n",
              "       [0.5838928 ],\n",
              "       [0.57772213],\n",
              "       [0.5947475 ],\n",
              "       [0.5702162 ],\n",
              "       [0.5697473 ],\n",
              "       [0.57071555],\n",
              "       [0.5783152 ],\n",
              "       [0.57336426],\n",
              "       [0.56907034],\n",
              "       [0.56005615],\n",
              "       [0.56516695],\n",
              "       [0.5597328 ],\n",
              "       [0.56798184],\n",
              "       [0.5811363 ],\n",
              "       [0.57112515],\n",
              "       [0.5669307 ],\n",
              "       [0.5539975 ],\n",
              "       [0.5672554 ],\n",
              "       [0.5754952 ],\n",
              "       [0.58271277],\n",
              "       [0.58548594],\n",
              "       [0.57409596],\n",
              "       [0.5566476 ],\n",
              "       [0.5752727 ],\n",
              "       [0.5959375 ],\n",
              "       [0.5872499 ],\n",
              "       [0.5646566 ],\n",
              "       [0.56309056],\n",
              "       [0.557144  ],\n",
              "       [0.5711192 ],\n",
              "       [0.57281506],\n",
              "       [0.5698774 ],\n",
              "       [0.56789964],\n",
              "       [0.56311685],\n",
              "       [0.55231166],\n",
              "       [0.56411743],\n",
              "       [0.56905806],\n",
              "       [0.5891175 ],\n",
              "       [0.5805225 ],\n",
              "       [0.5631832 ],\n",
              "       [0.5782487 ],\n",
              "       [0.5454985 ],\n",
              "       [0.56198394],\n",
              "       [0.5531594 ],\n",
              "       [0.57014203],\n",
              "       [0.5477045 ],\n",
              "       [0.5494749 ],\n",
              "       [0.561281  ],\n",
              "       [0.56641394],\n",
              "       [0.57195884],\n",
              "       [0.5574098 ],\n",
              "       [0.57742953],\n",
              "       [0.54439163],\n",
              "       [0.55310845],\n",
              "       [0.54477334],\n",
              "       [0.5395278 ],\n",
              "       [0.53729844],\n",
              "       [0.539645  ],\n",
              "       [0.51997596],\n",
              "       [0.49526906],\n",
              "       [0.5162517 ],\n",
              "       [0.523412  ],\n",
              "       [0.5044455 ],\n",
              "       [0.50227976],\n",
              "       [0.5077741 ],\n",
              "       [0.5299982 ],\n",
              "       [0.49219665],\n",
              "       [0.52075815],\n",
              "       [0.51286995],\n",
              "       [0.5031439 ],\n",
              "       [0.50164807],\n",
              "       [0.5021703 ],\n",
              "       [0.541634  ],\n",
              "       [0.51387376],\n",
              "       [0.5232446 ],\n",
              "       [0.4944737 ],\n",
              "       [0.49649084],\n",
              "       [0.5107231 ],\n",
              "       [0.52718335],\n",
              "       [0.53152883],\n",
              "       [0.51569664],\n",
              "       [0.50092685],\n",
              "       [0.499893  ],\n",
              "       [0.49261135],\n",
              "       [0.5196213 ],\n",
              "       [0.5167532 ],\n",
              "       [0.51171494],\n",
              "       [0.50863326],\n",
              "       [0.56158644],\n",
              "       [0.55500233],\n",
              "       [0.56049454],\n",
              "       [0.56234425],\n",
              "       [0.5576684 ],\n",
              "       [0.55992436],\n",
              "       [0.5525532 ],\n",
              "       [0.5572814 ],\n",
              "       [0.5591897 ],\n",
              "       [0.55494523],\n",
              "       [0.5613607 ],\n",
              "       [0.5601075 ],\n",
              "       [0.57195604],\n",
              "       [0.5604438 ],\n",
              "       [0.5718766 ],\n",
              "       [0.5593066 ],\n",
              "       [0.57282645],\n",
              "       [0.5667115 ],\n",
              "       [0.57030433],\n",
              "       [0.5751493 ],\n",
              "       [0.56919837],\n",
              "       [0.5723478 ],\n",
              "       [0.5682334 ],\n",
              "       [0.5751805 ],\n",
              "       [0.57217455],\n",
              "       [0.56543076],\n",
              "       [0.56594485],\n",
              "       [0.5664278 ],\n",
              "       [0.5713887 ],\n",
              "       [0.5670614 ],\n",
              "       [0.5764206 ],\n",
              "       [0.56116784],\n",
              "       [0.5703085 ],\n",
              "       [0.5675459 ],\n",
              "       [0.56997406],\n",
              "       [0.56476265],\n",
              "       [0.57256144],\n",
              "       [0.56519246],\n",
              "       [0.56465834],\n",
              "       [0.5727494 ],\n",
              "       [0.5816819 ],\n",
              "       [0.5752517 ],\n",
              "       [0.5662464 ],\n",
              "       [0.57567614],\n",
              "       [0.5642146 ],\n",
              "       [0.5751053 ],\n",
              "       [0.5719255 ],\n",
              "       [0.5825628 ],\n",
              "       [0.57432   ],\n",
              "       [0.57455605],\n",
              "       [0.5738227 ],\n",
              "       [0.579355  ],\n",
              "       [0.5696944 ],\n",
              "       [0.5754946 ],\n",
              "       [0.5718533 ],\n",
              "       [0.5767497 ],\n",
              "       [0.5708003 ],\n",
              "       [0.5690059 ],\n",
              "       [0.5822444 ],\n",
              "       [0.5780662 ],\n",
              "       [0.5733654 ],\n",
              "       [0.57354045],\n",
              "       [0.5677325 ],\n",
              "       [0.57684577],\n",
              "       [0.57905763],\n",
              "       [0.58489466],\n",
              "       [0.5803294 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4IG99a4QQQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc313bc-111c-4f12-9701-3b6ebd2a891a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(468, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI7rxWqyQQUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2272e7f4-65cb-4be7-c174-07786544fbab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(468,)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "x_test[:,1][win_len:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC53IFTiQQWU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "eac065b3-a828-4567-cd7c-1a38b7ed8638"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pred  Actual\n",
              "0   1.0     1.0\n",
              "1   1.0     1.0\n",
              "2   1.0     0.0\n",
              "3   1.0     1.0\n",
              "4   1.0     0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed54faa3-3d88-4e0f-a64b-4e4e71839724\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed54faa3-3d88-4e0f-a64b-4e4e71839724')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ed54faa3-3d88-4e0f-a64b-4e4e71839724 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ed54faa3-3d88-4e0f-a64b-4e4e71839724');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df_pred = pd.concat([pd.DataFrame(np.round(predictions)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f26pnoC5QyNo"
      },
      "outputs": [],
      "source": [
        "#function to print accuracy and MCC of the model\n",
        "def evaluation(df : pd.DataFrame):\n",
        "    conf = pd.crosstab(df[\"Actual\"], df[\"Pred\"])\n",
        "    fig = plt.figure(figsize = (6,4))\n",
        "    sn.heatmap(conf, annot = True, cmap = \"Blues\", fmt = \"g\")\n",
        "    plt.title(\"Confussion Matrix\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    TP = conf[1][1]\n",
        "    FN = conf[0][1]\n",
        "    FP = conf[1][0]\n",
        "    TN = conf[0][0]\n",
        "    Acc = (TP+TN)/(TP+TN+FN+FP)\n",
        "    Mcc = (TP*TN - FP*FN) / np.sqrt( (TP + FP)*(TP + FN)*(TN + FP)*(TN + FN) )\n",
        "    print(\"Accuracy =\",Acc)\n",
        "    print(\"MCC =\",Mcc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0S90xP7QyXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "96d8199d-6bfc-4ff1-de18-d8b15d18ebf1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.5213675213675214\n",
            "MCC = 0.0565221930986891\n"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aueKMz0dRPLD"
      },
      "source": [
        "<center><h2>Model 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irv8bjkYRcIx",
        "outputId": "872a618a-24fd-43d9-cf13-88e515e821aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 30, 12)      24          ['input_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 30, 12)      142812      ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_4[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 30, 12)      0           ['dropout_13[0][0]',             \n",
            " mbda)                                                            'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 30, 4)        0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 30, 12)       60          ['dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 30, 12)      0           ['conv1d_9[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_9[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 30, 12)      142812      ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 30, 12)      0           ['dropout_15[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 30, 4)        0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 30, 12)       60          ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 30, 12)      0           ['conv1d_11[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_11[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 30, 12)      142812      ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 30, 12)      0           ['dropout_17[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 30, 4)        0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 30, 12)       60          ['dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 30, 12)      0           ['conv1d_13[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 30, 12)      142812      ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 30, 12)      0           ['dropout_19[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 30, 4)        0           ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 30, 12)       60          ['dropout_20[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 30, 12)      0           ['conv1d_15[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_15[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, 30, 12)      142812      ['layer_normalization_16[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_8[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (TFOpL  (None, 30, 12)      0           ['dropout_21[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_15[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_16[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 30, 4)        0           ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 30, 12)       60          ['dropout_22[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_17 (TFOpL  (None, 30, 12)      0           ['conv1d_17[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_17[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 30, 12)      142812      ['layer_normalization_18[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_9[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (TFOpL  (None, 30, 12)      0           ['dropout_23[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_18[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 30, 4)        0           ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 30, 12)       60          ['dropout_24[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_19 (TFOpL  (None, 30, 12)      0           ['conv1d_19[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 30)          0           ['tf.__operators__.add_19[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 200)          6200        ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 200)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            201         ['dropout_25[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 864,233\n",
            "Trainable params: 864,233\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 7s 51ms/step - loss: 4.5902 - val_loss: 0.7016\n",
            "Epoch 2/150\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 3.5698 - val_loss: 7.3965\n",
            "Epoch 3/150\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 3.8327 - val_loss: 0.7523\n",
            "Epoch 4/150\n",
            "46/46 [==============================] - 3s 54ms/step - loss: 3.5272 - val_loss: 0.7304\n",
            "Epoch 5/150\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 3.5556 - val_loss: 0.9798\n",
            "Epoch 6/150\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 2.7355 - val_loss: 0.7924\n",
            "Epoch 7/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.7414 - val_loss: 0.7065\n",
            "Epoch 8/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 2.6143 - val_loss: 0.7077\n",
            "Epoch 9/150\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 2.5690 - val_loss: 0.7087\n",
            "Epoch 10/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.4208 - val_loss: 0.7069\n",
            "Epoch 11/150\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 2.1243 - val_loss: 0.7061\n",
            "Epoch 12/150\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 2.3487 - val_loss: 0.7057\n",
            "Epoch 13/150\n",
            "46/46 [==============================] - 2s 44ms/step - loss: 2.2326 - val_loss: 0.7004\n",
            "Epoch 14/150\n",
            "46/46 [==============================] - 3s 55ms/step - loss: 2.4784 - val_loss: 0.7156\n",
            "Epoch 15/150\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 2.0527 - val_loss: 0.7044\n",
            "Epoch 16/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.0704 - val_loss: 0.7127\n",
            "Epoch 17/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.7140 - val_loss: 0.7034\n",
            "Epoch 18/150\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 1.7601 - val_loss: 0.7094\n",
            "Epoch 19/150\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 1.7164 - val_loss: 0.7079\n",
            "Epoch 20/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.5529 - val_loss: 0.6985\n",
            "Epoch 21/150\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 1.7657 - val_loss: 0.6982\n",
            "Epoch 22/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4644 - val_loss: 0.6982\n",
            "Epoch 23/150\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 1.5400 - val_loss: 0.6970\n",
            "Epoch 24/150\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 1.4608 - val_loss: 0.7025\n",
            "Epoch 25/150\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 7.0043 - val_loss: 7.9432\n",
            "Epoch 26/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 4.1322 - val_loss: 0.6977\n",
            "Epoch 27/150\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 1.9456 - val_loss: 0.7054\n",
            "Epoch 28/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 1.7512 - val_loss: 1.2337\n",
            "Epoch 29/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.0054 - val_loss: 0.6945\n",
            "Epoch 30/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.6167 - val_loss: 0.7181\n",
            "Epoch 31/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.5543 - val_loss: 0.7093\n",
            "Epoch 32/150\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 2.4019 - val_loss: 7.2620\n",
            "Epoch 33/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 5.2859 - val_loss: 7.3965\n",
            "Epoch 34/150\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 4.9920 - val_loss: 7.3965\n",
            "Epoch 35/150\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 4.4551 - val_loss: 0.8292\n",
            "Epoch 36/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.6066 - val_loss: 1.5596\n",
            "Epoch 37/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.0824 - val_loss: 1.0558\n",
            "Epoch 38/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.1544 - val_loss: 0.9686\n",
            "Epoch 39/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.6685 - val_loss: 0.7493\n",
            "Epoch 40/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.8921 - val_loss: 0.6913\n",
            "Epoch 41/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.7031 - val_loss: 0.7423\n",
            "Epoch 42/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.5398 - val_loss: 0.7144\n",
            "Epoch 43/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4796 - val_loss: 0.6959\n",
            "Epoch 44/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.7719 - val_loss: 0.7038\n",
            "Epoch 45/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.5959 - val_loss: 0.8556\n",
            "Epoch 46/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.6585 - val_loss: 0.7504\n",
            "Epoch 47/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.9019 - val_loss: 0.9825\n",
            "Epoch 48/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4336 - val_loss: 0.6924\n",
            "Epoch 49/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4747 - val_loss: 0.6945\n",
            "Epoch 50/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.2548 - val_loss: 0.9912\n",
            "Epoch 51/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.8303 - val_loss: 7.3965\n",
            "Epoch 52/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 4.7732 - val_loss: 7.3965\n",
            "Epoch 53/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.4090 - val_loss: 0.6913\n",
            "Epoch 54/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.9693 - val_loss: 0.7290\n",
            "Epoch 55/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.9021 - val_loss: 0.7078\n",
            "Epoch 56/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 1.8510 - val_loss: 0.6936\n",
            "Epoch 57/150\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 1.9064 - val_loss: 0.8327\n",
            "Epoch 58/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.6080 - val_loss: 0.7361\n",
            "Epoch 59/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4299 - val_loss: 0.7202\n",
            "Epoch 60/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4534 - val_loss: 0.7322\n",
            "Epoch 61/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4495 - val_loss: 0.7316\n",
            "Epoch 62/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.2925 - val_loss: 0.7222\n",
            "Epoch 63/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.5972 - val_loss: 0.6918\n",
            "Epoch 64/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.2962 - val_loss: 0.6916\n",
            "Epoch 65/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.2161 - val_loss: 0.6912\n",
            "Epoch 66/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.1942 - val_loss: 0.6910\n",
            "Epoch 67/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.2158 - val_loss: 0.6911\n",
            "Epoch 68/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.1335 - val_loss: 0.7020\n",
            "Epoch 69/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.2339 - val_loss: 0.6931\n",
            "Epoch 70/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.0625 - val_loss: 0.6909\n",
            "Epoch 71/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.0485 - val_loss: 7.9432\n",
            "Epoch 72/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 5.8374 - val_loss: 7.9432\n",
            "Epoch 73/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 5.2939 - val_loss: 0.9719\n",
            "Epoch 74/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 4.9392 - val_loss: 1.0542\n",
            "Epoch 75/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 4.9169 - val_loss: 0.7780\n",
            "Epoch 76/150\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 4.7690 - val_loss: 7.3965\n",
            "Epoch 77/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 4.5468 - val_loss: 0.8042\n",
            "Epoch 78/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 4.7627 - val_loss: 7.9217\n",
            "Epoch 79/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 4.2639 - val_loss: 1.4805\n",
            "Epoch 80/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 4.2398 - val_loss: 1.1096\n",
            "Epoch 81/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 4.0335 - val_loss: 1.0430\n",
            "Epoch 82/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.8815 - val_loss: 1.1971\n",
            "Epoch 83/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.8404 - val_loss: 0.8842\n",
            "Epoch 84/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.5062 - val_loss: 0.7839\n",
            "Epoch 85/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.1973 - val_loss: 0.7023\n",
            "Epoch 86/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.6808 - val_loss: 0.6983\n",
            "Epoch 87/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.0894 - val_loss: 0.6976\n",
            "Epoch 88/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.5991 - val_loss: 0.7275\n",
            "Epoch 89/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.2758 - val_loss: 0.7031\n",
            "Epoch 90/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.7013 - val_loss: 0.7255\n",
            "Epoch 91/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.7695 - val_loss: 0.7207\n",
            "Epoch 92/150\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 2.0496 - val_loss: 0.8619\n",
            "Epoch 93/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.8198 - val_loss: 0.7491\n",
            "Epoch 94/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.7737 - val_loss: 0.8405\n",
            "Epoch 95/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.1226 - val_loss: 0.7565\n",
            "Epoch 96/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.4727 - val_loss: 0.9299\n",
            "Epoch 97/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.3813 - val_loss: 0.7641\n",
            "Epoch 98/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.8655 - val_loss: 0.7526\n",
            "Epoch 99/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.9161 - val_loss: 0.7920\n",
            "Epoch 100/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.8781 - val_loss: 0.8610\n",
            "Epoch 101/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.7969 - val_loss: 0.7707\n",
            "Epoch 102/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.7439 - val_loss: 0.7204\n",
            "Epoch 103/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.5117 - val_loss: 0.7127\n",
            "Epoch 104/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.5405 - val_loss: 0.7286\n",
            "Epoch 105/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4571 - val_loss: 0.7211\n",
            "Epoch 106/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.5507 - val_loss: 0.7580\n",
            "Epoch 107/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.6082 - val_loss: 1.0101\n",
            "Epoch 108/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.4700 - val_loss: 0.8713\n",
            "Epoch 109/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.3262 - val_loss: 0.7529\n",
            "Epoch 110/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.1951 - val_loss: 0.7026\n",
            "Epoch 111/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.1267 - val_loss: 0.7012\n",
            "Epoch 112/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.0616 - val_loss: 0.7083\n",
            "Epoch 113/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.0693 - val_loss: 0.7033\n",
            "Epoch 114/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.9638 - val_loss: 7.3965\n",
            "Epoch 115/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.8900 - val_loss: 0.6990\n",
            "Epoch 116/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9577 - val_loss: 0.7566\n",
            "Epoch 117/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.1881 - val_loss: 2.5960\n",
            "Epoch 118/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.3737 - val_loss: 0.7138\n",
            "Epoch 119/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9761 - val_loss: 0.7073\n",
            "Epoch 120/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9338 - val_loss: 0.7236\n",
            "Epoch 121/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9047 - val_loss: 0.7127\n",
            "Epoch 122/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.9226 - val_loss: 0.7352\n",
            "Epoch 123/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9296 - val_loss: 0.7286\n",
            "Epoch 124/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9078 - val_loss: 0.7132\n",
            "Epoch 125/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8661 - val_loss: 0.6991\n",
            "Epoch 126/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8871 - val_loss: 0.7260\n",
            "Epoch 127/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8951 - val_loss: 0.7675\n",
            "Epoch 128/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8616 - val_loss: 0.6959\n",
            "Epoch 129/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8356 - val_loss: 0.7293\n",
            "Epoch 130/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.8396 - val_loss: 0.7146\n",
            "Epoch 131/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.8454 - val_loss: 0.7029\n",
            "Epoch 132/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.8719 - val_loss: 0.6956\n",
            "Epoch 133/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.8195 - val_loss: 0.7381\n",
            "Epoch 134/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8253 - val_loss: 0.6941\n",
            "Epoch 135/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8686 - val_loss: 0.7093\n",
            "Epoch 136/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8202 - val_loss: 0.6949\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8052 - val_loss: 0.7004\n",
            "Epoch 138/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7757 - val_loss: 0.7157\n",
            "Epoch 139/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.8132 - val_loss: 0.7267\n",
            "Epoch 140/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.8103 - val_loss: 0.6962\n",
            "Epoch 141/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7920 - val_loss: 0.6952\n",
            "Epoch 142/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.8098 - val_loss: 0.7124\n",
            "Epoch 143/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7747 - val_loss: 0.7136\n",
            "Epoch 144/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7957 - val_loss: 0.7082\n",
            "Epoch 145/150\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7810 - val_loss: 0.7199\n",
            "Epoch 146/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7674 - val_loss: 0.7134\n",
            "Epoch 147/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7718 - val_loss: 0.7237\n",
            "Epoch 148/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8085 - val_loss: 0.7058\n",
            "Epoch 149/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7755 - val_loss: 0.7048\n",
            "Epoch 150/150\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9206 - val_loss: 0.7232\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6fc3574650>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# input_shape = x_train.shape[1:]\n",
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model2 = build_model(\n",
        "    input_shape,\n",
        "    head_size=350,\n",
        "    num_heads=8,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=6,\n",
        "    mlp_units=[200],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model2.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "model2.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, \\\n",
        "    restore_best_weights=True)]\n",
        "\n",
        "# model.fit(\n",
        "#     train_generator,\n",
        "#     y_train,\n",
        "#     validation_split=0.2,\n",
        "#     epochs=200,\n",
        "#     batch_size=64,\n",
        "#     callbacks=callbacks,\n",
        "# )\n",
        "\n",
        "model2.fit_generator(train_generator, epochs=150, validation_data=test_generator)\n",
        "\n",
        "# model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7Z2TjLtRcLq"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model2.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model2.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pNFbLZ7RcOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6fc903-7005-4fc8-fae9-d225557c07a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 1.2528 - accuracy: 0.5076\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51496, saving model to clas_logs\\model2.hdf5\n",
            "46/46 [==============================] - 8s 56ms/step - loss: 1.2443 - accuracy: 0.5086 - val_loss: 1.2750 - val_accuracy: 0.5150\n",
            "Epoch 2/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 1.3799 - accuracy: 0.4840\n",
            "Epoch 2: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.3727 - accuracy: 0.4839 - val_loss: 0.8832 - val_accuracy: 0.4850\n",
            "Epoch 3/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 1.8154 - accuracy: 0.4944\n",
            "Epoch 3: val_accuracy improved from 0.51496 to 0.52778, saving model to clas_logs\\model2.hdf5\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 1.8130 - accuracy: 0.4949 - val_loss: 0.6935 - val_accuracy: 0.5278\n",
            "Epoch 4/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 1.7991 - accuracy: 0.4847\n",
            "Epoch 4: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.8731 - accuracy: 0.4825 - val_loss: 1.3665 - val_accuracy: 0.5150\n",
            "Epoch 5/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 5.0284 - accuracy: 0.5063\n",
            "Epoch 5: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 5.0750 - accuracy: 0.5044 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 6/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 2.7970 - accuracy: 0.4958\n",
            "Epoch 6: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 2.7787 - accuracy: 0.4962 - val_loss: 0.6976 - val_accuracy: 0.5107\n",
            "Epoch 7/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 2.6876 - accuracy: 0.5104\n",
            "Epoch 7: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 2.6569 - accuracy: 0.5120 - val_loss: 0.7887 - val_accuracy: 0.4850\n",
            "Epoch 8/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.9398 - accuracy: 0.5132\n",
            "Epoch 8: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9366 - accuracy: 0.5154 - val_loss: 0.7964 - val_accuracy: 0.4850\n",
            "Epoch 9/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 1.0648 - accuracy: 0.5014\n",
            "Epoch 9: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 1.0598 - accuracy: 0.5024 - val_loss: 0.8545 - val_accuracy: 0.4850\n",
            "Epoch 10/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.8980 - accuracy: 0.4965\n",
            "Epoch 10: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9034 - accuracy: 0.4990 - val_loss: 0.8185 - val_accuracy: 0.4850\n",
            "Epoch 11/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.8938 - accuracy: 0.4882\n",
            "Epoch 11: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.9003 - accuracy: 0.4901 - val_loss: 0.8396 - val_accuracy: 0.4850\n",
            "Epoch 12/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.9085 - accuracy: 0.5090\n",
            "Epoch 12: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.9191 - accuracy: 0.5072 - val_loss: 0.8044 - val_accuracy: 0.4850\n",
            "Epoch 13/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.8169 - accuracy: 0.5049\n",
            "Epoch 13: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8181 - accuracy: 0.5044 - val_loss: 0.7726 - val_accuracy: 0.4850\n",
            "Epoch 14/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.8222 - accuracy: 0.5021\n",
            "Epoch 14: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8197 - accuracy: 0.5031 - val_loss: 0.8595 - val_accuracy: 0.4850\n",
            "Epoch 15/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7860 - accuracy: 0.5000\n",
            "Epoch 15: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7963 - accuracy: 0.4990 - val_loss: 0.7747 - val_accuracy: 0.4850\n",
            "Epoch 16/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7903 - accuracy: 0.4972\n",
            "Epoch 16: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7899 - accuracy: 0.4956 - val_loss: 0.9084 - val_accuracy: 0.4850\n",
            "Epoch 17/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.8648 - accuracy: 0.4569\n",
            "Epoch 17: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.8634 - accuracy: 0.4572 - val_loss: 0.8945 - val_accuracy: 0.4850\n",
            "Epoch 18/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7661 - accuracy: 0.4931\n",
            "Epoch 18: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7659 - accuracy: 0.4956 - val_loss: 0.8491 - val_accuracy: 0.4850\n",
            "Epoch 19/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7537 - accuracy: 0.5000\n",
            "Epoch 19: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7651 - accuracy: 0.4976 - val_loss: 0.8237 - val_accuracy: 0.4850\n",
            "Epoch 20/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.8329 - accuracy: 0.4806\n",
            "Epoch 20: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.8327 - accuracy: 0.4805 - val_loss: 0.7620 - val_accuracy: 0.4850\n",
            "Epoch 21/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7844 - accuracy: 0.5132\n",
            "Epoch 21: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7865 - accuracy: 0.5120 - val_loss: 0.8752 - val_accuracy: 0.4850\n",
            "Epoch 22/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7894 - accuracy: 0.4910\n",
            "Epoch 22: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7901 - accuracy: 0.4908 - val_loss: 0.7249 - val_accuracy: 0.4872\n",
            "Epoch 23/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7869 - accuracy: 0.4792\n",
            "Epoch 23: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7884 - accuracy: 0.4805 - val_loss: 0.8335 - val_accuracy: 0.4850\n",
            "Epoch 24/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7775 - accuracy: 0.5007\n",
            "Epoch 24: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7761 - accuracy: 0.5003 - val_loss: 0.8335 - val_accuracy: 0.4850\n",
            "Epoch 25/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7569 - accuracy: 0.4938\n",
            "Epoch 25: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7567 - accuracy: 0.4949 - val_loss: 0.8542 - val_accuracy: 0.4850\n",
            "Epoch 26/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7571 - accuracy: 0.5076\n",
            "Epoch 26: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7571 - accuracy: 0.5065 - val_loss: 0.8475 - val_accuracy: 0.4850\n",
            "Epoch 27/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7239 - accuracy: 0.5042\n",
            "Epoch 27: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7242 - accuracy: 0.5044 - val_loss: 0.7785 - val_accuracy: 0.4850\n",
            "Epoch 28/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7469 - accuracy: 0.4903\n",
            "Epoch 28: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7473 - accuracy: 0.4908 - val_loss: 0.7843 - val_accuracy: 0.4850\n",
            "Epoch 29/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7779 - accuracy: 0.4646\n",
            "Epoch 29: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7756 - accuracy: 0.4682 - val_loss: 0.8348 - val_accuracy: 0.4850\n",
            "Epoch 30/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7337 - accuracy: 0.5139\n",
            "Epoch 30: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.7327 - accuracy: 0.5133 - val_loss: 0.8324 - val_accuracy: 0.4850\n",
            "Epoch 31/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7453 - accuracy: 0.5051\n",
            "Epoch 31: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 46ms/step - loss: 0.7453 - accuracy: 0.5051 - val_loss: 0.8437 - val_accuracy: 0.4850\n",
            "Epoch 32/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7318 - accuracy: 0.4910\n",
            "Epoch 32: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7324 - accuracy: 0.4901 - val_loss: 0.7797 - val_accuracy: 0.4850\n",
            "Epoch 33/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7316 - accuracy: 0.5181\n",
            "Epoch 33: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7317 - accuracy: 0.5168 - val_loss: 0.7530 - val_accuracy: 0.4850\n",
            "Epoch 34/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7457 - accuracy: 0.4729\n",
            "Epoch 34: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7449 - accuracy: 0.4736 - val_loss: 0.8394 - val_accuracy: 0.4850\n",
            "Epoch 35/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7533 - accuracy: 0.4826\n",
            "Epoch 35: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7515 - accuracy: 0.4846 - val_loss: 0.8173 - val_accuracy: 0.4850\n",
            "Epoch 36/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7349 - accuracy: 0.4931\n",
            "Epoch 36: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7316 - accuracy: 0.4969 - val_loss: 0.7269 - val_accuracy: 0.4850\n",
            "Epoch 37/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7430 - accuracy: 0.5042\n",
            "Epoch 37: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.7441 - accuracy: 0.5017 - val_loss: 0.8057 - val_accuracy: 0.4850\n",
            "Epoch 38/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7405 - accuracy: 0.5069\n",
            "Epoch 38: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.7397 - accuracy: 0.5072 - val_loss: 0.8374 - val_accuracy: 0.4850\n",
            "Epoch 39/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7138 - accuracy: 0.5222\n",
            "Epoch 39: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7142 - accuracy: 0.5209 - val_loss: 0.8139 - val_accuracy: 0.4850\n",
            "Epoch 40/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7336 - accuracy: 0.5035\n",
            "Epoch 40: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7326 - accuracy: 0.5065 - val_loss: 0.8049 - val_accuracy: 0.4850\n",
            "Epoch 41/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7292 - accuracy: 0.4965\n",
            "Epoch 41: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7298 - accuracy: 0.4969 - val_loss: 0.8113 - val_accuracy: 0.4850\n",
            "Epoch 42/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7335 - accuracy: 0.4965\n",
            "Epoch 42: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7345 - accuracy: 0.4956 - val_loss: 0.8113 - val_accuracy: 0.4850\n",
            "Epoch 43/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7224 - accuracy: 0.5132\n",
            "Epoch 43: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7222 - accuracy: 0.5147 - val_loss: 0.8081 - val_accuracy: 0.4850\n",
            "Epoch 44/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7308 - accuracy: 0.4868\n",
            "Epoch 44: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7297 - accuracy: 0.4873 - val_loss: 0.7889 - val_accuracy: 0.4850\n",
            "Epoch 45/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7305 - accuracy: 0.5042\n",
            "Epoch 45: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7302 - accuracy: 0.5024 - val_loss: 0.7880 - val_accuracy: 0.4850\n",
            "Epoch 46/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7345 - accuracy: 0.4986\n",
            "Epoch 46: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7337 - accuracy: 0.5003 - val_loss: 0.7728 - val_accuracy: 0.4850\n",
            "Epoch 47/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7295 - accuracy: 0.4840\n",
            "Epoch 47: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7274 - accuracy: 0.4880 - val_loss: 0.7352 - val_accuracy: 0.4850\n",
            "Epoch 48/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7313 - accuracy: 0.5118\n",
            "Epoch 48: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7324 - accuracy: 0.5106 - val_loss: 0.8036 - val_accuracy: 0.4850\n",
            "Epoch 49/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7119 - accuracy: 0.5111\n",
            "Epoch 49: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7110 - accuracy: 0.5120 - val_loss: 0.7349 - val_accuracy: 0.4850\n",
            "Epoch 50/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7192 - accuracy: 0.5083\n",
            "Epoch 50: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7193 - accuracy: 0.5079 - val_loss: 0.7730 - val_accuracy: 0.4850\n",
            "Epoch 51/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7307 - accuracy: 0.4854\n",
            "Epoch 51: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7303 - accuracy: 0.4873 - val_loss: 0.7337 - val_accuracy: 0.4850\n",
            "Epoch 52/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7156 - accuracy: 0.5153\n",
            "Epoch 52: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7148 - accuracy: 0.5161 - val_loss: 0.7691 - val_accuracy: 0.4850\n",
            "Epoch 53/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7233 - accuracy: 0.5056\n",
            "Epoch 53: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7225 - accuracy: 0.5058 - val_loss: 0.7925 - val_accuracy: 0.4850\n",
            "Epoch 54/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7093 - accuracy: 0.5188\n",
            "Epoch 54: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7096 - accuracy: 0.5175 - val_loss: 0.7554 - val_accuracy: 0.4850\n",
            "Epoch 55/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7419 - accuracy: 0.4757\n",
            "Epoch 55: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7415 - accuracy: 0.4771 - val_loss: 0.7636 - val_accuracy: 0.4850\n",
            "Epoch 56/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7116 - accuracy: 0.5236\n",
            "Epoch 56: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7122 - accuracy: 0.5209 - val_loss: 0.7842 - val_accuracy: 0.4850\n",
            "Epoch 57/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7122 - accuracy: 0.5194\n",
            "Epoch 57: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7123 - accuracy: 0.5209 - val_loss: 0.7679 - val_accuracy: 0.4850\n",
            "Epoch 58/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7174 - accuracy: 0.5236\n",
            "Epoch 58: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7162 - accuracy: 0.5243 - val_loss: 0.7572 - val_accuracy: 0.4850\n",
            "Epoch 59/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7072 - accuracy: 0.5160\n",
            "Epoch 59: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7077 - accuracy: 0.5175 - val_loss: 0.7380 - val_accuracy: 0.4850\n",
            "Epoch 60/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7261 - accuracy: 0.5153\n",
            "Epoch 60: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7254 - accuracy: 0.5175 - val_loss: 0.6971 - val_accuracy: 0.4786\n",
            "Epoch 61/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7189 - accuracy: 0.5035\n",
            "Epoch 61: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7188 - accuracy: 0.5038 - val_loss: 0.7265 - val_accuracy: 0.4850\n",
            "Epoch 62/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7136 - accuracy: 0.5063\n",
            "Epoch 62: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7143 - accuracy: 0.5058 - val_loss: 0.7507 - val_accuracy: 0.4850\n",
            "Epoch 63/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7227 - accuracy: 0.5035\n",
            "Epoch 63: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7204 - accuracy: 0.5065 - val_loss: 0.7524 - val_accuracy: 0.4850\n",
            "Epoch 64/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7178 - accuracy: 0.5035\n",
            "Epoch 64: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7184 - accuracy: 0.5003 - val_loss: 0.7403 - val_accuracy: 0.4850\n",
            "Epoch 65/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7542 - accuracy: 0.4958\n",
            "Epoch 65: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7552 - accuracy: 0.4942 - val_loss: 0.7148 - val_accuracy: 0.4850\n",
            "Epoch 66/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7354 - accuracy: 0.4938\n",
            "Epoch 66: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7347 - accuracy: 0.4956 - val_loss: 0.7650 - val_accuracy: 0.4850\n",
            "Epoch 67/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7219 - accuracy: 0.4979\n",
            "Epoch 67: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7217 - accuracy: 0.5010 - val_loss: 0.7637 - val_accuracy: 0.4850\n",
            "Epoch 68/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7189 - accuracy: 0.5063\n",
            "Epoch 68: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7189 - accuracy: 0.5051 - val_loss: 0.7429 - val_accuracy: 0.4850\n",
            "Epoch 69/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7114 - accuracy: 0.5111\n",
            "Epoch 69: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7099 - accuracy: 0.5133 - val_loss: 0.7434 - val_accuracy: 0.4850\n",
            "Epoch 70/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7100 - accuracy: 0.4986\n",
            "Epoch 70: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7095 - accuracy: 0.5010 - val_loss: 0.7330 - val_accuracy: 0.4850\n",
            "Epoch 71/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7077 - accuracy: 0.5264\n",
            "Epoch 71: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7075 - accuracy: 0.5284 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 72/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7088 - accuracy: 0.5306\n",
            "Epoch 72: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7088 - accuracy: 0.5311 - val_loss: 0.7025 - val_accuracy: 0.4850\n",
            "Epoch 73/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.4903\n",
            "Epoch 73: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7076 - accuracy: 0.4942 - val_loss: 0.7029 - val_accuracy: 0.4829\n",
            "Epoch 74/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7208 - accuracy: 0.5056\n",
            "Epoch 74: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7189 - accuracy: 0.5072 - val_loss: 0.7335 - val_accuracy: 0.4850\n",
            "Epoch 75/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7193 - accuracy: 0.4931\n",
            "Epoch 75: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7189 - accuracy: 0.4928 - val_loss: 0.7383 - val_accuracy: 0.4850\n",
            "Epoch 76/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7006 - accuracy: 0.5375\n",
            "Epoch 76: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7000 - accuracy: 0.5387 - val_loss: 0.7405 - val_accuracy: 0.4850\n",
            "Epoch 77/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7134 - accuracy: 0.5174\n",
            "Epoch 77: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7133 - accuracy: 0.5195 - val_loss: 0.7174 - val_accuracy: 0.4850\n",
            "Epoch 78/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7170 - accuracy: 0.5076\n",
            "Epoch 78: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7163 - accuracy: 0.5092 - val_loss: 0.7265 - val_accuracy: 0.4850\n",
            "Epoch 79/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7036 - accuracy: 0.5160\n",
            "Epoch 79: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7034 - accuracy: 0.5161 - val_loss: 0.7118 - val_accuracy: 0.4850\n",
            "Epoch 80/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7174 - accuracy: 0.4958\n",
            "Epoch 80: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7173 - accuracy: 0.4935 - val_loss: 0.7485 - val_accuracy: 0.4850\n",
            "Epoch 81/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7184 - accuracy: 0.4882\n",
            "Epoch 81: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7176 - accuracy: 0.4894 - val_loss: 0.6959 - val_accuracy: 0.4679\n",
            "Epoch 82/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7148 - accuracy: 0.4931\n",
            "Epoch 82: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7137 - accuracy: 0.4956 - val_loss: 0.7054 - val_accuracy: 0.4850\n",
            "Epoch 83/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7212 - accuracy: 0.5035\n",
            "Epoch 83: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7209 - accuracy: 0.5044 - val_loss: 0.6999 - val_accuracy: 0.4893\n",
            "Epoch 84/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7071 - accuracy: 0.5056\n",
            "Epoch 84: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7079 - accuracy: 0.5051 - val_loss: 0.6933 - val_accuracy: 0.5150\n",
            "Epoch 85/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7195 - accuracy: 0.4986\n",
            "Epoch 85: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7194 - accuracy: 0.4990 - val_loss: 0.7329 - val_accuracy: 0.4850\n",
            "Epoch 86/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7074 - accuracy: 0.5201\n",
            "Epoch 86: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7073 - accuracy: 0.5202 - val_loss: 0.7065 - val_accuracy: 0.4850\n",
            "Epoch 87/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7185 - accuracy: 0.4965\n",
            "Epoch 87: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7182 - accuracy: 0.4962 - val_loss: 0.7020 - val_accuracy: 0.4808\n",
            "Epoch 88/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7194 - accuracy: 0.4951\n",
            "Epoch 88: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7190 - accuracy: 0.4949 - val_loss: 0.7107 - val_accuracy: 0.4850\n",
            "Epoch 89/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7170 - accuracy: 0.4875\n",
            "Epoch 89: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7157 - accuracy: 0.4873 - val_loss: 0.7111 - val_accuracy: 0.4850\n",
            "Epoch 90/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7112 - accuracy: 0.5167\n",
            "Epoch 90: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7115 - accuracy: 0.5140 - val_loss: 0.7004 - val_accuracy: 0.4850\n",
            "Epoch 91/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7109 - accuracy: 0.5076\n",
            "Epoch 91: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7112 - accuracy: 0.5058 - val_loss: 0.7289 - val_accuracy: 0.4850\n",
            "Epoch 92/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7165 - accuracy: 0.5090\n",
            "Epoch 92: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7164 - accuracy: 0.5099 - val_loss: 0.7177 - val_accuracy: 0.4850\n",
            "Epoch 93/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7187 - accuracy: 0.4958\n",
            "Epoch 93: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7177 - accuracy: 0.4969 - val_loss: 0.7073 - val_accuracy: 0.4850\n",
            "Epoch 94/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7115 - accuracy: 0.5097\n",
            "Epoch 94: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7111 - accuracy: 0.5113 - val_loss: 0.7002 - val_accuracy: 0.5107\n",
            "Epoch 95/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7199 - accuracy: 0.4833\n",
            "Epoch 95: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7193 - accuracy: 0.4846 - val_loss: 0.6947 - val_accuracy: 0.4829\n",
            "Epoch 96/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.5083\n",
            "Epoch 96: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7099 - accuracy: 0.5086 - val_loss: 0.7145 - val_accuracy: 0.4850\n",
            "Epoch 97/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7453 - accuracy: 0.5243\n",
            "Epoch 97: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7447 - accuracy: 0.5236 - val_loss: 0.7232 - val_accuracy: 0.4850\n",
            "Epoch 98/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7200 - accuracy: 0.5007\n",
            "Epoch 98: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7207 - accuracy: 0.4983 - val_loss: 0.6972 - val_accuracy: 0.4957\n",
            "Epoch 99/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7090 - accuracy: 0.5167\n",
            "Epoch 99: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7108 - accuracy: 0.5161 - val_loss: 0.7458 - val_accuracy: 0.4850\n",
            "Epoch 100/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7401 - accuracy: 0.4840\n",
            "Epoch 100: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7398 - accuracy: 0.4853 - val_loss: 0.6943 - val_accuracy: 0.4979\n",
            "Epoch 101/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7256 - accuracy: 0.5049\n",
            "Epoch 101: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7268 - accuracy: 0.5031 - val_loss: 0.7236 - val_accuracy: 0.4850\n",
            "Epoch 102/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7244 - accuracy: 0.4958\n",
            "Epoch 102: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7257 - accuracy: 0.4935 - val_loss: 0.7375 - val_accuracy: 0.4850\n",
            "Epoch 103/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7113 - accuracy: 0.5250\n",
            "Epoch 103: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7107 - accuracy: 0.5270 - val_loss: 0.7029 - val_accuracy: 0.4850\n",
            "Epoch 104/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7104 - accuracy: 0.5056\n",
            "Epoch 104: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7101 - accuracy: 0.5058 - val_loss: 0.7003 - val_accuracy: 0.4850\n",
            "Epoch 105/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7058 - accuracy: 0.5250\n",
            "Epoch 105: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 0.7058 - accuracy: 0.5236 - val_loss: 0.7202 - val_accuracy: 0.4850\n",
            "Epoch 106/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7095 - accuracy: 0.5111\n",
            "Epoch 106: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7092 - accuracy: 0.5113 - val_loss: 0.6975 - val_accuracy: 0.4893\n",
            "Epoch 107/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7125 - accuracy: 0.4944\n",
            "Epoch 107: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7133 - accuracy: 0.4921 - val_loss: 0.7048 - val_accuracy: 0.4850\n",
            "Epoch 108/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7085 - accuracy: 0.4972\n",
            "Epoch 108: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7085 - accuracy: 0.4962 - val_loss: 0.7071 - val_accuracy: 0.4850\n",
            "Epoch 109/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7165 - accuracy: 0.4861\n",
            "Epoch 109: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7167 - accuracy: 0.4860 - val_loss: 0.6957 - val_accuracy: 0.4957\n",
            "Epoch 110/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7052 - accuracy: 0.5153\n",
            "Epoch 110: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7052 - accuracy: 0.5133 - val_loss: 0.6984 - val_accuracy: 0.4701\n",
            "Epoch 111/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6997 - accuracy: 0.5319\n",
            "Epoch 111: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7007 - accuracy: 0.5284 - val_loss: 0.7032 - val_accuracy: 0.4850\n",
            "Epoch 112/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7044 - accuracy: 0.5000\n",
            "Epoch 112: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7035 - accuracy: 0.5024 - val_loss: 0.7018 - val_accuracy: 0.4872\n",
            "Epoch 113/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7157 - accuracy: 0.5181\n",
            "Epoch 113: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7158 - accuracy: 0.5195 - val_loss: 0.7351 - val_accuracy: 0.4850\n",
            "Epoch 114/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7019 - accuracy: 0.5243\n",
            "Epoch 114: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7018 - accuracy: 0.5236 - val_loss: 0.6950 - val_accuracy: 0.4979\n",
            "Epoch 115/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7135 - accuracy: 0.4917\n",
            "Epoch 115: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7135 - accuracy: 0.4901 - val_loss: 0.7014 - val_accuracy: 0.4850\n",
            "Epoch 116/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7036 - accuracy: 0.5368\n",
            "Epoch 116: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7046 - accuracy: 0.5373 - val_loss: 0.7141 - val_accuracy: 0.4808\n",
            "Epoch 117/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7033 - accuracy: 0.5181\n",
            "Epoch 117: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7030 - accuracy: 0.5181 - val_loss: 0.7109 - val_accuracy: 0.4850\n",
            "Epoch 118/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7171 - accuracy: 0.5236\n",
            "Epoch 118: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7172 - accuracy: 0.5250 - val_loss: 0.7374 - val_accuracy: 0.5150\n",
            "Epoch 119/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7421 - accuracy: 0.4938\n",
            "Epoch 119: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.7410 - accuracy: 0.4942 - val_loss: 0.7166 - val_accuracy: 0.4679\n",
            "Epoch 120/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 3.3373 - accuracy: 0.5194\n",
            "Epoch 120: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.3624 - accuracy: 0.5216 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 4.4860 - accuracy: 0.5347\n",
            "Epoch 121: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 4.4945 - accuracy: 0.5366 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 4.2335 - accuracy: 0.5208\n",
            "Epoch 122: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 4.2265 - accuracy: 0.5188 - val_loss: 0.8523 - val_accuracy: 0.5150\n",
            "Epoch 123/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.5764 - accuracy: 0.4500\n",
            "Epoch 123: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.6027 - accuracy: 0.4483 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 124/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.6597 - accuracy: 0.5215\n",
            "Epoch 124: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.7012 - accuracy: 0.5195 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 125/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 3.6762 - accuracy: 0.5139\n",
            "Epoch 125: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 3.6967 - accuracy: 0.5161 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.4737 - accuracy: 0.5382\n",
            "Epoch 126: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.4448 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.0095 - accuracy: 0.5354\n",
            "Epoch 127: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.0115 - accuracy: 0.5352 - val_loss: 4.5497 - val_accuracy: 0.4850\n",
            "Epoch 128/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 5.3761 - accuracy: 0.5083\n",
            "Epoch 128: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 5.3733 - accuracy: 0.5099 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 5.9614 - accuracy: 0.5174\n",
            "Epoch 129: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.0236 - accuracy: 0.5147 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 130/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0310 - accuracy: 0.5021\n",
            "Epoch 130: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 7.0030 - accuracy: 0.5044 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9610 - accuracy: 0.5424\n",
            "Epoch 131: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9340 - accuracy: 0.5441 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0013 - accuracy: 0.5410\n",
            "Epoch 132: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9737 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0221 - accuracy: 0.5396\n",
            "Epoch 133: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9942 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0129 - accuracy: 0.5382\n",
            "Epoch 134: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9852 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0135 - accuracy: 0.5382\n",
            "Epoch 135: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9858 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0318 - accuracy: 0.5389\n",
            "Epoch 136: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 7.0038 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0017 - accuracy: 0.5396\n",
            "Epoch 137: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 3s 61ms/step - loss: 6.9741 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9846 - accuracy: 0.5414\n",
            "Epoch 138: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 6.9846 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0014 - accuracy: 0.5396\n",
            "Epoch 139: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9738 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0141 - accuracy: 0.5389\n",
            "Epoch 140: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9863 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0473 - accuracy: 0.5368\n",
            "Epoch 141: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0191 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9371 - accuracy: 0.5417\n",
            "Epoch 142: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9338 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9420 - accuracy: 0.5437\n",
            "Epoch 143: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9152 - accuracy: 0.5455 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0256 - accuracy: 0.5382\n",
            "Epoch 144: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9977 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.7278 - accuracy: 0.5222\n",
            "Epoch 145: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.7044 - accuracy: 0.5243 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9395 - accuracy: 0.5368\n",
            "Epoch 146: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9028 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9908 - accuracy: 0.5319\n",
            "Epoch 147: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9529 - accuracy: 0.5346 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9024 - accuracy: 0.5160\n",
            "Epoch 148: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.8882 - accuracy: 0.5175 - val_loss: 1.5832 - val_accuracy: 0.4850\n",
            "Epoch 149/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.4196 - accuracy: 0.4667\n",
            "Epoch 149: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.4499 - accuracy: 0.4654 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 150/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.2621 - accuracy: 0.4590\n",
            "Epoch 150: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 8.2911 - accuracy: 0.4572 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 151/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.1785 - accuracy: 0.4646\n",
            "Epoch 151: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 8.2086 - accuracy: 0.4627 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 152/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.2844 - accuracy: 0.4576\n",
            "Epoch 152: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 8.3026 - accuracy: 0.4565 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 153/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.1889 - accuracy: 0.4646\n",
            "Epoch 153: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 8.2084 - accuracy: 0.4634 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 154/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3992 - accuracy: 0.4479\n",
            "Epoch 154: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 8.4050 - accuracy: 0.4476 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 155/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.8039 - accuracy: 0.4812\n",
            "Epoch 155: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 7.8296 - accuracy: 0.4791 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 156/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.8812 - accuracy: 0.4722\n",
            "Epoch 156: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.9049 - accuracy: 0.4709 - val_loss: 0.7263 - val_accuracy: 0.4850\n",
            "Epoch 157/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.4889 - accuracy: 0.4958\n",
            "Epoch 157: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.4547 - accuracy: 0.4983 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 158/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0904 - accuracy: 0.5243\n",
            "Epoch 158: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0618 - accuracy: 0.5264 - val_loss: 6.3586 - val_accuracy: 0.5171\n",
            "Epoch 159/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.7516 - accuracy: 0.5444\n",
            "Epoch 159: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.7295 - accuracy: 0.5455 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 160/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8448 - accuracy: 0.5403\n",
            "Epoch 160: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.8227 - accuracy: 0.5414 - val_loss: 6.6103 - val_accuracy: 0.5085\n",
            "Epoch 161/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8406 - accuracy: 0.5347\n",
            "Epoch 161: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.8154 - accuracy: 0.5366 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 162/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0065 - accuracy: 0.5299\n",
            "Epoch 162: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9895 - accuracy: 0.5311 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 163/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9065 - accuracy: 0.5299\n",
            "Epoch 163: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9016 - accuracy: 0.5305 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 164/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.7161 - accuracy: 0.5361\n",
            "Epoch 164: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.7037 - accuracy: 0.5373 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 165/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.6652 - accuracy: 0.5299\n",
            "Epoch 165: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.6542 - accuracy: 0.5305 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 166/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8105 - accuracy: 0.5160\n",
            "Epoch 166: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.8068 - accuracy: 0.5168 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 167/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0227 - accuracy: 0.5389\n",
            "Epoch 167: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9948 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 168/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0211 - accuracy: 0.5396\n",
            "Epoch 168: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9933 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 169/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0105 - accuracy: 0.5403\n",
            "Epoch 169: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9828 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 170/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9900 - accuracy: 0.5410\n",
            "Epoch 170: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9626 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 171/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 171: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 172/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0104 - accuracy: 0.5403\n",
            "Epoch 172: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 173/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0317 - accuracy: 0.5389\n",
            "Epoch 173: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 174/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0007 - accuracy: 0.5403\n",
            "Epoch 174: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9731 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 175/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0535 - accuracy: 0.5375\n",
            "Epoch 175: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0252 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 176/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 176: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 177/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0425 - accuracy: 0.5382\n",
            "Epoch 177: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0039 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 178/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9892 - accuracy: 0.5417\n",
            "Epoch 178: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9618 - accuracy: 0.5435 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 179/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0320 - accuracy: 0.5389\n",
            "Epoch 179: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 7.0039 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 180/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0320 - accuracy: 0.5389\n",
            "Epoch 180: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0039 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 181/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9900 - accuracy: 0.5417\n",
            "Epoch 181: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9626 - accuracy: 0.5435 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 182/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0107 - accuracy: 0.5403\n",
            "Epoch 182: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9829 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 183/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0764 - accuracy: 0.5354\n",
            "Epoch 183: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0373 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 184/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0864 - accuracy: 0.5347\n",
            "Epoch 184: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0576 - accuracy: 0.5366 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 185/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9298 - accuracy: 0.5451\n",
            "Epoch 185: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9349 - accuracy: 0.5448 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 186/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.4532 - accuracy: 0.4497\n",
            "Epoch 186: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 48ms/step - loss: 8.4532 - accuracy: 0.4497 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 187/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3445 - accuracy: 0.4583\n",
            "Epoch 187: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 8.3723 - accuracy: 0.4565 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 188/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3120 - accuracy: 0.4611\n",
            "Epoch 188: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 8.3403 - accuracy: 0.4593 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 189/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3323 - accuracy: 0.4597\n",
            "Epoch 189: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 8.3498 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 190/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.2572 - accuracy: 0.4618\n",
            "Epoch 190: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 8.2546 - accuracy: 0.4620 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 191/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.1075 - accuracy: 0.4722\n",
            "Epoch 191: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 8.1084 - accuracy: 0.4716 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 192/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.4154 - accuracy: 0.5118\n",
            "Epoch 192: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.3924 - accuracy: 0.5133 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 193/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.1008 - accuracy: 0.5340\n",
            "Epoch 193: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0718 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 194/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0345 - accuracy: 0.5382\n",
            "Epoch 194: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0064 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 195/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0320 - accuracy: 0.5389\n",
            "Epoch 195: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 7.0039 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 196/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0211 - accuracy: 0.5396\n",
            "Epoch 196: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9933 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 197/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0424 - accuracy: 0.5382\n",
            "Epoch 197: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 7.0143 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 198/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0112 - accuracy: 0.5396\n",
            "Epoch 198: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9835 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 199/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9683 - accuracy: 0.5431\n",
            "Epoch 199: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9412 - accuracy: 0.5448 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 200/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0227 - accuracy: 0.5389\n",
            "Epoch 200: val_accuracy did not improve from 0.52778\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9948 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "history2 = model2.fit(train_generator, epochs=200, validation_data=test_generator, shuffle=False, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGinTOsWVEHx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "0bc24d73-ff11-46ed-bc3f-0ca055e8d2a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plotHist(history2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHlTxmBmVEMV"
      },
      "outputs": [],
      "source": [
        "model2 = tf.keras.models.load_model(\"/content/clas_logs\\model2.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRhAXlTtVEQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15765563-3eb1-4e97-f01d-a211a2437c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 1s 16ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions2 = model2.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNwIdi_sVZJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29b6f9a-c02c-4ca7-cfb1-23c20ac6304a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5535327 ],\n",
              "       [0.5506118 ],\n",
              "       [0.54666513],\n",
              "       [0.5483796 ],\n",
              "       [0.5456146 ],\n",
              "       [0.5352675 ],\n",
              "       [0.56208104],\n",
              "       [0.531432  ],\n",
              "       [0.52815723],\n",
              "       [0.55096555],\n",
              "       [0.5172801 ],\n",
              "       [0.5593196 ],\n",
              "       [0.503261  ],\n",
              "       [0.5423986 ],\n",
              "       [0.5512324 ],\n",
              "       [0.5178233 ],\n",
              "       [0.49532062],\n",
              "       [0.56557244],\n",
              "       [0.51217926],\n",
              "       [0.5119808 ],\n",
              "       [0.5325368 ],\n",
              "       [0.50876784],\n",
              "       [0.5410707 ],\n",
              "       [0.520125  ],\n",
              "       [0.54906374],\n",
              "       [0.50922567],\n",
              "       [0.53966725],\n",
              "       [0.5346074 ],\n",
              "       [0.5572576 ],\n",
              "       [0.53136027],\n",
              "       [0.5586558 ],\n",
              "       [0.5380905 ],\n",
              "       [0.56935483],\n",
              "       [0.5489971 ],\n",
              "       [0.5682056 ],\n",
              "       [0.5336119 ],\n",
              "       [0.5747488 ],\n",
              "       [0.57046336],\n",
              "       [0.55387723],\n",
              "       [0.5860741 ],\n",
              "       [0.5560387 ],\n",
              "       [0.5777422 ],\n",
              "       [0.5581887 ],\n",
              "       [0.5540468 ],\n",
              "       [0.5640474 ],\n",
              "       [0.5701609 ],\n",
              "       [0.5445462 ],\n",
              "       [0.56378955],\n",
              "       [0.5538661 ],\n",
              "       [0.5495286 ],\n",
              "       [0.5448681 ],\n",
              "       [0.53864115],\n",
              "       [0.5449723 ],\n",
              "       [0.5531805 ],\n",
              "       [0.5279329 ],\n",
              "       [0.5350409 ],\n",
              "       [0.53519005],\n",
              "       [0.4915546 ],\n",
              "       [0.5211506 ],\n",
              "       [0.49437156],\n",
              "       [0.5375981 ],\n",
              "       [0.4858876 ],\n",
              "       [0.48249772],\n",
              "       [0.5227791 ],\n",
              "       [0.5024382 ],\n",
              "       [0.49025458],\n",
              "       [0.49477088],\n",
              "       [0.46506462],\n",
              "       [0.5043126 ],\n",
              "       [0.49310148],\n",
              "       [0.47639543],\n",
              "       [0.46677962],\n",
              "       [0.49892431],\n",
              "       [0.51078516],\n",
              "       [0.44779715],\n",
              "       [0.53279984],\n",
              "       [0.49050814],\n",
              "       [0.49308693],\n",
              "       [0.5137044 ],\n",
              "       [0.50865597],\n",
              "       [0.51051265],\n",
              "       [0.49452603],\n",
              "       [0.49756688],\n",
              "       [0.522864  ],\n",
              "       [0.4987467 ],\n",
              "       [0.51275235],\n",
              "       [0.51453966],\n",
              "       [0.49670514],\n",
              "       [0.5192129 ],\n",
              "       [0.5029386 ],\n",
              "       [0.507873  ],\n",
              "       [0.5215933 ],\n",
              "       [0.52261215],\n",
              "       [0.51211536],\n",
              "       [0.528214  ],\n",
              "       [0.4874722 ],\n",
              "       [0.5418166 ],\n",
              "       [0.51271963],\n",
              "       [0.52482337],\n",
              "       [0.51867104],\n",
              "       [0.5312998 ],\n",
              "       [0.5160486 ],\n",
              "       [0.5363956 ],\n",
              "       [0.5265265 ],\n",
              "       [0.5331497 ],\n",
              "       [0.5127005 ],\n",
              "       [0.53635085],\n",
              "       [0.5345587 ],\n",
              "       [0.53215903],\n",
              "       [0.51242906],\n",
              "       [0.5352297 ],\n",
              "       [0.54150546],\n",
              "       [0.5075587 ],\n",
              "       [0.5094626 ],\n",
              "       [0.54510206],\n",
              "       [0.52783144],\n",
              "       [0.54842305],\n",
              "       [0.50650185],\n",
              "       [0.5540626 ],\n",
              "       [0.526346  ],\n",
              "       [0.49389344],\n",
              "       [0.53856355],\n",
              "       [0.5291854 ],\n",
              "       [0.51421237],\n",
              "       [0.52516574],\n",
              "       [0.52315617],\n",
              "       [0.54688597],\n",
              "       [0.5157349 ],\n",
              "       [0.5154881 ],\n",
              "       [0.5377363 ],\n",
              "       [0.49388197],\n",
              "       [0.5103689 ],\n",
              "       [0.5445768 ],\n",
              "       [0.5155446 ],\n",
              "       [0.53306884],\n",
              "       [0.5125971 ],\n",
              "       [0.5431154 ],\n",
              "       [0.51267636],\n",
              "       [0.5139883 ],\n",
              "       [0.5341292 ],\n",
              "       [0.53615355],\n",
              "       [0.5167731 ],\n",
              "       [0.5422684 ],\n",
              "       [0.51362216],\n",
              "       [0.5169467 ],\n",
              "       [0.5432171 ],\n",
              "       [0.5202609 ],\n",
              "       [0.541129  ],\n",
              "       [0.5156996 ],\n",
              "       [0.52350426],\n",
              "       [0.55062383],\n",
              "       [0.5412036 ],\n",
              "       [0.5250801 ],\n",
              "       [0.5153985 ],\n",
              "       [0.5370537 ],\n",
              "       [0.5342744 ],\n",
              "       [0.5125521 ],\n",
              "       [0.5275914 ],\n",
              "       [0.5145339 ],\n",
              "       [0.551642  ],\n",
              "       [0.49701545],\n",
              "       [0.5138806 ],\n",
              "       [0.52766967],\n",
              "       [0.5176763 ],\n",
              "       [0.49721995],\n",
              "       [0.56256574],\n",
              "       [0.50112754],\n",
              "       [0.5008302 ],\n",
              "       [0.5133384 ],\n",
              "       [0.5313866 ],\n",
              "       [0.4997233 ],\n",
              "       [0.50906175],\n",
              "       [0.5078924 ],\n",
              "       [0.5311539 ],\n",
              "       [0.5168002 ],\n",
              "       [0.509865  ],\n",
              "       [0.5135079 ],\n",
              "       [0.50704783],\n",
              "       [0.5098783 ],\n",
              "       [0.5319533 ],\n",
              "       [0.5088671 ],\n",
              "       [0.5230945 ],\n",
              "       [0.5174152 ],\n",
              "       [0.5175148 ],\n",
              "       [0.503394  ],\n",
              "       [0.53566396],\n",
              "       [0.5011001 ],\n",
              "       [0.52484   ],\n",
              "       [0.5021849 ],\n",
              "       [0.53586304],\n",
              "       [0.5144499 ],\n",
              "       [0.5301993 ],\n",
              "       [0.524065  ],\n",
              "       [0.5251894 ],\n",
              "       [0.5288575 ],\n",
              "       [0.5208989 ],\n",
              "       [0.49278018],\n",
              "       [0.5656358 ],\n",
              "       [0.5284574 ],\n",
              "       [0.52296644],\n",
              "       [0.5275442 ],\n",
              "       [0.51125866],\n",
              "       [0.54085296],\n",
              "       [0.53297853],\n",
              "       [0.51246274],\n",
              "       [0.54566205],\n",
              "       [0.5237769 ],\n",
              "       [0.5296368 ],\n",
              "       [0.53907347],\n",
              "       [0.5166007 ],\n",
              "       [0.5387639 ],\n",
              "       [0.51569575],\n",
              "       [0.51827735],\n",
              "       [0.5384625 ],\n",
              "       [0.5389052 ],\n",
              "       [0.5077901 ],\n",
              "       [0.5654593 ],\n",
              "       [0.529465  ],\n",
              "       [0.5058774 ],\n",
              "       [0.53283185],\n",
              "       [0.54505324],\n",
              "       [0.5240096 ],\n",
              "       [0.5239574 ],\n",
              "       [0.507064  ],\n",
              "       [0.53914404],\n",
              "       [0.5343334 ],\n",
              "       [0.5024243 ],\n",
              "       [0.5409445 ],\n",
              "       [0.51729476],\n",
              "       [0.54058576],\n",
              "       [0.53922546],\n",
              "       [0.5225331 ],\n",
              "       [0.5327328 ],\n",
              "       [0.5446059 ],\n",
              "       [0.5160341 ],\n",
              "       [0.57553715],\n",
              "       [0.522373  ],\n",
              "       [0.5396507 ],\n",
              "       [0.5572019 ],\n",
              "       [0.5557214 ],\n",
              "       [0.5377535 ],\n",
              "       [0.56179905],\n",
              "       [0.5613409 ],\n",
              "       [0.5493705 ],\n",
              "       [0.5663222 ],\n",
              "       [0.54997987],\n",
              "       [0.5712734 ],\n",
              "       [0.56629777],\n",
              "       [0.5638076 ],\n",
              "       [0.5674542 ],\n",
              "       [0.5647506 ],\n",
              "       [0.5556418 ],\n",
              "       [0.5832061 ],\n",
              "       [0.56012833],\n",
              "       [0.59198284],\n",
              "       [0.5674502 ],\n",
              "       [0.56935805],\n",
              "       [0.5942905 ],\n",
              "       [0.5749389 ],\n",
              "       [0.5730326 ],\n",
              "       [0.5810555 ],\n",
              "       [0.5777734 ],\n",
              "       [0.5821234 ],\n",
              "       [0.57400817],\n",
              "       [0.5789064 ],\n",
              "       [0.5605577 ],\n",
              "       [0.6051322 ],\n",
              "       [0.58303237],\n",
              "       [0.5531594 ],\n",
              "       [0.58543456],\n",
              "       [0.5788808 ],\n",
              "       [0.5615718 ],\n",
              "       [0.5842753 ],\n",
              "       [0.5559153 ],\n",
              "       [0.58321685],\n",
              "       [0.5854262 ],\n",
              "       [0.582002  ],\n",
              "       [0.5869967 ],\n",
              "       [0.5576456 ],\n",
              "       [0.6115891 ],\n",
              "       [0.5779212 ],\n",
              "       [0.5564596 ],\n",
              "       [0.5652771 ],\n",
              "       [0.58825606],\n",
              "       [0.55287135],\n",
              "       [0.5643196 ],\n",
              "       [0.5527632 ],\n",
              "       [0.5611585 ],\n",
              "       [0.55355716],\n",
              "       [0.5363956 ],\n",
              "       [0.5594421 ],\n",
              "       [0.5527179 ],\n",
              "       [0.5600254 ],\n",
              "       [0.52911913],\n",
              "       [0.54367846],\n",
              "       [0.53661966],\n",
              "       [0.558712  ],\n",
              "       [0.53754985],\n",
              "       [0.5486973 ],\n",
              "       [0.5384336 ],\n",
              "       [0.5430045 ],\n",
              "       [0.5447686 ],\n",
              "       [0.53597057],\n",
              "       [0.5489813 ],\n",
              "       [0.5295522 ],\n",
              "       [0.5307422 ],\n",
              "       [0.5552137 ],\n",
              "       [0.54042506],\n",
              "       [0.5065287 ],\n",
              "       [0.5415648 ],\n",
              "       [0.5568212 ],\n",
              "       [0.5239996 ],\n",
              "       [0.5236101 ],\n",
              "       [0.53618294],\n",
              "       [0.5488547 ],\n",
              "       [0.5465858 ],\n",
              "       [0.52492595],\n",
              "       [0.5426994 ],\n",
              "       [0.527532  ],\n",
              "       [0.54171455],\n",
              "       [0.53659976],\n",
              "       [0.52549773],\n",
              "       [0.5558523 ],\n",
              "       [0.5367821 ],\n",
              "       [0.51455736],\n",
              "       [0.55676025],\n",
              "       [0.5489022 ],\n",
              "       [0.5349202 ],\n",
              "       [0.5560064 ],\n",
              "       [0.5214447 ],\n",
              "       [0.55082124],\n",
              "       [0.5147142 ],\n",
              "       [0.5528017 ],\n",
              "       [0.5419566 ],\n",
              "       [0.53256404],\n",
              "       [0.50385314],\n",
              "       [0.5675652 ],\n",
              "       [0.53245574],\n",
              "       [0.5430405 ],\n",
              "       [0.5143786 ],\n",
              "       [0.53373057],\n",
              "       [0.5304304 ],\n",
              "       [0.55156887],\n",
              "       [0.52842426],\n",
              "       [0.5455786 ],\n",
              "       [0.5564887 ],\n",
              "       [0.5292936 ],\n",
              "       [0.56375223],\n",
              "       [0.5464162 ],\n",
              "       [0.56455034],\n",
              "       [0.54978955],\n",
              "       [0.5609206 ],\n",
              "       [0.56207603],\n",
              "       [0.5653609 ],\n",
              "       [0.53502905],\n",
              "       [0.57159287],\n",
              "       [0.57068497],\n",
              "       [0.56859875],\n",
              "       [0.5471236 ],\n",
              "       [0.56190413],\n",
              "       [0.57821846],\n",
              "       [0.57817197],\n",
              "       [0.54587096],\n",
              "       [0.58453274],\n",
              "       [0.5571771 ],\n",
              "       [0.5721322 ],\n",
              "       [0.5709132 ],\n",
              "       [0.56705326],\n",
              "       [0.58981556],\n",
              "       [0.5725946 ],\n",
              "       [0.5955077 ],\n",
              "       [0.5706699 ],\n",
              "       [0.58316493],\n",
              "       [0.57416666],\n",
              "       [0.58599234],\n",
              "       [0.56483465],\n",
              "       [0.57838726],\n",
              "       [0.5784535 ],\n",
              "       [0.57164013],\n",
              "       [0.5589837 ],\n",
              "       [0.6018953 ],\n",
              "       [0.58270997],\n",
              "       [0.5541321 ],\n",
              "       [0.5662789 ],\n",
              "       [0.5780038 ],\n",
              "       [0.5770887 ],\n",
              "       [0.5564707 ],\n",
              "       [0.5611644 ],\n",
              "       [0.5700797 ],\n",
              "       [0.53669024],\n",
              "       [0.54564255],\n",
              "       [0.55437803],\n",
              "       [0.54832643],\n",
              "       [0.5462556 ],\n",
              "       [0.54311264],\n",
              "       [0.5436433 ],\n",
              "       [0.533413  ],\n",
              "       [0.54467803],\n",
              "       [0.5428691 ],\n",
              "       [0.5108867 ],\n",
              "       [0.52592677],\n",
              "       [0.5147083 ],\n",
              "       [0.5143155 ],\n",
              "       [0.549865  ],\n",
              "       [0.48965353],\n",
              "       [0.5250578 ],\n",
              "       [0.52708524],\n",
              "       [0.504728  ],\n",
              "       [0.52158296],\n",
              "       [0.528919  ],\n",
              "       [0.47082052],\n",
              "       [0.53550696],\n",
              "       [0.49508196],\n",
              "       [0.5101978 ],\n",
              "       [0.5258624 ],\n",
              "       [0.5154749 ],\n",
              "       [0.51648504],\n",
              "       [0.5365083 ],\n",
              "       [0.48165086],\n",
              "       [0.5323317 ],\n",
              "       [0.5032272 ],\n",
              "       [0.52590066],\n",
              "       [0.49921507],\n",
              "       [0.5238714 ],\n",
              "       [0.52438825],\n",
              "       [0.5170192 ],\n",
              "       [0.5199173 ],\n",
              "       [0.5276475 ],\n",
              "       [0.5136428 ],\n",
              "       [0.5065735 ],\n",
              "       [0.52488446],\n",
              "       [0.508358  ],\n",
              "       [0.5320856 ],\n",
              "       [0.5148035 ],\n",
              "       [0.5356166 ],\n",
              "       [0.4913244 ],\n",
              "       [0.5469887 ],\n",
              "       [0.5314984 ],\n",
              "       [0.52785206],\n",
              "       [0.511408  ],\n",
              "       [0.5527207 ],\n",
              "       [0.5342058 ],\n",
              "       [0.52037996],\n",
              "       [0.55366904],\n",
              "       [0.5284975 ],\n",
              "       [0.52699494],\n",
              "       [0.55272824],\n",
              "       [0.5139357 ],\n",
              "       [0.5653998 ],\n",
              "       [0.5254338 ],\n",
              "       [0.5234013 ],\n",
              "       [0.5485818 ],\n",
              "       [0.5485415 ],\n",
              "       [0.5041996 ],\n",
              "       [0.5756233 ],\n",
              "       [0.5089384 ],\n",
              "       [0.56179744],\n",
              "       [0.5097984 ],\n",
              "       [0.5519741 ],\n",
              "       [0.5476257 ],\n",
              "       [0.5199043 ],\n",
              "       [0.53950584],\n",
              "       [0.5535846 ],\n",
              "       [0.5260613 ],\n",
              "       [0.5425813 ],\n",
              "       [0.5358735 ],\n",
              "       [0.5331649 ],\n",
              "       [0.536963  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "predictions2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmnQxsLTVR1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7219fc-7e2b-46d6-9038-6a8e97f71e95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(468, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "predictions2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs4KS_13VSR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5af0e2c-1009-4d11-ee11-78ef94adc5a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(468,)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "x_test[:,1][win_len:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARXtQAk_VTSR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a27218a4-93ba-4c92-8f62-8f7d28893654"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pred  Actual\n",
              "0   1.0     1.0\n",
              "1   1.0     1.0\n",
              "2   1.0     0.0\n",
              "3   1.0     1.0\n",
              "4   1.0     0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88776248-6d55-4490-a165-6fad3ab8d178\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88776248-6d55-4490-a165-6fad3ab8d178')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88776248-6d55-4490-a165-6fad3ab8d178 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88776248-6d55-4490-a165-6fad3ab8d178');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "df_pred2 = pd.concat([pd.DataFrame(np.round(predictions2)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred2.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAk4zyKoVkcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "fc0dc956-0805-44ca-b204-ae8446b093f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.5277777777777778\n",
            "MCC = 0.058927052285185404\n"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV7GOe1MVw7d"
      },
      "source": [
        "<center><h3>Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmoWgWwi2HyB",
        "outputId": "1bc96c95-05e2-47dd-cbde-3ec4c6319317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_106 (Layer  (None, 30, 12)      24          ['input_9[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_53 (Multi  (None, 30, 12)      206562      ['layer_normalization_106[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " dropout_144 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_53[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_106 (TFOp  (None, 30, 12)      0           ['dropout_144[0][0]',            \n",
            " Lambda)                                                          'input_9[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_107 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_106[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_106 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " dropout_145 (Dropout)          (None, 30, 9)        0           ['conv1d_106[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_107 (Conv1D)            (None, 30, 12)       120         ['dropout_145[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_107 (TFOp  (None, 30, 12)      0           ['conv1d_107[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_106[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_108 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_107[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_54 (Multi  (None, 30, 12)      206562      ['layer_normalization_108[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " dropout_146 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_54[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_108 (TFOp  (None, 30, 12)      0           ['dropout_146[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_107[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_109 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_108[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_108 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " dropout_147 (Dropout)          (None, 30, 9)        0           ['conv1d_108[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_109 (Conv1D)            (None, 30, 12)       120         ['dropout_147[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_109 (TFOp  (None, 30, 12)      0           ['conv1d_109[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_108[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_110 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_109[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_55 (Multi  (None, 30, 12)      206562      ['layer_normalization_110[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " dropout_148 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_55[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_110 (TFOp  (None, 30, 12)      0           ['dropout_148[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_109[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_111 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_110[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_110 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            " dropout_149 (Dropout)          (None, 30, 9)        0           ['conv1d_110[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_111 (Conv1D)            (None, 30, 12)       120         ['dropout_149[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_111 (TFOp  (None, 30, 12)      0           ['conv1d_111[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_110[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_112 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_111[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_56 (Multi  (None, 30, 12)      206562      ['layer_normalization_112[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " dropout_150 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_56[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_112 (TFOp  (None, 30, 12)      0           ['dropout_150[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_111[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_113 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_112[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_112 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_113[0][0]']\n",
            "                                                                                                  \n",
            " dropout_151 (Dropout)          (None, 30, 9)        0           ['conv1d_112[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_113 (Conv1D)            (None, 30, 12)       120         ['dropout_151[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_113 (TFOp  (None, 30, 12)      0           ['conv1d_113[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_112[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_114 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_113[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_57 (Multi  (None, 30, 12)      206562      ['layer_normalization_114[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_114[0][0]']\n",
            "                                                                                                  \n",
            " dropout_152 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_57[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_114 (TFOp  (None, 30, 12)      0           ['dropout_152[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_113[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_115 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_114[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_114 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " dropout_153 (Dropout)          (None, 30, 9)        0           ['conv1d_114[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_115 (Conv1D)            (None, 30, 12)       120         ['dropout_153[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_115 (TFOp  (None, 30, 12)      0           ['conv1d_115[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_114[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_116 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_115[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_58 (Multi  (None, 30, 12)      206562      ['layer_normalization_116[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_116[0][0]']\n",
            "                                                                                                  \n",
            " dropout_154 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_58[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_116 (TFOp  (None, 30, 12)      0           ['dropout_154[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_115[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_117 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_116[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_116 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_117[0][0]']\n",
            "                                                                                                  \n",
            " dropout_155 (Dropout)          (None, 30, 9)        0           ['conv1d_116[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_117 (Conv1D)            (None, 30, 12)       120         ['dropout_155[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_117 (TFOp  (None, 30, 12)      0           ['conv1d_117[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_116[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_118 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_117[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_59 (Multi  (None, 30, 12)      206562      ['layer_normalization_118[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " dropout_156 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_59[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_118 (TFOp  (None, 30, 12)      0           ['dropout_156[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_117[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_119 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_118[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_118 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_119[0][0]']\n",
            "                                                                                                  \n",
            " dropout_157 (Dropout)          (None, 30, 9)        0           ['conv1d_118[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_119 (Conv1D)            (None, 30, 12)       120         ['dropout_157[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_119 (TFOp  (None, 30, 12)      0           ['conv1d_119[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_118[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_120 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_119[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_60 (Multi  (None, 30, 12)      206562      ['layer_normalization_120[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_120[0][0]']\n",
            "                                                                                                  \n",
            " dropout_158 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_60[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_120 (TFOp  (None, 30, 12)      0           ['dropout_158[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_119[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_121 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_120[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_120 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " dropout_159 (Dropout)          (None, 30, 9)        0           ['conv1d_120[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_121 (Conv1D)            (None, 30, 12)       120         ['dropout_159[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_121 (TFOp  (None, 30, 12)      0           ['conv1d_121[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_120[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_122 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_121[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_61 (Multi  (None, 30, 12)      206562      ['layer_normalization_122[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_122[0][0]']\n",
            "                                                                                                  \n",
            " dropout_160 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_61[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_122 (TFOp  (None, 30, 12)      0           ['dropout_160[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_121[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_123 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_122[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_122 (Conv1D)            (None, 30, 9)        117         ['layer_normalization_123[0][0]']\n",
            "                                                                                                  \n",
            " dropout_161 (Dropout)          (None, 30, 9)        0           ['conv1d_122[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_123 (Conv1D)            (None, 30, 12)       120         ['dropout_161[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_123 (TFOp  (None, 30, 12)      0           ['conv1d_123[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_122[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_8 (Gl  (None, 30)          0           ['tf.__operators__.add_123[0][0]'\n",
            " obalAveragePooling1D)                                           ]                                \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 250)          7750        ['global_average_pooling1d_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_162 (Dropout)          (None, 250)          0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 1)            251         ['dropout_162[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,869,624\n",
            "Trainable params: 1,869,624\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 12s 90ms/step - loss: 4.5309 - val_loss: 7.2726\n",
            "Epoch 2/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 3.0184 - val_loss: 0.7068\n",
            "Epoch 3/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 3.0546 - val_loss: 0.7817\n",
            "Epoch 4/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 3.2859 - val_loss: 7.9432\n",
            "Epoch 5/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 7.4808 - val_loss: 7.9432\n",
            "Epoch 6/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.3625 - val_loss: 6.9615\n",
            "Epoch 7/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 5.3543 - val_loss: 4.4934\n",
            "Epoch 8/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 4.1506 - val_loss: 0.8429\n",
            "Epoch 9/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 2.4308 - val_loss: 0.8191\n",
            "Epoch 10/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 2.8294 - val_loss: 0.7188\n",
            "Epoch 11/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 2.3397 - val_loss: 0.7753\n",
            "Epoch 12/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.6772 - val_loss: 0.7765\n",
            "Epoch 13/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.9758 - val_loss: 1.0370\n",
            "Epoch 14/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.7275 - val_loss: 0.8283\n",
            "Epoch 15/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.9559 - val_loss: 0.7495\n",
            "Epoch 16/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 2.4348 - val_loss: 0.7932\n",
            "Epoch 17/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.9714 - val_loss: 0.7677\n",
            "Epoch 18/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.7074 - val_loss: 0.7500\n",
            "Epoch 19/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.2947 - val_loss: 0.7221\n",
            "Epoch 20/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 2.4951 - val_loss: 7.3965\n",
            "Epoch 21/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 2.1357 - val_loss: 0.6968\n",
            "Epoch 22/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.4082 - val_loss: 0.6965\n",
            "Epoch 23/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.3072 - val_loss: 0.6961\n",
            "Epoch 24/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.2342 - val_loss: 0.7193\n",
            "Epoch 25/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2125 - val_loss: 0.7102\n",
            "Epoch 26/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.1953 - val_loss: 0.7387\n",
            "Epoch 27/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 3.1725 - val_loss: 7.3965\n",
            "Epoch 28/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.7631 - val_loss: 7.3965\n",
            "Epoch 29/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.1824 - val_loss: 0.8011\n",
            "Epoch 30/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 2.5764 - val_loss: 0.9422\n",
            "Epoch 31/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 2.9340 - val_loss: 0.7085\n",
            "Epoch 32/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.9843 - val_loss: 0.7009\n",
            "Epoch 33/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.5774 - val_loss: 0.7594\n",
            "Epoch 34/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.3637 - val_loss: 0.6954\n",
            "Epoch 35/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.5752 - val_loss: 7.3965\n",
            "Epoch 36/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.2684 - val_loss: 7.3965\n",
            "Epoch 37/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 5.5204 - val_loss: 0.8831\n",
            "Epoch 38/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 3.6501 - val_loss: 0.8550\n",
            "Epoch 39/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.8321 - val_loss: 4.2777\n",
            "Epoch 40/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 2.9914 - val_loss: 0.7121\n",
            "Epoch 41/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 2.1502 - val_loss: 0.7178\n",
            "Epoch 42/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.4341 - val_loss: 0.7577\n",
            "Epoch 43/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.5183 - val_loss: 1.1981\n",
            "Epoch 44/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.5757 - val_loss: 2.8875\n",
            "Epoch 45/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.4674 - val_loss: 5.6487\n",
            "Epoch 46/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 5.2757 - val_loss: 7.3965\n",
            "Epoch 47/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 5.2296 - val_loss: 1.3475\n",
            "Epoch 48/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.9700 - val_loss: 0.9381\n",
            "Epoch 49/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.7615 - val_loss: 0.7192\n",
            "Epoch 50/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.8523 - val_loss: 0.6993\n",
            "Epoch 51/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.4400 - val_loss: 0.8667\n",
            "Epoch 52/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.4854 - val_loss: 0.7147\n",
            "Epoch 53/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.5687 - val_loss: 0.7328\n",
            "Epoch 54/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.3848 - val_loss: 0.7086\n",
            "Epoch 55/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.4000 - val_loss: 0.9118\n",
            "Epoch 56/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2599 - val_loss: 0.6999\n",
            "Epoch 57/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.6725 - val_loss: 7.3965\n",
            "Epoch 58/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 6.4807 - val_loss: 7.3965\n",
            "Epoch 59/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.2329 - val_loss: 7.3965\n",
            "Epoch 60/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 5.9510 - val_loss: 7.3965\n",
            "Epoch 61/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 5.1171 - val_loss: 0.8309\n",
            "Epoch 62/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 4.1570 - val_loss: 0.7765\n",
            "Epoch 63/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.6278 - val_loss: 0.8910\n",
            "Epoch 64/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.4458 - val_loss: 0.6984\n",
            "Epoch 65/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0781 - val_loss: 0.7222\n",
            "Epoch 66/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1754 - val_loss: 0.6958\n",
            "Epoch 67/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1182 - val_loss: 0.6982\n",
            "Epoch 68/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0470 - val_loss: 0.7103\n",
            "Epoch 69/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2326 - val_loss: 0.7866\n",
            "Epoch 70/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1687 - val_loss: 0.7234\n",
            "Epoch 71/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0287 - val_loss: 0.7694\n",
            "Epoch 72/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1071 - val_loss: 0.7032\n",
            "Epoch 73/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.5264 - val_loss: 0.7799\n",
            "Epoch 74/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 3.5675 - val_loss: 7.3965\n",
            "Epoch 75/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 1.9839 - val_loss: 0.8268\n",
            "Epoch 76/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2724 - val_loss: 0.8946\n",
            "Epoch 77/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0322 - val_loss: 0.8753\n",
            "Epoch 78/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0254 - val_loss: 0.8500\n",
            "Epoch 79/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0855 - val_loss: 0.7438\n",
            "Epoch 80/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9930 - val_loss: 0.7660\n",
            "Epoch 81/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0393 - val_loss: 0.8416\n",
            "Epoch 82/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0427 - val_loss: 0.8347\n",
            "Epoch 83/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9642 - val_loss: 0.8856\n",
            "Epoch 84/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0058 - val_loss: 0.7687\n",
            "Epoch 85/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9049 - val_loss: 0.8764\n",
            "Epoch 86/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.3660 - val_loss: 0.7018\n",
            "Epoch 87/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 2.8855 - val_loss: 0.8599\n",
            "Epoch 88/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 4.5260 - val_loss: 0.7489\n",
            "Epoch 89/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 3.1954 - val_loss: 7.3965\n",
            "Epoch 90/150\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.1680 - val_loss: 7.3965\n",
            "Epoch 91/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.0604 - val_loss: 1.5594\n",
            "Epoch 92/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.1142 - val_loss: 0.9408\n",
            "Epoch 93/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 5.7554 - val_loss: 7.3965\n",
            "Epoch 94/150\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 5.4135 - val_loss: 7.3965\n",
            "Epoch 95/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 2.8350 - val_loss: 0.7095\n",
            "Epoch 96/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.3512 - val_loss: 0.7126\n",
            "Epoch 97/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1427 - val_loss: 0.7091\n",
            "Epoch 98/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2121 - val_loss: 0.6966\n",
            "Epoch 99/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1754 - val_loss: 0.7029\n",
            "Epoch 100/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2510 - val_loss: 0.6956\n",
            "Epoch 101/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1891 - val_loss: 0.7657\n",
            "Epoch 102/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0629 - val_loss: 0.8279\n",
            "Epoch 103/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1806 - val_loss: 0.7410\n",
            "Epoch 104/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1223 - val_loss: 0.7052\n",
            "Epoch 105/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2934 - val_loss: 0.6943\n",
            "Epoch 106/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0783 - val_loss: 0.7144\n",
            "Epoch 107/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1593 - val_loss: 0.7297\n",
            "Epoch 108/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.0733 - val_loss: 0.7611\n",
            "Epoch 109/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.1374 - val_loss: 0.7277\n",
            "Epoch 110/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2115 - val_loss: 0.7213\n",
            "Epoch 111/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0102 - val_loss: 0.7707\n",
            "Epoch 112/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0838 - val_loss: 0.7703\n",
            "Epoch 113/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 0.9606 - val_loss: 0.8480\n",
            "Epoch 114/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.7449 - val_loss: 0.8031\n",
            "Epoch 115/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.3949 - val_loss: 0.8007\n",
            "Epoch 116/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1969 - val_loss: 0.7606\n",
            "Epoch 117/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0945 - val_loss: 0.7229\n",
            "Epoch 118/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.6107 - val_loss: 0.7151\n",
            "Epoch 119/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.9321 - val_loss: 0.7406\n",
            "Epoch 120/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1874 - val_loss: 0.7980\n",
            "Epoch 121/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0109 - val_loss: 0.8356\n",
            "Epoch 122/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 2.4516 - val_loss: 0.7594\n",
            "Epoch 123/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.3901 - val_loss: 0.8056\n",
            "Epoch 124/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.2190 - val_loss: 0.8597\n",
            "Epoch 125/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.0175 - val_loss: 0.7680\n",
            "Epoch 126/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9984 - val_loss: 0.8477\n",
            "Epoch 127/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.7619 - val_loss: 1.6325\n",
            "Epoch 128/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.7559 - val_loss: 0.7482\n",
            "Epoch 129/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0947 - val_loss: 0.7877\n",
            "Epoch 130/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9162 - val_loss: 0.8319\n",
            "Epoch 131/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9530 - val_loss: 0.8158\n",
            "Epoch 132/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9161 - val_loss: 0.8606\n",
            "Epoch 133/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8905 - val_loss: 0.7270\n",
            "Epoch 134/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.4089 - val_loss: 0.7474\n",
            "Epoch 135/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8578 - val_loss: 0.8043\n",
            "Epoch 136/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8371 - val_loss: 0.8127\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8731 - val_loss: 0.8279\n",
            "Epoch 138/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9214 - val_loss: 0.7931\n",
            "Epoch 139/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8721 - val_loss: 0.7904\n",
            "Epoch 140/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8477 - val_loss: 0.7958\n",
            "Epoch 141/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8723 - val_loss: 0.8127\n",
            "Epoch 142/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8297 - val_loss: 0.8303\n",
            "Epoch 143/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.9036 - val_loss: 0.7261\n",
            "Epoch 144/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8628 - val_loss: 0.7378\n",
            "Epoch 145/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.0295 - val_loss: 0.7045\n",
            "Epoch 146/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8574 - val_loss: 0.7291\n",
            "Epoch 147/150\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 0.8459 - val_loss: 0.7205\n",
            "Epoch 148/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 0.7904 - val_loss: 0.7105\n",
            "Epoch 149/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 0.8313 - val_loss: 0.7586\n",
            "Epoch 150/150\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 0.8354 - val_loss: 0.7611\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f0124f7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "# input_shape = x_train.shape[1:]\n",
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model3 = build_model(\n",
        "    input_shape,\n",
        "    head_size=450,\n",
        "    num_heads=9,\n",
        "    ff_dim=9,\n",
        "    num_transformer_blocks=9,\n",
        "    mlp_units=[250],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model3.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "model3.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, \\\n",
        "    restore_best_weights=True)]\n",
        "\n",
        "# model.fit(\n",
        "#     train_generator,\n",
        "#     y_train,\n",
        "#     validation_split=0.2,\n",
        "#     epochs=200,\n",
        "#     batch_size=64,\n",
        "#     callbacks=callbacks,\n",
        "# )\n",
        "\n",
        "model3.fit_generator(train_generator, epochs=150, validation_data=test_generator)\n",
        "\n",
        "# model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLiUPx122HyB"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model3.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model3.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8pUfZJp2HyC",
        "outputId": "ab5bbab2-f628-401a-b7ce-3a6d6e9ecffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.3286 - accuracy: 0.5333\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51496, saving model to clas_logs\\model3.hdf5\n",
            "46/46 [==============================] - 12s 98ms/step - loss: 6.3107 - accuracy: 0.5352 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 2/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 2: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 3/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 3: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 4/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 4: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 5/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 5: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 6/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 6: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 7/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 7: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 8/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 8: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 9/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 9: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 10/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 10: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 11/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 11: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 12/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 12: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 13/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 13: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 14/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 14: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 15/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 15: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 16/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 16: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 17/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 17: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 18/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 18: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 19/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 19: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 20/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 20: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 21/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 21: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 22/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 22: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 23/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 23: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 24/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 24: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 25/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 25: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 26/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 26: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 27/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 27: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 28/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 28: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 29/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 29: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 30/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 30: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 31/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 31: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 32/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 32: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 33/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 33: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 34/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 34: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 35/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 35: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 36/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 36: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 37/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 37: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 38/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 38: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 39/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 39: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 40/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 40: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 41/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 41: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 42/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 42: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 43/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 43: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 44/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 44: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 45/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 45: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 46/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 46: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 47/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 47: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 48/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 48: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 49/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 49: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 50/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 50: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 51/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 51: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 52/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 52: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 53/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 53: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 54/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 54: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 55/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 55: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 56/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 56: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 57/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 57: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 58/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 58: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 59/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 59: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 60/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 60: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 61/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 61: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 62/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 62: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 63/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 63: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 64/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 64: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 65/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 65: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 66/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 66: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 67/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 67: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 68/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 68: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 69/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 69: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 70/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 70: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 71/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 71: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 72/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 72: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 73/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 73: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 74/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 74: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 75/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 75: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 76/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 76: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 77/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 77: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 78/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 78: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 79/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 79: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 80/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 80: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 81/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 81: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 82/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 82: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 83/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 83: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 84/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 84: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 85/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 85: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 86/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 86: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 87/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 87: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 88/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 88: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 89/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 89: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 90/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 90: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 91/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 91: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 92/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 92: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 93/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 93: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 94/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 94: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 95/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 95: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 96/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 96: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 97/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 97: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 98/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 98: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 99/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 99: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 100/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 100: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 101/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 101: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 102/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 102: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 103/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 103: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 104/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 104: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 105/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 105: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 106/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 106: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 107/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 107: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 108/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 108: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 109/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 109: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 110/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 110: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 111/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 111: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 112/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 112: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 113/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 113: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 114/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 114: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 115/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 115: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 116/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 116: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 117/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 117: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 118/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 118: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 119/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 119: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 120/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 120: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 121: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 122: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 123/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 123: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 124/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 124: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 125/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 125: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 126: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 127: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 128/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 128: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 129: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 130/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 130: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 131: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 132: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 133: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 134: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 135: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 136: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 137: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 138: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 139: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 140: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 141: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 142: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 143: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 144: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 145: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 146: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 72ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 147: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 148: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 149/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 149: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 150/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 150: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "history3 = model3.fit(train_generator, epochs=150, validation_data=test_generator, shuffle=False, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "JtCN0h9F2HyC",
        "outputId": "a1e0f5d1-f498-4874-8bcd-7e02e317113f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plotHist(history3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cE5np6_2HyC"
      },
      "outputs": [],
      "source": [
        "model3 = tf.keras.models.load_model(\"/content/clas_logs\\model3.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ_gxK9x2HyC",
        "outputId": "99c0d534-fa83-474a-ac60-7bae5435ce09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 1s 28ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions3 = model3.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IHZeems2HyC",
        "outputId": "51ef100c-95a6-4019-abf6-07b5b5e845f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 9.747593 ],\n",
              "       [ 9.783359 ],\n",
              "       [ 9.763592 ],\n",
              "       [ 9.758897 ],\n",
              "       [ 9.7247   ],\n",
              "       [ 9.74351  ],\n",
              "       [ 9.765233 ],\n",
              "       [ 9.755658 ],\n",
              "       [ 9.694031 ],\n",
              "       [ 9.738562 ],\n",
              "       [ 9.719694 ],\n",
              "       [ 9.707327 ],\n",
              "       [ 9.694519 ],\n",
              "       [ 9.67099  ],\n",
              "       [ 9.677248 ],\n",
              "       [ 9.667394 ],\n",
              "       [ 9.619271 ],\n",
              "       [ 9.608254 ],\n",
              "       [ 9.569378 ],\n",
              "       [ 9.63227  ],\n",
              "       [ 9.632567 ],\n",
              "       [ 9.585481 ],\n",
              "       [ 9.618376 ],\n",
              "       [ 9.627742 ],\n",
              "       [ 9.667226 ],\n",
              "       [ 9.686107 ],\n",
              "       [ 9.729934 ],\n",
              "       [ 9.765457 ],\n",
              "       [ 9.786975 ],\n",
              "       [ 9.768919 ],\n",
              "       [ 9.866006 ],\n",
              "       [ 9.8955345],\n",
              "       [ 9.904599 ],\n",
              "       [ 9.942588 ],\n",
              "       [ 9.960629 ],\n",
              "       [ 9.978027 ],\n",
              "       [ 9.964924 ],\n",
              "       [ 9.954353 ],\n",
              "       [ 9.946191 ],\n",
              "       [ 9.95039  ],\n",
              "       [ 9.964726 ],\n",
              "       [10.00547  ],\n",
              "       [ 9.944361 ],\n",
              "       [ 9.951421 ],\n",
              "       [ 9.925909 ],\n",
              "       [ 9.930319 ],\n",
              "       [ 9.966446 ],\n",
              "       [ 9.981799 ],\n",
              "       [ 9.962901 ],\n",
              "       [ 9.944328 ],\n",
              "       [ 9.890032 ],\n",
              "       [ 9.924224 ],\n",
              "       [ 9.886362 ],\n",
              "       [ 9.805209 ],\n",
              "       [ 9.77438  ],\n",
              "       [ 9.787423 ],\n",
              "       [ 9.738054 ],\n",
              "       [ 9.664032 ],\n",
              "       [ 9.685083 ],\n",
              "       [ 9.642854 ],\n",
              "       [ 9.580893 ],\n",
              "       [ 9.556098 ],\n",
              "       [ 9.458138 ],\n",
              "       [ 9.428232 ],\n",
              "       [ 9.440151 ],\n",
              "       [ 9.40301  ],\n",
              "       [ 9.362557 ],\n",
              "       [ 9.324481 ],\n",
              "       [ 9.413897 ],\n",
              "       [ 9.43764  ],\n",
              "       [ 9.327296 ],\n",
              "       [ 9.330095 ],\n",
              "       [ 9.367924 ],\n",
              "       [ 9.383327 ],\n",
              "       [ 9.425013 ],\n",
              "       [ 9.408386 ],\n",
              "       [ 9.388796 ],\n",
              "       [ 9.436315 ],\n",
              "       [ 9.45521  ],\n",
              "       [ 9.482653 ],\n",
              "       [ 9.504372 ],\n",
              "       [ 9.492191 ],\n",
              "       [ 9.476783 ],\n",
              "       [ 9.497455 ],\n",
              "       [ 9.46196  ],\n",
              "       [ 9.49248  ],\n",
              "       [ 9.474954 ],\n",
              "       [ 9.50919  ],\n",
              "       [ 9.50317  ],\n",
              "       [ 9.559038 ],\n",
              "       [ 9.510199 ],\n",
              "       [ 9.530838 ],\n",
              "       [ 9.563486 ],\n",
              "       [ 9.550017 ],\n",
              "       [ 9.559908 ],\n",
              "       [ 9.590395 ],\n",
              "       [ 9.625644 ],\n",
              "       [ 9.644795 ],\n",
              "       [ 9.657582 ],\n",
              "       [ 9.669012 ],\n",
              "       [ 9.662599 ],\n",
              "       [ 9.656242 ],\n",
              "       [ 9.66309  ],\n",
              "       [ 9.660124 ],\n",
              "       [ 9.637476 ],\n",
              "       [ 9.634946 ],\n",
              "       [ 9.620531 ],\n",
              "       [ 9.649085 ],\n",
              "       [ 9.602753 ],\n",
              "       [ 9.622245 ],\n",
              "       [ 9.623217 ],\n",
              "       [ 9.61061  ],\n",
              "       [ 9.633185 ],\n",
              "       [ 9.608744 ],\n",
              "       [ 9.645618 ],\n",
              "       [ 9.655646 ],\n",
              "       [ 9.63678  ],\n",
              "       [ 9.648615 ],\n",
              "       [ 9.63954  ],\n",
              "       [ 9.603776 ],\n",
              "       [ 9.6383295],\n",
              "       [ 9.666205 ],\n",
              "       [ 9.648751 ],\n",
              "       [ 9.648351 ],\n",
              "       [ 9.666954 ],\n",
              "       [ 9.622846 ],\n",
              "       [ 9.529356 ],\n",
              "       [ 9.590464 ],\n",
              "       [ 9.577998 ],\n",
              "       [ 9.564392 ],\n",
              "       [ 9.619703 ],\n",
              "       [ 9.609841 ],\n",
              "       [ 9.598001 ],\n",
              "       [ 9.651436 ],\n",
              "       [ 9.637616 ],\n",
              "       [ 9.6216755],\n",
              "       [ 9.633679 ],\n",
              "       [ 9.602436 ],\n",
              "       [ 9.628477 ],\n",
              "       [ 9.6246395],\n",
              "       [ 9.660777 ],\n",
              "       [ 9.627814 ],\n",
              "       [ 9.6637125],\n",
              "       [ 9.573472 ],\n",
              "       [ 9.588881 ],\n",
              "       [ 9.636884 ],\n",
              "       [ 9.667518 ],\n",
              "       [ 9.646366 ],\n",
              "       [ 9.661788 ],\n",
              "       [ 9.647467 ],\n",
              "       [ 9.690508 ],\n",
              "       [ 9.699128 ],\n",
              "       [ 9.658476 ],\n",
              "       [ 9.654829 ],\n",
              "       [ 9.609156 ],\n",
              "       [ 9.56457  ],\n",
              "       [ 9.601927 ],\n",
              "       [ 9.576309 ],\n",
              "       [ 9.597182 ],\n",
              "       [ 9.564347 ],\n",
              "       [ 9.536259 ],\n",
              "       [ 9.507435 ],\n",
              "       [ 9.508512 ],\n",
              "       [ 9.4394865],\n",
              "       [ 9.481795 ],\n",
              "       [ 9.504025 ],\n",
              "       [ 9.471536 ],\n",
              "       [ 9.43358  ],\n",
              "       [ 9.469251 ],\n",
              "       [ 9.448005 ],\n",
              "       [ 9.4525385],\n",
              "       [ 9.506495 ],\n",
              "       [ 9.45138  ],\n",
              "       [ 9.52052  ],\n",
              "       [ 9.481165 ],\n",
              "       [ 9.447083 ],\n",
              "       [ 9.466883 ],\n",
              "       [ 9.440683 ],\n",
              "       [ 9.473286 ],\n",
              "       [ 9.467403 ],\n",
              "       [ 9.468556 ],\n",
              "       [ 9.418113 ],\n",
              "       [ 9.431925 ],\n",
              "       [ 9.412806 ],\n",
              "       [ 9.422539 ],\n",
              "       [ 9.483465 ],\n",
              "       [ 9.467656 ],\n",
              "       [ 9.454238 ],\n",
              "       [ 9.458678 ],\n",
              "       [ 9.462334 ],\n",
              "       [ 9.504696 ],\n",
              "       [ 9.539251 ],\n",
              "       [ 9.531627 ],\n",
              "       [ 9.56911  ],\n",
              "       [ 9.535045 ],\n",
              "       [ 9.5491705],\n",
              "       [ 9.608468 ],\n",
              "       [ 9.628883 ],\n",
              "       [ 9.617931 ],\n",
              "       [ 9.577646 ],\n",
              "       [ 9.538826 ],\n",
              "       [ 9.530764 ],\n",
              "       [ 9.558467 ],\n",
              "       [ 9.583829 ],\n",
              "       [ 9.5842085],\n",
              "       [ 9.560779 ],\n",
              "       [ 9.564355 ],\n",
              "       [ 9.572692 ],\n",
              "       [ 9.588707 ],\n",
              "       [ 9.59532  ],\n",
              "       [ 9.566532 ],\n",
              "       [ 9.609036 ],\n",
              "       [ 9.640577 ],\n",
              "       [ 9.664164 ],\n",
              "       [ 9.656731 ],\n",
              "       [ 9.67716  ],\n",
              "       [ 9.770672 ],\n",
              "       [ 9.713129 ],\n",
              "       [ 9.724874 ],\n",
              "       [ 9.708344 ],\n",
              "       [ 9.686865 ],\n",
              "       [ 9.683588 ],\n",
              "       [ 9.677571 ],\n",
              "       [ 9.6610155],\n",
              "       [ 9.7041235],\n",
              "       [ 9.71192  ],\n",
              "       [ 9.659967 ],\n",
              "       [ 9.678665 ],\n",
              "       [ 9.721077 ],\n",
              "       [ 9.773728 ],\n",
              "       [ 9.792334 ],\n",
              "       [ 9.788595 ],\n",
              "       [ 9.765811 ],\n",
              "       [ 9.806987 ],\n",
              "       [ 9.809215 ],\n",
              "       [ 9.830241 ],\n",
              "       [ 9.844924 ],\n",
              "       [ 9.839582 ],\n",
              "       [ 9.895644 ],\n",
              "       [ 9.843109 ],\n",
              "       [ 9.904083 ],\n",
              "       [ 9.896759 ],\n",
              "       [ 9.87808  ],\n",
              "       [ 9.915586 ],\n",
              "       [ 9.896047 ],\n",
              "       [ 9.8829975],\n",
              "       [ 9.880243 ],\n",
              "       [ 9.905113 ],\n",
              "       [ 9.905721 ],\n",
              "       [ 9.9123125],\n",
              "       [ 9.934106 ],\n",
              "       [ 9.957439 ],\n",
              "       [ 9.992989 ],\n",
              "       [10.0508995],\n",
              "       [10.058594 ],\n",
              "       [10.055209 ],\n",
              "       [10.051095 ],\n",
              "       [10.069312 ],\n",
              "       [10.081972 ],\n",
              "       [10.052666 ],\n",
              "       [10.084608 ],\n",
              "       [10.11524  ],\n",
              "       [10.171671 ],\n",
              "       [10.157095 ],\n",
              "       [10.108833 ],\n",
              "       [10.187994 ],\n",
              "       [10.178849 ],\n",
              "       [10.183055 ],\n",
              "       [10.1901865],\n",
              "       [10.180124 ],\n",
              "       [10.183613 ],\n",
              "       [10.226875 ],\n",
              "       [10.233469 ],\n",
              "       [10.2531805],\n",
              "       [10.269513 ],\n",
              "       [10.308245 ],\n",
              "       [10.325198 ],\n",
              "       [10.332571 ],\n",
              "       [10.368112 ],\n",
              "       [10.377396 ],\n",
              "       [10.35005  ],\n",
              "       [10.327848 ],\n",
              "       [10.281219 ],\n",
              "       [10.2759495],\n",
              "       [10.239846 ],\n",
              "       [10.185995 ],\n",
              "       [10.196047 ],\n",
              "       [10.170236 ],\n",
              "       [10.14705  ],\n",
              "       [10.127872 ],\n",
              "       [10.100034 ],\n",
              "       [10.078463 ],\n",
              "       [10.090452 ],\n",
              "       [10.092063 ],\n",
              "       [10.086676 ],\n",
              "       [10.0435095],\n",
              "       [10.040472 ],\n",
              "       [10.041112 ],\n",
              "       [10.008244 ],\n",
              "       [ 9.992856 ],\n",
              "       [ 9.950928 ],\n",
              "       [ 9.939916 ],\n",
              "       [ 9.940208 ],\n",
              "       [ 9.896013 ],\n",
              "       [ 9.867654 ],\n",
              "       [ 9.848069 ],\n",
              "       [ 9.805242 ],\n",
              "       [ 9.810424 ],\n",
              "       [ 9.772998 ],\n",
              "       [ 9.803685 ],\n",
              "       [ 9.812863 ],\n",
              "       [ 9.793504 ],\n",
              "       [ 9.771658 ],\n",
              "       [ 9.774188 ],\n",
              "       [ 9.780517 ],\n",
              "       [ 9.812022 ],\n",
              "       [ 9.764258 ],\n",
              "       [ 9.729112 ],\n",
              "       [ 9.726659 ],\n",
              "       [ 9.7791815],\n",
              "       [ 9.752489 ],\n",
              "       [ 9.749421 ],\n",
              "       [ 9.733925 ],\n",
              "       [ 9.70809  ],\n",
              "       [ 9.74631  ],\n",
              "       [ 9.7506485],\n",
              "       [ 9.744072 ],\n",
              "       [ 9.756394 ],\n",
              "       [ 9.776793 ],\n",
              "       [ 9.732951 ],\n",
              "       [ 9.727212 ],\n",
              "       [ 9.738633 ],\n",
              "       [ 9.712344 ],\n",
              "       [ 9.735958 ],\n",
              "       [ 9.73676  ],\n",
              "       [ 9.692978 ],\n",
              "       [ 9.715227 ],\n",
              "       [ 9.688722 ],\n",
              "       [ 9.699437 ],\n",
              "       [ 9.655743 ],\n",
              "       [ 9.664754 ],\n",
              "       [ 9.69901  ],\n",
              "       [ 9.710288 ],\n",
              "       [ 9.73175  ],\n",
              "       [ 9.77903  ],\n",
              "       [ 9.748629 ],\n",
              "       [ 9.811232 ],\n",
              "       [ 9.840882 ],\n",
              "       [ 9.871144 ],\n",
              "       [ 9.890832 ],\n",
              "       [ 9.90898  ],\n",
              "       [ 9.946809 ],\n",
              "       [ 9.952238 ],\n",
              "       [ 9.976837 ],\n",
              "       [ 9.978075 ],\n",
              "       [ 9.998456 ],\n",
              "       [10.045063 ],\n",
              "       [10.040011 ],\n",
              "       [10.094466 ],\n",
              "       [10.137322 ],\n",
              "       [10.145095 ],\n",
              "       [10.233677 ],\n",
              "       [10.261171 ],\n",
              "       [10.241369 ],\n",
              "       [10.237514 ],\n",
              "       [10.254096 ],\n",
              "       [10.274039 ],\n",
              "       [10.301696 ],\n",
              "       [10.345331 ],\n",
              "       [10.397628 ],\n",
              "       [10.381358 ],\n",
              "       [10.381639 ],\n",
              "       [10.390804 ],\n",
              "       [10.4034815],\n",
              "       [10.368538 ],\n",
              "       [10.377911 ],\n",
              "       [10.361364 ],\n",
              "       [10.334607 ],\n",
              "       [10.323358 ],\n",
              "       [10.358209 ],\n",
              "       [10.382083 ],\n",
              "       [10.351467 ],\n",
              "       [10.332894 ],\n",
              "       [10.282893 ],\n",
              "       [10.235572 ],\n",
              "       [10.209089 ],\n",
              "       [10.1653385],\n",
              "       [10.145168 ],\n",
              "       [10.099556 ],\n",
              "       [10.090156 ],\n",
              "       [10.072781 ],\n",
              "       [10.000311 ],\n",
              "       [ 9.974585 ],\n",
              "       [ 9.98162  ],\n",
              "       [ 9.9677925],\n",
              "       [ 9.927756 ],\n",
              "       [ 9.904938 ],\n",
              "       [ 9.93136  ],\n",
              "       [ 9.889309 ],\n",
              "       [ 9.830451 ],\n",
              "       [ 9.824343 ],\n",
              "       [ 9.805324 ],\n",
              "       [ 9.825433 ],\n",
              "       [ 9.764522 ],\n",
              "       [ 9.767166 ],\n",
              "       [ 9.746016 ],\n",
              "       [ 9.699081 ],\n",
              "       [ 9.711319 ],\n",
              "       [ 9.699608 ],\n",
              "       [ 9.655141 ],\n",
              "       [ 9.6059265],\n",
              "       [ 9.606793 ],\n",
              "       [ 9.63537  ],\n",
              "       [ 9.652335 ],\n",
              "       [ 9.658756 ],\n",
              "       [ 9.604268 ],\n",
              "       [ 9.59145  ],\n",
              "       [ 9.601184 ],\n",
              "       [ 9.580074 ],\n",
              "       [ 9.642312 ],\n",
              "       [ 9.641645 ],\n",
              "       [ 9.601174 ],\n",
              "       [ 9.594292 ],\n",
              "       [ 9.618757 ],\n",
              "       [ 9.584258 ],\n",
              "       [ 9.638443 ],\n",
              "       [ 9.636358 ],\n",
              "       [ 9.573668 ],\n",
              "       [ 9.614394 ],\n",
              "       [ 9.608843 ],\n",
              "       [ 9.592844 ],\n",
              "       [ 9.605017 ],\n",
              "       [ 9.591171 ],\n",
              "       [ 9.605881 ],\n",
              "       [ 9.562296 ],\n",
              "       [ 9.585472 ],\n",
              "       [ 9.584107 ],\n",
              "       [ 9.659597 ],\n",
              "       [ 9.642871 ],\n",
              "       [ 9.677938 ],\n",
              "       [ 9.67926  ],\n",
              "       [ 9.691469 ],\n",
              "       [ 9.664458 ],\n",
              "       [ 9.666904 ],\n",
              "       [ 9.662627 ],\n",
              "       [ 9.705228 ],\n",
              "       [ 9.688056 ],\n",
              "       [ 9.709212 ],\n",
              "       [ 9.693856 ],\n",
              "       [ 9.646053 ],\n",
              "       [ 9.662049 ],\n",
              "       [ 9.673278 ],\n",
              "       [ 9.703928 ],\n",
              "       [ 9.691779 ],\n",
              "       [ 9.746244 ],\n",
              "       [ 9.727405 ],\n",
              "       [ 9.653465 ],\n",
              "       [ 9.716677 ],\n",
              "       [ 9.708744 ],\n",
              "       [ 9.715034 ],\n",
              "       [ 9.708803 ],\n",
              "       [ 9.677174 ],\n",
              "       [ 9.698419 ],\n",
              "       [ 9.750961 ],\n",
              "       [ 9.723762 ],\n",
              "       [ 9.716156 ],\n",
              "       [ 9.7040415],\n",
              "       [ 9.694077 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "predictions3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2IEwe2vZ2HyC",
        "outputId": "f7831d85-bff4-422b-9fcc-72724d1ab533"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pred  Actual\n",
              "0  10.0     1.0\n",
              "1  10.0     1.0\n",
              "2  10.0     0.0\n",
              "3  10.0     1.0\n",
              "4  10.0     0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e64983fe-5c22-4b7c-8644-830832f11bf5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e64983fe-5c22-4b7c-8644-830832f11bf5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e64983fe-5c22-4b7c-8644-830832f11bf5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e64983fe-5c22-4b7c-8644-830832f11bf5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "df_pred3 = pd.concat([pd.DataFrame(np.round(predictions3)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred3.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "g_nrw0Pw2HyC",
        "outputId": "eb1d0c57-c1d1-41e1-f301-90cdca1b69dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1.0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-9749eb37b650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pred3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-4f0006d59e97>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mFP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgV7PnCr2HyD"
      },
      "source": [
        "<center><h3>Model 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scZTUkuW2HyD",
        "outputId": "991bda9c-5417-4f05-f7dc-fd0e2ecd6799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)          [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_124 (Layer  (None, 30, 12)      24          ['input_10[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_62 (Multi  (None, 30, 12)      280512      ['layer_normalization_124[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " dropout_172 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_62[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_124 (TFOp  (None, 30, 12)      0           ['dropout_172[0][0]',            \n",
            " Lambda)                                                          'input_10[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_125 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_124[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_124 (Conv1D)            (None, 30, 10)       130         ['layer_normalization_125[0][0]']\n",
            "                                                                                                  \n",
            " dropout_173 (Dropout)          (None, 30, 10)       0           ['conv1d_124[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_125 (Conv1D)            (None, 30, 12)       132         ['dropout_173[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_125 (TFOp  (None, 30, 12)      0           ['conv1d_125[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_124[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_126 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_125[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_63 (Multi  (None, 30, 12)      280512      ['layer_normalization_126[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_126[0][0]']\n",
            "                                                                                                  \n",
            " dropout_174 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_63[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_126 (TFOp  (None, 30, 12)      0           ['dropout_174[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_125[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_127 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_126[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_126 (Conv1D)            (None, 30, 10)       130         ['layer_normalization_127[0][0]']\n",
            "                                                                                                  \n",
            " dropout_175 (Dropout)          (None, 30, 10)       0           ['conv1d_126[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_127 (Conv1D)            (None, 30, 12)       132         ['dropout_175[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_127 (TFOp  (None, 30, 12)      0           ['conv1d_127[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_126[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_128 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_127[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_64 (Multi  (None, 30, 12)      280512      ['layer_normalization_128[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_128[0][0]']\n",
            "                                                                                                  \n",
            " dropout_176 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_64[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_128 (TFOp  (None, 30, 12)      0           ['dropout_176[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_127[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_129 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_128[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_128 (Conv1D)            (None, 30, 10)       130         ['layer_normalization_129[0][0]']\n",
            "                                                                                                  \n",
            " dropout_177 (Dropout)          (None, 30, 10)       0           ['conv1d_128[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_129 (Conv1D)            (None, 30, 12)       132         ['dropout_177[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_129 (TFOp  (None, 30, 12)      0           ['conv1d_129[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_128[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_130 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_129[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_65 (Multi  (None, 30, 12)      280512      ['layer_normalization_130[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_130[0][0]']\n",
            "                                                                                                  \n",
            " dropout_178 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_65[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_130 (TFOp  (None, 30, 12)      0           ['dropout_178[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_129[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_131 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_130[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_130 (Conv1D)            (None, 30, 10)       130         ['layer_normalization_131[0][0]']\n",
            "                                                                                                  \n",
            " dropout_179 (Dropout)          (None, 30, 10)       0           ['conv1d_130[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_131 (Conv1D)            (None, 30, 12)       132         ['dropout_179[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_131 (TFOp  (None, 30, 12)      0           ['conv1d_131[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_130[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_132 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_131[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_66 (Multi  (None, 30, 12)      280512      ['layer_normalization_132[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_132[0][0]']\n",
            "                                                                                                  \n",
            " dropout_180 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_66[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_132 (TFOp  (None, 30, 12)      0           ['dropout_180[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_131[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_133 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_132[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_132 (Conv1D)            (None, 30, 10)       130         ['layer_normalization_133[0][0]']\n",
            "                                                                                                  \n",
            " dropout_181 (Dropout)          (None, 30, 10)       0           ['conv1d_132[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_133 (Conv1D)            (None, 30, 12)       132         ['dropout_181[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_133 (TFOp  (None, 30, 12)      0           ['conv1d_133[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_132[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_134 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_133[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_67 (Multi  (None, 30, 12)      280512      ['layer_normalization_134[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_134[0][0]']\n",
            "                                                                                                  \n",
            " dropout_182 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_67[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_134 (TFOp  (None, 30, 12)      0           ['dropout_182[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_133[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_135 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_134[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_134 (Conv1D)            (None, 30, 10)       130         ['layer_normalization_135[0][0]']\n",
            "                                                                                                  \n",
            " dropout_183 (Dropout)          (None, 30, 10)       0           ['conv1d_134[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_135 (Conv1D)            (None, 30, 12)       132         ['dropout_183[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_135 (TFOp  (None, 30, 12)      0           ['conv1d_135[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_134[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_9 (Gl  (None, 30)          0           ['tf.__operators__.add_135[0][0]'\n",
            " obalAveragePooling1D)                                           ]                                \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 300)          9300        ['global_average_pooling1d_9[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_184 (Dropout)          (None, 300)          0           ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 1)            301         ['dropout_184[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,694,533\n",
            "Trainable params: 1,694,533\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 9s 77ms/step - loss: 2.0739 - val_loss: 1.8317\n",
            "Epoch 2/150\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 1.7474 - val_loss: 0.7293\n",
            "Epoch 3/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 1.2199 - val_loss: 0.7069\n",
            "Epoch 4/150\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 1.1690 - val_loss: 0.7329\n",
            "Epoch 5/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 1.0550 - val_loss: 0.7125\n",
            "Epoch 6/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 2.9659 - val_loss: 0.7073\n",
            "Epoch 7/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 1.7776 - val_loss: 0.8980\n",
            "Epoch 8/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 1.3169 - val_loss: 0.7474\n",
            "Epoch 9/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 1.5141 - val_loss: 0.6978\n",
            "Epoch 10/150\n",
            "46/46 [==============================] - 3s 66ms/step - loss: 1.2797 - val_loss: 0.7206\n",
            "Epoch 11/150\n",
            "46/46 [==============================] - 3s 66ms/step - loss: 1.2710 - val_loss: 0.7237\n",
            "Epoch 12/150\n",
            "46/46 [==============================] - 3s 67ms/step - loss: 0.9586 - val_loss: 0.7055\n",
            "Epoch 13/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.8762 - val_loss: 0.6993\n",
            "Epoch 14/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.9379 - val_loss: 0.7638\n",
            "Epoch 15/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 2.0336 - val_loss: 2.1990\n",
            "Epoch 16/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 1.0002 - val_loss: 0.9292\n",
            "Epoch 17/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 2.7503 - val_loss: 0.6962\n",
            "Epoch 18/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 1.5593 - val_loss: 0.7001\n",
            "Epoch 19/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.9120 - val_loss: 0.7425\n",
            "Epoch 20/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8728 - val_loss: 0.7052\n",
            "Epoch 21/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8522 - val_loss: 0.7510\n",
            "Epoch 22/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8853 - val_loss: 0.7492\n",
            "Epoch 23/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7914 - val_loss: 0.7176\n",
            "Epoch 24/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8597 - val_loss: 0.7325\n",
            "Epoch 25/150\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 0.8315 - val_loss: 0.7008\n",
            "Epoch 26/150\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 0.7945 - val_loss: 0.7142\n",
            "Epoch 27/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7853 - val_loss: 0.7136\n",
            "Epoch 28/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7969 - val_loss: 0.7073\n",
            "Epoch 29/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7781 - val_loss: 0.7013\n",
            "Epoch 30/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7839 - val_loss: 0.8615\n",
            "Epoch 31/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 1.1237 - val_loss: 1.0734\n",
            "Epoch 32/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.9556 - val_loss: 0.8877\n",
            "Epoch 33/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.9708 - val_loss: 0.8031\n",
            "Epoch 34/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.9040 - val_loss: 0.7878\n",
            "Epoch 35/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8356 - val_loss: 0.7721\n",
            "Epoch 36/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8394 - val_loss: 0.8069\n",
            "Epoch 37/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8108 - val_loss: 0.7757\n",
            "Epoch 38/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8137 - val_loss: 0.7250\n",
            "Epoch 39/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8104 - val_loss: 0.6985\n",
            "Epoch 40/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8061 - val_loss: 0.7022\n",
            "Epoch 41/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7480 - val_loss: 0.7038\n",
            "Epoch 42/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7780 - val_loss: 0.7051\n",
            "Epoch 43/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7509 - val_loss: 0.7021\n",
            "Epoch 44/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7452 - val_loss: 0.7042\n",
            "Epoch 45/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7594 - val_loss: 0.7011\n",
            "Epoch 46/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7673 - val_loss: 0.7463\n",
            "Epoch 47/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7596 - val_loss: 0.7260\n",
            "Epoch 48/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7444 - val_loss: 0.6950\n",
            "Epoch 49/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7423 - val_loss: 0.6979\n",
            "Epoch 50/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7518 - val_loss: 0.6942\n",
            "Epoch 51/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7382 - val_loss: 0.6940\n",
            "Epoch 52/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7245 - val_loss: 0.6944\n",
            "Epoch 53/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7449 - val_loss: 0.6976\n",
            "Epoch 54/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7418 - val_loss: 0.6932\n",
            "Epoch 55/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7354 - val_loss: 0.6954\n",
            "Epoch 56/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7448 - val_loss: 0.6950\n",
            "Epoch 57/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7511 - val_loss: 0.6943\n",
            "Epoch 58/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7330 - val_loss: 0.7038\n",
            "Epoch 59/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7431 - val_loss: 0.6961\n",
            "Epoch 60/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7468 - val_loss: 0.6936\n",
            "Epoch 61/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7347 - val_loss: 0.6965\n",
            "Epoch 62/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7396 - val_loss: 0.6996\n",
            "Epoch 63/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7155 - val_loss: 0.6994\n",
            "Epoch 64/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7155 - val_loss: 0.6931\n",
            "Epoch 65/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7210 - val_loss: 0.6937\n",
            "Epoch 66/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7361 - val_loss: 0.6940\n",
            "Epoch 67/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7306 - val_loss: 0.6931\n",
            "Epoch 68/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7224 - val_loss: 0.6947\n",
            "Epoch 69/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7290 - val_loss: 0.6933\n",
            "Epoch 70/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7189 - val_loss: 0.6934\n",
            "Epoch 71/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7081 - val_loss: 0.7108\n",
            "Epoch 72/150\n",
            "46/46 [==============================] - 3s 66ms/step - loss: 0.7340 - val_loss: 0.6950\n",
            "Epoch 73/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7279 - val_loss: 0.7000\n",
            "Epoch 74/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7206 - val_loss: 0.7031\n",
            "Epoch 75/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7187 - val_loss: 0.6933\n",
            "Epoch 76/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7317 - val_loss: 0.6971\n",
            "Epoch 77/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7340 - val_loss: 0.7037\n",
            "Epoch 78/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7308 - val_loss: 0.6938\n",
            "Epoch 79/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7165 - val_loss: 0.7028\n",
            "Epoch 80/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7286 - val_loss: 0.6933\n",
            "Epoch 81/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7181 - val_loss: 0.7004\n",
            "Epoch 82/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7202 - val_loss: 0.6934\n",
            "Epoch 83/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7256 - val_loss: 0.6931\n",
            "Epoch 84/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7137 - val_loss: 0.6931\n",
            "Epoch 85/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7357 - val_loss: 0.6973\n",
            "Epoch 86/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7145 - val_loss: 0.6958\n",
            "Epoch 87/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7247 - val_loss: 0.6939\n",
            "Epoch 88/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7125 - val_loss: 0.6932\n",
            "Epoch 89/150\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 3.1334 - val_loss: 1.1453\n",
            "Epoch 90/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 1.2159 - val_loss: 0.7142\n",
            "Epoch 91/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7844 - val_loss: 0.7545\n",
            "Epoch 92/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7524 - val_loss: 0.7929\n",
            "Epoch 93/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7633 - val_loss: 0.7413\n",
            "Epoch 94/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7520 - val_loss: 0.7687\n",
            "Epoch 95/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7465 - val_loss: 0.7037\n",
            "Epoch 96/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7471 - val_loss: 0.6984\n",
            "Epoch 97/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7270 - val_loss: 0.6947\n",
            "Epoch 98/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7266 - val_loss: 0.6960\n",
            "Epoch 99/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7314 - val_loss: 0.6921\n",
            "Epoch 100/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7598 - val_loss: 0.7491\n",
            "Epoch 101/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7318 - val_loss: 0.7004\n",
            "Epoch 102/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7266 - val_loss: 0.7003\n",
            "Epoch 103/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7859 - val_loss: 0.8592\n",
            "Epoch 104/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7421 - val_loss: 0.7070\n",
            "Epoch 105/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7371 - val_loss: 0.7096\n",
            "Epoch 106/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7257 - val_loss: 0.6992\n",
            "Epoch 107/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7242 - val_loss: 0.6968\n",
            "Epoch 108/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7182 - val_loss: 0.6932\n",
            "Epoch 109/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7204 - val_loss: 0.6935\n",
            "Epoch 110/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7110 - val_loss: 0.6938\n",
            "Epoch 111/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7301 - val_loss: 0.7002\n",
            "Epoch 112/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7309 - val_loss: 0.6935\n",
            "Epoch 113/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7190 - val_loss: 0.6927\n",
            "Epoch 114/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7255 - val_loss: 0.6930\n",
            "Epoch 115/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7154 - val_loss: 0.6930\n",
            "Epoch 116/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7229 - val_loss: 0.6955\n",
            "Epoch 117/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7357 - val_loss: 0.7057\n",
            "Epoch 118/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7095 - val_loss: 0.6956\n",
            "Epoch 119/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7128 - val_loss: 0.6979\n",
            "Epoch 120/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7153 - val_loss: 0.6930\n",
            "Epoch 121/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7041 - val_loss: 0.6958\n",
            "Epoch 122/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7257 - val_loss: 0.7175\n",
            "Epoch 123/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7202 - val_loss: 0.6939\n",
            "Epoch 124/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7073 - val_loss: 0.6956\n",
            "Epoch 125/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7154 - val_loss: 0.6929\n",
            "Epoch 126/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7124 - val_loss: 0.6938\n",
            "Epoch 127/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7090 - val_loss: 0.6965\n",
            "Epoch 128/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7028 - val_loss: 0.6925\n",
            "Epoch 129/150\n",
            "46/46 [==============================] - 3s 66ms/step - loss: 0.7147 - val_loss: 0.6923\n",
            "Epoch 130/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7235 - val_loss: 0.6925\n",
            "Epoch 131/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7140 - val_loss: 0.6959\n",
            "Epoch 132/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7204 - val_loss: 0.6939\n",
            "Epoch 133/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7171 - val_loss: 0.6966\n",
            "Epoch 134/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7117 - val_loss: 0.6938\n",
            "Epoch 135/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7127 - val_loss: 0.6939\n",
            "Epoch 136/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7053 - val_loss: 0.6945\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7181 - val_loss: 0.7206\n",
            "Epoch 138/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7152 - val_loss: 0.6956\n",
            "Epoch 139/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7063 - val_loss: 0.6951\n",
            "Epoch 140/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7062 - val_loss: 0.6942\n",
            "Epoch 141/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7146 - val_loss: 0.6955\n",
            "Epoch 142/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7035 - val_loss: 0.6942\n",
            "Epoch 143/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7097 - val_loss: 0.7105\n",
            "Epoch 144/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7797 - val_loss: 0.8002\n",
            "Epoch 145/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7436 - val_loss: 0.7769\n",
            "Epoch 146/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7189 - val_loss: 0.7712\n",
            "Epoch 147/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7201 - val_loss: 0.7682\n",
            "Epoch 148/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7103 - val_loss: 0.7618\n",
            "Epoch 149/150\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 0.7076 - val_loss: 0.7570\n",
            "Epoch 150/150\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.7080 - val_loss: 0.7498\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f0a8a3310>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "# input_shape = x_train.shape[1:]\n",
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model4 = build_model(\n",
        "    input_shape,\n",
        "    head_size=500,\n",
        "    num_heads=11,\n",
        "    ff_dim=10,\n",
        "    num_transformer_blocks=6,\n",
        "    mlp_units=[300],\n",
        "    mlp_dropout=0.3,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model4.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "model4.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=14, \\\n",
        "    restore_best_weights=True)]\n",
        "\n",
        "# model.fit(\n",
        "#     train_generator,\n",
        "#     y_train,\n",
        "#     validation_split=0.2,\n",
        "#     epochs=200,\n",
        "#     batch_size=64,\n",
        "#     callbacks=callbacks,\n",
        "# )\n",
        "\n",
        "model4.fit_generator(train_generator, epochs=150, validation_data=test_generator)\n",
        "\n",
        "# model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTG65To52HyD"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model4.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model4.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8z77Nj72HyD",
        "outputId": "e9bd7d2e-6eca-468b-9e93-057b0ae924c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.9217 - accuracy: 0.5160\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51282, saving model to clas_logs\\model4.hdf5\n",
            "46/46 [==============================] - 9s 84ms/step - loss: 0.9176 - accuracy: 0.5175 - val_loss: 0.8053 - val_accuracy: 0.5128\n",
            "Epoch 2/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 5.9221 - accuracy: 0.5403\n",
            "Epoch 2: val_accuracy improved from 0.51282 to 0.51496, saving model to clas_logs\\model4.hdf5\n",
            "46/46 [==============================] - 3s 69ms/step - loss: 5.9100 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 3/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 3: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 4/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 4: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 5/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 5: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 6/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 6: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 7/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 7: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 8/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 8: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 66ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 9/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 9: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 10/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 10: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 11/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 11: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 12/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 12: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 13/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 13: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 14/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 14: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 15/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 15: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 16/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 16: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 17/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 17: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 18/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 18: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 19/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 19: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 20/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 20: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 21/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 21: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 22/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 22: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 23/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 23: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 24/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 24: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 25/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 25: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 26/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 26: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 27/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 27: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 28/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 28: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 62ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 29/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 29: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 30/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 30: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 31/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 31: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 32/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 32: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 33/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 33: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 34/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 34: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 35/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 35: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 36/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 36: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 37/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 37: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 38/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 38: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 39/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 39: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 40/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 40: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 41/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 41: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 42/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 42: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 43/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 43: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 44/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 44: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 45/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 45: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 46/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 46: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 47/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 47: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 48/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 48: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 49/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 49: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 50/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 50: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 51/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 51: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 52/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 52: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 53/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 53: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 54/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 54: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 55/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 55: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 56/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 56: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 57/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 57: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 58/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 58: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 59/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 59: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 60/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 60: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 61/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 61: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 62/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 62: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 63/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 63: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 64/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 64: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 65/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 65: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 66/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 66: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 67/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 67: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 68/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 68: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 69/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 69: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 70/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 70: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 71/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 71: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 72/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 72: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 73/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 73: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 74/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 74: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 75/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 75: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 76/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 76: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 77/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 77: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 78/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 78: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 79/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 79: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 80/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 80: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 81/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 81: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 82/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 82: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 83/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 83: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 84/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 84: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 85/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 85: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 86/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 86: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 87/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 87: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 88/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 88: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 89/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 89: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 90/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 90: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 91/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 91: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 92/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 92: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 93/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 93: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 94/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 94: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 95/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 95: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 96/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 96: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 97/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 97: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 98/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 98: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 99/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 99: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 100/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 100: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 101/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 101: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 102/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 102: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 103/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 103: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 104/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 104: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 105/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 105: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 106/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 106: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 107/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 107: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 108/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 108: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 109/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 109: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 110/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 110: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 111/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 111: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 112/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 112: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 113/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 113: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 114/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 114: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 115/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 115: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 116/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 116: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 117/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 117: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 118/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 118: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 119/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 119: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 120/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 120: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 121: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 122: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 123/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 123: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 124/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 124: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 125/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 125: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 126: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 127: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 128/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 128: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 129: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 130/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 130: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 131: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 132: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 133: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 134: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 135: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 136: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 137: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 138: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 139: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 140: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 141: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 142: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 143: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 144: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 145: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 146: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 147: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 66ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 148: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 149/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 149: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 65ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 150/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 150: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "history4 = model4.fit(train_generator, epochs=150, validation_data=test_generator, shuffle=False, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "6-ViyeJ12HyD",
        "outputId": "efaa61e6-eac7-483b-bc1d-faf35c576cff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plotHist(history4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qynScu502HyD"
      },
      "outputs": [],
      "source": [
        "model4 = tf.keras.models.load_model(\"/content/clas_logs\\model4.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTNiFr-N2HyD",
        "outputId": "6a98cb42-1165-45a3-8b80-eabbebcc4c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 1s 21ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions4 = model4.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeA67fFh2HyD",
        "outputId": "c7a19845-dbea-4f3a-9aa9-3675f69802b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.3215084],\n",
              "       [3.4050608],\n",
              "       [3.2921395],\n",
              "       [3.4049504],\n",
              "       [3.2581468],\n",
              "       [3.3097057],\n",
              "       [3.313111 ],\n",
              "       [3.407962 ],\n",
              "       [3.5901024],\n",
              "       [3.6665406],\n",
              "       [3.2929988],\n",
              "       [3.5433261],\n",
              "       [3.5703282],\n",
              "       [3.4948459],\n",
              "       [3.8510652],\n",
              "       [3.7977884],\n",
              "       [4.0752673],\n",
              "       [4.0210733],\n",
              "       [4.206144 ],\n",
              "       [4.032189 ],\n",
              "       [3.7855654],\n",
              "       [3.9592848],\n",
              "       [3.7968845],\n",
              "       [3.7047477],\n",
              "       [3.8092325],\n",
              "       [3.5190825],\n",
              "       [3.3899846],\n",
              "       [3.190496 ],\n",
              "       [3.2095585],\n",
              "       [3.2945192],\n",
              "       [3.050312 ],\n",
              "       [3.1160994],\n",
              "       [2.8627424],\n",
              "       [2.9833465],\n",
              "       [2.7230802],\n",
              "       [2.8342466],\n",
              "       [2.5177863],\n",
              "       [2.4624746],\n",
              "       [2.5176706],\n",
              "       [2.7853694],\n",
              "       [3.0478303],\n",
              "       [2.9241555],\n",
              "       [3.12039  ],\n",
              "       [3.3833241],\n",
              "       [3.3361518],\n",
              "       [3.3386874],\n",
              "       [3.0760844],\n",
              "       [3.0380304],\n",
              "       [3.034384 ],\n",
              "       [3.043663 ],\n",
              "       [3.320035 ],\n",
              "       [3.268593 ],\n",
              "       [3.5055766],\n",
              "       [3.4376688],\n",
              "       [3.5715094],\n",
              "       [3.8439603],\n",
              "       [3.756367 ],\n",
              "       [3.9194899],\n",
              "       [3.7632759],\n",
              "       [3.8030603],\n",
              "       [4.106184 ],\n",
              "       [3.7978354],\n",
              "       [3.957158 ],\n",
              "       [4.067301 ],\n",
              "       [4.252871 ],\n",
              "       [4.322485 ],\n",
              "       [4.478822 ],\n",
              "       [4.7236323],\n",
              "       [4.419215 ],\n",
              "       [4.426094 ],\n",
              "       [4.3790298],\n",
              "       [4.3542185],\n",
              "       [4.31813  ],\n",
              "       [4.0592084],\n",
              "       [3.8379655],\n",
              "       [3.6114247],\n",
              "       [3.7301679],\n",
              "       [3.6142814],\n",
              "       [3.286527 ],\n",
              "       [3.2593188],\n",
              "       [3.2509127],\n",
              "       [3.2277281],\n",
              "       [3.185469 ],\n",
              "       [3.153593 ],\n",
              "       [3.1547003],\n",
              "       [3.132242 ],\n",
              "       [3.363841 ],\n",
              "       [3.1110394],\n",
              "       [3.1070497],\n",
              "       [2.9419   ],\n",
              "       [2.8998008],\n",
              "       [2.9445844],\n",
              "       [2.6491208],\n",
              "       [2.7067742],\n",
              "       [2.7359967],\n",
              "       [2.6346874],\n",
              "       [2.4590201],\n",
              "       [2.1364963],\n",
              "       [2.1444945],\n",
              "       [1.8683555],\n",
              "       [1.8322796],\n",
              "       [2.0477552],\n",
              "       [2.0441787],\n",
              "       [2.3295865],\n",
              "       [2.3435225],\n",
              "       [2.5642738],\n",
              "       [2.318672 ],\n",
              "       [2.225923 ],\n",
              "       [2.6206915],\n",
              "       [2.565129 ],\n",
              "       [2.528412 ],\n",
              "       [2.6108932],\n",
              "       [2.7052324],\n",
              "       [2.7273374],\n",
              "       [2.506389 ],\n",
              "       [2.2789438],\n",
              "       [2.2321508],\n",
              "       [2.2422068],\n",
              "       [2.4590936],\n",
              "       [2.658569 ],\n",
              "       [2.653875 ],\n",
              "       [2.6644628],\n",
              "       [2.5889893],\n",
              "       [2.6706488],\n",
              "       [2.4461195],\n",
              "       [2.7090163],\n",
              "       [2.9507873],\n",
              "       [2.975986 ],\n",
              "       [2.975682 ],\n",
              "       [3.3017335],\n",
              "       [3.139246 ],\n",
              "       [2.937723 ],\n",
              "       [2.8817255],\n",
              "       [2.6150768],\n",
              "       [2.8497994],\n",
              "       [2.6507492],\n",
              "       [2.618542 ],\n",
              "       [2.9628618],\n",
              "       [2.6874554],\n",
              "       [2.9758146],\n",
              "       [2.8253474],\n",
              "       [2.6825   ],\n",
              "       [2.5135307],\n",
              "       [2.4250903],\n",
              "       [2.4644723],\n",
              "       [2.4602273],\n",
              "       [2.2606869],\n",
              "       [2.31512  ],\n",
              "       [2.2918859],\n",
              "       [2.3787417],\n",
              "       [2.129383 ],\n",
              "       [2.1589484],\n",
              "       [2.4445345],\n",
              "       [2.4601371],\n",
              "       [2.3909857],\n",
              "       [2.2884762],\n",
              "       [2.3343496],\n",
              "       [2.5307539],\n",
              "       [2.5943506],\n",
              "       [2.5584764],\n",
              "       [2.7529612],\n",
              "       [2.7471352],\n",
              "       [2.5014925],\n",
              "       [2.7328908],\n",
              "       [2.5568938],\n",
              "       [2.7449152],\n",
              "       [3.0151649],\n",
              "       [2.9885366],\n",
              "       [3.0227227],\n",
              "       [2.9144158],\n",
              "       [3.0146158],\n",
              "       [2.8474126],\n",
              "       [2.9837584],\n",
              "       [3.013782 ],\n",
              "       [2.957183 ],\n",
              "       [2.9431758],\n",
              "       [3.1819565],\n",
              "       [3.428969 ],\n",
              "       [3.2262328],\n",
              "       [3.0001953],\n",
              "       [3.0215049],\n",
              "       [3.1944282],\n",
              "       [3.200454 ],\n",
              "       [3.1585877],\n",
              "       [3.3678565],\n",
              "       [3.1991143],\n",
              "       [3.1555276],\n",
              "       [3.0326748],\n",
              "       [2.9555695],\n",
              "       [2.955391 ],\n",
              "       [2.7553153],\n",
              "       [2.7841322],\n",
              "       [2.9922192],\n",
              "       [2.7452886],\n",
              "       [2.9639735],\n",
              "       [2.8085797],\n",
              "       [2.6353424],\n",
              "       [2.4314854],\n",
              "       [2.3569775],\n",
              "       [2.3604224],\n",
              "       [2.6028585],\n",
              "       [2.5546577],\n",
              "       [2.3935142],\n",
              "       [2.442246 ],\n",
              "       [2.7258036],\n",
              "       [2.9480429],\n",
              "       [2.730626 ],\n",
              "       [2.7023356],\n",
              "       [2.9744468],\n",
              "       [2.9462655],\n",
              "       [3.0062563],\n",
              "       [3.0532932],\n",
              "       [2.856747 ],\n",
              "       [2.932518 ],\n",
              "       [3.00172  ],\n",
              "       [3.017148 ],\n",
              "       [2.883065 ],\n",
              "       [3.0004585],\n",
              "       [3.0652065],\n",
              "       [3.0985167],\n",
              "       [3.2429829],\n",
              "       [3.5089374],\n",
              "       [3.558671 ],\n",
              "       [3.5816479],\n",
              "       [3.5993104],\n",
              "       [3.641346 ],\n",
              "       [3.7518506],\n",
              "       [3.8180945],\n",
              "       [3.7700353],\n",
              "       [3.6516395],\n",
              "       [3.3995585],\n",
              "       [3.431603 ],\n",
              "       [3.6576245],\n",
              "       [3.629723 ],\n",
              "       [3.4036434],\n",
              "       [3.1972136],\n",
              "       [3.2169785],\n",
              "       [2.9678283],\n",
              "       [2.7708619],\n",
              "       [2.672855 ],\n",
              "       [2.748578 ],\n",
              "       [2.6646779],\n",
              "       [2.617024 ],\n",
              "       [2.437096 ],\n",
              "       [2.458131 ],\n",
              "       [2.6522095],\n",
              "       [2.92716  ],\n",
              "       [2.707919 ],\n",
              "       [2.7545836],\n",
              "       [2.4437783],\n",
              "       [2.2321904],\n",
              "       [2.0720005],\n",
              "       [1.7574311],\n",
              "       [2.0664096],\n",
              "       [2.0992484],\n",
              "       [2.061555 ],\n",
              "       [1.8738774],\n",
              "       [1.8764392],\n",
              "       [2.1084304],\n",
              "       [2.400723 ],\n",
              "       [2.454308 ],\n",
              "       [2.706152 ],\n",
              "       [2.5423846],\n",
              "       [2.5791907],\n",
              "       [2.8910682],\n",
              "       [2.929515 ],\n",
              "       [3.158657 ],\n",
              "       [3.4739792],\n",
              "       [3.529878 ],\n",
              "       [3.5043988],\n",
              "       [3.66334  ],\n",
              "       [3.5047204],\n",
              "       [3.5639243],\n",
              "       [3.6368704],\n",
              "       [3.5938563],\n",
              "       [3.3855011],\n",
              "       [3.4815264],\n",
              "       [3.878418 ],\n",
              "       [3.8615956],\n",
              "       [4.1790957],\n",
              "       [4.4143815],\n",
              "       [4.6881886],\n",
              "       [4.973605 ],\n",
              "       [4.8686714],\n",
              "       [4.783033 ],\n",
              "       [5.0244265],\n",
              "       [5.0348268],\n",
              "       [4.978983 ],\n",
              "       [4.9881287],\n",
              "       [4.8907576],\n",
              "       [5.095931 ],\n",
              "       [4.740711 ],\n",
              "       [4.707502 ],\n",
              "       [4.7380314],\n",
              "       [4.7354383],\n",
              "       [4.649162 ],\n",
              "       [4.3379936],\n",
              "       [4.2840443],\n",
              "       [4.244321 ],\n",
              "       [4.2737284],\n",
              "       [4.2420835],\n",
              "       [4.467652 ],\n",
              "       [4.412699 ],\n",
              "       [4.3875794],\n",
              "       [4.369173 ],\n",
              "       [4.3161516],\n",
              "       [4.1683006],\n",
              "       [3.8893034],\n",
              "       [3.813409 ],\n",
              "       [3.621151 ],\n",
              "       [3.5525465],\n",
              "       [3.5282333],\n",
              "       [3.4345586],\n",
              "       [3.0494688],\n",
              "       [2.9933877],\n",
              "       [2.7641373],\n",
              "       [2.8827107],\n",
              "       [3.1382785],\n",
              "       [2.807393 ],\n",
              "       [2.6181927],\n",
              "       [2.6000078],\n",
              "       [2.663316 ],\n",
              "       [2.6669972],\n",
              "       [2.587027 ],\n",
              "       [2.3722832],\n",
              "       [2.340712 ],\n",
              "       [2.3953414],\n",
              "       [2.3976986],\n",
              "       [2.4228523],\n",
              "       [2.5265572],\n",
              "       [2.6082494],\n",
              "       [2.5493803],\n",
              "       [2.5103693],\n",
              "       [2.5147085],\n",
              "       [2.5018609],\n",
              "       [2.750645 ],\n",
              "       [2.6579406],\n",
              "       [2.646097 ],\n",
              "       [2.634746 ],\n",
              "       [2.797572 ],\n",
              "       [2.5619097],\n",
              "       [2.402017 ],\n",
              "       [2.237276 ],\n",
              "       [2.5383027],\n",
              "       [2.3734088],\n",
              "       [2.3265655],\n",
              "       [2.217614 ],\n",
              "       [2.2909431],\n",
              "       [2.594512 ],\n",
              "       [2.6510534],\n",
              "       [2.6726246],\n",
              "       [2.594358 ],\n",
              "       [2.7721357],\n",
              "       [2.635042 ],\n",
              "       [2.9855926],\n",
              "       [3.3289528],\n",
              "       [3.265676 ],\n",
              "       [3.3299265],\n",
              "       [3.4483967],\n",
              "       [3.4140136],\n",
              "       [3.519041 ],\n",
              "       [3.2858162],\n",
              "       [3.3612757],\n",
              "       [3.587774 ],\n",
              "       [3.6277206],\n",
              "       [3.4530206],\n",
              "       [3.276446 ],\n",
              "       [3.2871187],\n",
              "       [3.3166523],\n",
              "       [3.0521066],\n",
              "       [3.360634 ],\n",
              "       [3.6469245],\n",
              "       [3.9668448],\n",
              "       [3.730228 ],\n",
              "       [3.7067156],\n",
              "       [4.0638433],\n",
              "       [4.0592937],\n",
              "       [4.0498166],\n",
              "       [4.0362067],\n",
              "       [4.0739   ],\n",
              "       [3.6922674],\n",
              "       [4.07826  ],\n",
              "       [4.1313896],\n",
              "       [4.3476806],\n",
              "       [4.1776786],\n",
              "       [4.0772805],\n",
              "       [4.138932 ],\n",
              "       [4.03307  ],\n",
              "       [4.0210977],\n",
              "       [3.9494703],\n",
              "       [3.938375 ],\n",
              "       [4.154927 ],\n",
              "       [4.3449583],\n",
              "       [4.3596077],\n",
              "       [4.3229213],\n",
              "       [4.5521626],\n",
              "       [4.523151 ],\n",
              "       [4.525895 ],\n",
              "       [4.6643505],\n",
              "       [4.8840885],\n",
              "       [4.804414 ],\n",
              "       [4.7879477],\n",
              "       [4.4752374],\n",
              "       [4.606511 ],\n",
              "       [4.692493 ],\n",
              "       [4.4286413],\n",
              "       [4.583984 ],\n",
              "       [4.3078775],\n",
              "       [4.2436404],\n",
              "       [4.228758 ],\n",
              "       [4.3565907],\n",
              "       [4.4122353],\n",
              "       [4.3982573],\n",
              "       [4.2075634],\n",
              "       [4.1879706],\n",
              "       [4.070023 ],\n",
              "       [4.2835846],\n",
              "       [4.0553713],\n",
              "       [4.311011 ],\n",
              "       [4.155853 ],\n",
              "       [4.0857387],\n",
              "       [4.057417 ],\n",
              "       [3.8136165],\n",
              "       [3.5279117],\n",
              "       [3.4919317],\n",
              "       [3.3012724],\n",
              "       [3.3251004],\n",
              "       [3.5038984],\n",
              "       [3.5477257],\n",
              "       [3.3821077],\n",
              "       [3.4719424],\n",
              "       [3.1697812],\n",
              "       [3.4234848],\n",
              "       [3.2479043],\n",
              "       [3.3895502],\n",
              "       [3.4598227],\n",
              "       [3.1742396],\n",
              "       [3.2714791],\n",
              "       [3.3072276],\n",
              "       [3.320446 ],\n",
              "       [3.2453117],\n",
              "       [3.2911217],\n",
              "       [3.044217 ],\n",
              "       [3.2322323],\n",
              "       [3.0556486],\n",
              "       [3.080664 ],\n",
              "       [3.1186917],\n",
              "       [3.0854666],\n",
              "       [3.12173  ],\n",
              "       [3.2678635],\n",
              "       [3.318661 ],\n",
              "       [3.171189 ],\n",
              "       [3.1767466],\n",
              "       [3.1951804],\n",
              "       [2.986128 ],\n",
              "       [2.9455254],\n",
              "       [3.1248548],\n",
              "       [2.9615204],\n",
              "       [2.8632197],\n",
              "       [3.1277921],\n",
              "       [2.8110979],\n",
              "       [3.0802412],\n",
              "       [2.8030624],\n",
              "       [2.8486645],\n",
              "       [2.880451 ],\n",
              "       [3.0969887],\n",
              "       [3.2396283],\n",
              "       [3.4449658]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "predictions4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "StsYCGN-2HyE",
        "outputId": "9152257d-bef1-4fda-c51c-c180e1084966"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pred  Actual\n",
              "0   3.0     1.0\n",
              "1   3.0     1.0\n",
              "2   3.0     0.0\n",
              "3   3.0     1.0\n",
              "4   3.0     0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9429896f-953a-41b4-b6e0-42fc016fd5f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9429896f-953a-41b4-b6e0-42fc016fd5f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9429896f-953a-41b4-b6e0-42fc016fd5f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9429896f-953a-41b4-b6e0-42fc016fd5f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "df_pred4 = pd.concat([pd.DataFrame(np.round(predictions4)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred4.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "TitxfAtJ2HyE",
        "outputId": "4b45c41d-f93c-4587-955a-4aa86d09573f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1.0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-040da9ee3241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pred4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-4f0006d59e97>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mFP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEHMFG1-2HyE"
      },
      "source": [
        "<center><h3>Model 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coRou55U2HyE",
        "outputId": "6f178945-74b6-41df-bc3f-3d74694c417a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_78 (LayerN  (None, 30, 12)      24          ['input_7[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_39 (Multi  (None, 30, 12)      76512       ['layer_normalization_78[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_109 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_39[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_78 (TFOpL  (None, 30, 12)      0           ['dropout_109[0][0]',            \n",
            " ambda)                                                           'input_7[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_79 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_78[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_78 (Conv1D)             (None, 30, 7)        91          ['layer_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_110 (Dropout)          (None, 30, 7)        0           ['conv1d_78[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_79 (Conv1D)             (None, 30, 12)       96          ['dropout_110[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_79 (TFOpL  (None, 30, 12)      0           ['conv1d_79[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_78[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_80 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_79[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_40 (Multi  (None, 30, 12)      76512       ['layer_normalization_80[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_111 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_40[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_80 (TFOpL  (None, 30, 12)      0           ['dropout_111[0][0]',            \n",
            " ambda)                                                           'tf.__operators__.add_79[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_81 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_80[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_80 (Conv1D)             (None, 30, 7)        91          ['layer_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_112 (Dropout)          (None, 30, 7)        0           ['conv1d_80[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_81 (Conv1D)             (None, 30, 12)       96          ['dropout_112[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_81 (TFOpL  (None, 30, 12)      0           ['conv1d_81[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_80[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_82 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_81[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_41 (Multi  (None, 30, 12)      76512       ['layer_normalization_82[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_113 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_41[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_82 (TFOpL  (None, 30, 12)      0           ['dropout_113[0][0]',            \n",
            " ambda)                                                           'tf.__operators__.add_81[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_83 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_82[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_82 (Conv1D)             (None, 30, 7)        91          ['layer_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_114 (Dropout)          (None, 30, 7)        0           ['conv1d_82[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_83 (Conv1D)             (None, 30, 12)       96          ['dropout_114[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_83 (TFOpL  (None, 30, 12)      0           ['conv1d_83[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_82[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_84 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_83[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_42 (Multi  (None, 30, 12)      76512       ['layer_normalization_84[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_115 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_42[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_84 (TFOpL  (None, 30, 12)      0           ['dropout_115[0][0]',            \n",
            " ambda)                                                           'tf.__operators__.add_83[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_85 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_84[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_84 (Conv1D)             (None, 30, 7)        91          ['layer_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_116 (Dropout)          (None, 30, 7)        0           ['conv1d_84[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_85 (Conv1D)             (None, 30, 12)       96          ['dropout_116[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_85 (TFOpL  (None, 30, 12)      0           ['conv1d_85[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_84[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_86 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_85[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_43 (Multi  (None, 30, 12)      76512       ['layer_normalization_86[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_117 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_43[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_86 (TFOpL  (None, 30, 12)      0           ['dropout_117[0][0]',            \n",
            " ambda)                                                           'tf.__operators__.add_85[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_87 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_86[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_86 (Conv1D)             (None, 30, 7)        91          ['layer_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_118 (Dropout)          (None, 30, 7)        0           ['conv1d_86[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_87 (Conv1D)             (None, 30, 12)       96          ['dropout_118[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_87 (TFOpL  (None, 30, 12)      0           ['conv1d_87[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_86[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_6 (Gl  (None, 30)          0           ['tf.__operators__.add_87[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 600)          18600       ['global_average_pooling1d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_119 (Dropout)          (None, 600)          0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 1)            601         ['dropout_119[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 402,936\n",
            "Trainable params: 402,936\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 6s 35ms/step - loss: 1.4427 - val_loss: 0.7026\n",
            "Epoch 2/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.8398 - val_loss: 0.7056\n",
            "Epoch 3/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.8480 - val_loss: 0.7390\n",
            "Epoch 4/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7465 - val_loss: 0.6963\n",
            "Epoch 5/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7679 - val_loss: 0.7034\n",
            "Epoch 6/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7438 - val_loss: 0.7016\n",
            "Epoch 7/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7413 - val_loss: 0.7122\n",
            "Epoch 8/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7426 - val_loss: 0.7127\n",
            "Epoch 9/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7482 - val_loss: 0.7075\n",
            "Epoch 10/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7336 - val_loss: 0.7070\n",
            "Epoch 11/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7339 - val_loss: 0.7112\n",
            "Epoch 12/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7233 - val_loss: 0.7139\n",
            "Epoch 13/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7342 - val_loss: 0.7234\n",
            "Epoch 14/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7245 - val_loss: 0.7100\n",
            "Epoch 15/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7321 - val_loss: 0.7020\n",
            "Epoch 16/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7181 - val_loss: 0.7313\n",
            "Epoch 17/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7327 - val_loss: 0.8035\n",
            "Epoch 18/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7198 - val_loss: 0.7214\n",
            "Epoch 19/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7218 - val_loss: 0.7292\n",
            "Epoch 20/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7198 - val_loss: 0.7204\n",
            "Epoch 21/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7244 - val_loss: 0.7239\n",
            "Epoch 22/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7159 - val_loss: 0.7057\n",
            "Epoch 23/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7296 - val_loss: 0.7799\n",
            "Epoch 24/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7133 - val_loss: 0.7207\n",
            "Epoch 25/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7239 - val_loss: 0.7426\n",
            "Epoch 26/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7212 - val_loss: 0.7197\n",
            "Epoch 27/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7217 - val_loss: 0.7225\n",
            "Epoch 28/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7261 - val_loss: 0.7276\n",
            "Epoch 29/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7125 - val_loss: 0.7478\n",
            "Epoch 30/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7155 - val_loss: 0.7866\n",
            "Epoch 31/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7171 - val_loss: 0.7439\n",
            "Epoch 32/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7115 - val_loss: 0.7079\n",
            "Epoch 33/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7038 - val_loss: 0.7111\n",
            "Epoch 34/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7045 - val_loss: 0.7130\n",
            "Epoch 35/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7125 - val_loss: 0.7532\n",
            "Epoch 36/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7097 - val_loss: 0.7246\n",
            "Epoch 37/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7147 - val_loss: 0.7774\n",
            "Epoch 38/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7119 - val_loss: 0.7750\n",
            "Epoch 39/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7125 - val_loss: 0.7516\n",
            "Epoch 40/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7116 - val_loss: 0.7427\n",
            "Epoch 41/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7166 - val_loss: 0.7056\n",
            "Epoch 42/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7181 - val_loss: 0.7132\n",
            "Epoch 43/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7121 - val_loss: 0.7223\n",
            "Epoch 44/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7108 - val_loss: 0.7213\n",
            "Epoch 45/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7122 - val_loss: 0.7350\n",
            "Epoch 46/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7107 - val_loss: 0.7253\n",
            "Epoch 47/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7025 - val_loss: 0.7259\n",
            "Epoch 48/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7069 - val_loss: 0.7278\n",
            "Epoch 49/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7131 - val_loss: 0.7132\n",
            "Epoch 50/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7047 - val_loss: 0.7101\n",
            "Epoch 51/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7071 - val_loss: 0.7016\n",
            "Epoch 52/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7108 - val_loss: 0.7353\n",
            "Epoch 53/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7010 - val_loss: 0.6984\n",
            "Epoch 54/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7140 - val_loss: 0.7493\n",
            "Epoch 55/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7094 - val_loss: 0.7122\n",
            "Epoch 56/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7001 - val_loss: 0.7499\n",
            "Epoch 57/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7015 - val_loss: 0.7331\n",
            "Epoch 58/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7141 - val_loss: 0.7192\n",
            "Epoch 59/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7070 - val_loss: 0.7144\n",
            "Epoch 60/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7042 - val_loss: 0.7398\n",
            "Epoch 61/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7009 - val_loss: 0.7226\n",
            "Epoch 62/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6976 - val_loss: 0.7138\n",
            "Epoch 63/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7055 - val_loss: 0.7295\n",
            "Epoch 64/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6990 - val_loss: 0.7283\n",
            "Epoch 65/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6924 - val_loss: 0.7143\n",
            "Epoch 66/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6993 - val_loss: 0.7237\n",
            "Epoch 67/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7061 - val_loss: 0.7460\n",
            "Epoch 68/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7019 - val_loss: 0.7428\n",
            "Epoch 69/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7004 - val_loss: 0.7374\n",
            "Epoch 70/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7016 - val_loss: 0.7453\n",
            "Epoch 71/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7042 - val_loss: 0.7342\n",
            "Epoch 72/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6949 - val_loss: 0.7004\n",
            "Epoch 73/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7023 - val_loss: 0.7376\n",
            "Epoch 74/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6992 - val_loss: 0.7361\n",
            "Epoch 75/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7029 - val_loss: 0.7271\n",
            "Epoch 76/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7064 - val_loss: 0.7362\n",
            "Epoch 77/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6905 - val_loss: 0.7223\n",
            "Epoch 78/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7020 - val_loss: 0.7308\n",
            "Epoch 79/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6989 - val_loss: 0.7284\n",
            "Epoch 80/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7015 - val_loss: 0.7289\n",
            "Epoch 81/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6939 - val_loss: 0.7347\n",
            "Epoch 82/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6953 - val_loss: 0.7306\n",
            "Epoch 83/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7032 - val_loss: 0.7318\n",
            "Epoch 84/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7016 - val_loss: 0.7290\n",
            "Epoch 85/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6948 - val_loss: 0.7212\n",
            "Epoch 86/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7021 - val_loss: 0.7324\n",
            "Epoch 87/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7003 - val_loss: 0.7256\n",
            "Epoch 88/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6966 - val_loss: 0.7294\n",
            "Epoch 89/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7019 - val_loss: 0.7139\n",
            "Epoch 90/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7004 - val_loss: 0.7240\n",
            "Epoch 91/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6958 - val_loss: 0.7255\n",
            "Epoch 92/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7038 - val_loss: 0.7200\n",
            "Epoch 93/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7007 - val_loss: 0.7120\n",
            "Epoch 94/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6956 - val_loss: 0.7187\n",
            "Epoch 95/150\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.7066 - val_loss: 0.7136\n",
            "Epoch 96/150\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.6929 - val_loss: 0.7140\n",
            "Epoch 97/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6928 - val_loss: 0.7162\n",
            "Epoch 98/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7015 - val_loss: 0.7118\n",
            "Epoch 99/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6995 - val_loss: 0.7156\n",
            "Epoch 100/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6898 - val_loss: 0.7180\n",
            "Epoch 101/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7062 - val_loss: 0.7162\n",
            "Epoch 102/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7017 - val_loss: 0.7197\n",
            "Epoch 103/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7011 - val_loss: 0.7086\n",
            "Epoch 104/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6983 - val_loss: 0.7018\n",
            "Epoch 105/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6990 - val_loss: 0.7020\n",
            "Epoch 106/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6939 - val_loss: 0.7154\n",
            "Epoch 107/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6985 - val_loss: 0.7087\n",
            "Epoch 108/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6942 - val_loss: 0.7008\n",
            "Epoch 109/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6989 - val_loss: 0.7092\n",
            "Epoch 110/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7035 - val_loss: 0.7054\n",
            "Epoch 111/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7001 - val_loss: 0.7063\n",
            "Epoch 112/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6981 - val_loss: 0.7125\n",
            "Epoch 113/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6893 - val_loss: 0.7156\n",
            "Epoch 114/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6909 - val_loss: 0.7095\n",
            "Epoch 115/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6944 - val_loss: 0.7062\n",
            "Epoch 116/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7031 - val_loss: 0.7103\n",
            "Epoch 117/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6964 - val_loss: 0.7101\n",
            "Epoch 118/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6967 - val_loss: 0.7054\n",
            "Epoch 119/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6959 - val_loss: 0.7078\n",
            "Epoch 120/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6959 - val_loss: 0.7094\n",
            "Epoch 121/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7015 - val_loss: 0.6978\n",
            "Epoch 122/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6962 - val_loss: 0.7139\n",
            "Epoch 123/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6928 - val_loss: 0.7056\n",
            "Epoch 124/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7020 - val_loss: 0.7069\n",
            "Epoch 125/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6984 - val_loss: 0.7056\n",
            "Epoch 126/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7002 - val_loss: 0.6951\n",
            "Epoch 127/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6964 - val_loss: 0.7152\n",
            "Epoch 128/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6952 - val_loss: 0.6998\n",
            "Epoch 129/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6961 - val_loss: 0.7149\n",
            "Epoch 130/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6983 - val_loss: 0.7123\n",
            "Epoch 131/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6971 - val_loss: 0.7064\n",
            "Epoch 132/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6935 - val_loss: 0.7039\n",
            "Epoch 133/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7033 - val_loss: 0.7029\n",
            "Epoch 134/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6976 - val_loss: 0.7020\n",
            "Epoch 135/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6965 - val_loss: 0.7041\n",
            "Epoch 136/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6940 - val_loss: 0.6993\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6945 - val_loss: 0.6983\n",
            "Epoch 138/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6890 - val_loss: 0.6993\n",
            "Epoch 139/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6920 - val_loss: 0.6987\n",
            "Epoch 140/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6942 - val_loss: 0.7016\n",
            "Epoch 141/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6974 - val_loss: 0.7010\n",
            "Epoch 142/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6961 - val_loss: 0.7018\n",
            "Epoch 143/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6931 - val_loss: 0.6980\n",
            "Epoch 144/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6936 - val_loss: 0.6999\n",
            "Epoch 145/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6968 - val_loss: 0.7007\n",
            "Epoch 146/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6936 - val_loss: 0.6986\n",
            "Epoch 147/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6955 - val_loss: 0.6975\n",
            "Epoch 148/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6924 - val_loss: 0.6988\n",
            "Epoch 149/150\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6940 - val_loss: 0.6975\n",
            "Epoch 150/150\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6993 - val_loss: 0.6987\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f09c0ff10>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# input_shape = x_train.shape[1:]\n",
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model5 = build_model(\n",
        "    input_shape,\n",
        "    head_size=750,\n",
        "    num_heads=2,\n",
        "    ff_dim=7,\n",
        "    num_transformer_blocks=5,\n",
        "    mlp_units=[600],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model5.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "model5.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=11, \\\n",
        "    restore_best_weights=True)]\n",
        "\n",
        "# model.fit(\n",
        "#     train_generator,\n",
        "#     y_train,\n",
        "#     validation_split=0.2,\n",
        "#     epochs=200,\n",
        "#     batch_size=64,\n",
        "#     callbacks=callbacks,\n",
        "# )\n",
        "\n",
        "model5.fit_generator(train_generator, epochs=150, validation_data=test_generator)\n",
        "\n",
        "# model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxSr6P1T2HyE"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model5.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model5.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT28YMe82HyE",
        "outputId": "9b29b509-85ec-488e-aac9-cc365db97bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9745 - accuracy: 0.5072\n",
            "Epoch 1: val_accuracy improved from -inf to 0.48504, saving model to clas_logs\\model5.hdf5\n",
            "46/46 [==============================] - 6s 40ms/step - loss: 0.9745 - accuracy: 0.5072 - val_loss: 0.7179 - val_accuracy: 0.4850\n",
            "Epoch 2/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7442 - accuracy: 0.5140\n",
            "Epoch 2: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7442 - accuracy: 0.5140 - val_loss: 1.0791 - val_accuracy: 0.4850\n",
            "Epoch 3/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7600 - accuracy: 0.5133\n",
            "Epoch 3: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7600 - accuracy: 0.5133 - val_loss: 0.7554 - val_accuracy: 0.4850\n",
            "Epoch 4/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7299 - accuracy: 0.5086\n",
            "Epoch 4: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7299 - accuracy: 0.5086 - val_loss: 0.7248 - val_accuracy: 0.4850\n",
            "Epoch 5/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7181 - accuracy: 0.4882\n",
            "Epoch 5: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 0.7171 - accuracy: 0.4914 - val_loss: 0.7282 - val_accuracy: 0.4850\n",
            "Epoch 6/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7049 - accuracy: 0.5079\n",
            "Epoch 6: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7049 - accuracy: 0.5079 - val_loss: 0.7376 - val_accuracy: 0.4850\n",
            "Epoch 7/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7111 - accuracy: 0.5106\n",
            "Epoch 7: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7111 - accuracy: 0.5106 - val_loss: 0.7116 - val_accuracy: 0.4850\n",
            "Epoch 8/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6995 - accuracy: 0.5229\n",
            "Epoch 8: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6995 - accuracy: 0.5229 - val_loss: 0.7016 - val_accuracy: 0.4850\n",
            "Epoch 9/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7054 - accuracy: 0.5175\n",
            "Epoch 9: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.7054 - accuracy: 0.5175 - val_loss: 0.7076 - val_accuracy: 0.4850\n",
            "Epoch 10/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7029 - accuracy: 0.5140\n",
            "Epoch 10: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7029 - accuracy: 0.5140 - val_loss: 0.7119 - val_accuracy: 0.4850\n",
            "Epoch 11/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5444\n",
            "Epoch 11: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6904 - accuracy: 0.5469 - val_loss: 0.7027 - val_accuracy: 0.4850\n",
            "Epoch 12/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7027 - accuracy: 0.5086\n",
            "Epoch 12: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7027 - accuracy: 0.5086 - val_loss: 0.7222 - val_accuracy: 0.4850\n",
            "Epoch 13/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7021 - accuracy: 0.5133\n",
            "Epoch 13: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7021 - accuracy: 0.5133 - val_loss: 0.7038 - val_accuracy: 0.4850\n",
            "Epoch 14/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7099 - accuracy: 0.5058\n",
            "Epoch 14: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7099 - accuracy: 0.5058 - val_loss: 0.7054 - val_accuracy: 0.4850\n",
            "Epoch 15/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.5250\n",
            "Epoch 15: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7014 - accuracy: 0.5250 - val_loss: 0.7000 - val_accuracy: 0.4850\n",
            "Epoch 16/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.5236\n",
            "Epoch 16: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7005 - accuracy: 0.5236 - val_loss: 0.7068 - val_accuracy: 0.4850\n",
            "Epoch 17/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.4942\n",
            "Epoch 17: val_accuracy did not improve from 0.48504\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7101 - accuracy: 0.4942 - val_loss: 0.7062 - val_accuracy: 0.4850\n",
            "Epoch 18/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.5181\n",
            "Epoch 18: val_accuracy improved from 0.48504 to 0.48718, saving model to clas_logs\\model5.hdf5\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 0.6959 - accuracy: 0.5181 - val_loss: 0.7012 - val_accuracy: 0.4872\n",
            "Epoch 19/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.5270\n",
            "Epoch 19: val_accuracy did not improve from 0.48718\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6959 - accuracy: 0.5270 - val_loss: 0.7038 - val_accuracy: 0.4808\n",
            "Epoch 20/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7013 - accuracy: 0.5111\n",
            "Epoch 20: val_accuracy did not improve from 0.48718\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7008 - accuracy: 0.5106 - val_loss: 0.7005 - val_accuracy: 0.4872\n",
            "Epoch 21/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5368\n",
            "Epoch 21: val_accuracy improved from 0.48718 to 0.49786, saving model to clas_logs\\model5.hdf5\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 0.6917 - accuracy: 0.5373 - val_loss: 0.6977 - val_accuracy: 0.4979\n",
            "Epoch 22/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.5311\n",
            "Epoch 22: val_accuracy did not improve from 0.49786\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6944 - accuracy: 0.5311 - val_loss: 0.7056 - val_accuracy: 0.4850\n",
            "Epoch 23/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5510\n",
            "Epoch 23: val_accuracy improved from 0.49786 to 0.50855, saving model to clas_logs\\model5.hdf5\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 0.6927 - accuracy: 0.5510 - val_loss: 0.6908 - val_accuracy: 0.5085\n",
            "Epoch 24/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7111 - accuracy: 0.5188\n",
            "Epoch 24: val_accuracy did not improve from 0.50855\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7111 - accuracy: 0.5188 - val_loss: 0.7033 - val_accuracy: 0.4850\n",
            "Epoch 25/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.5524\n",
            "Epoch 25: val_accuracy improved from 0.50855 to 0.51282, saving model to clas_logs\\model5.hdf5\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 0.7014 - accuracy: 0.5524 - val_loss: 0.6951 - val_accuracy: 0.5128\n",
            "Epoch 26/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.5606\n",
            "Epoch 26: val_accuracy improved from 0.51282 to 0.55983, saving model to clas_logs\\model5.hdf5\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 0.6867 - accuracy: 0.5606 - val_loss: 0.6868 - val_accuracy: 0.5598\n",
            "Epoch 27/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6954 - accuracy: 0.5257\n",
            "Epoch 27: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6954 - accuracy: 0.5257 - val_loss: 0.6932 - val_accuracy: 0.5107\n",
            "Epoch 28/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.5444\n",
            "Epoch 28: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6859 - accuracy: 0.5455 - val_loss: 0.6925 - val_accuracy: 0.5427\n",
            "Epoch 29/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6836 - accuracy: 0.5667\n",
            "Epoch 29: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6836 - accuracy: 0.5667 - val_loss: 0.6922 - val_accuracy: 0.5192\n",
            "Epoch 30/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5708\n",
            "Epoch 30: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6804 - accuracy: 0.5722 - val_loss: 0.6967 - val_accuracy: 0.5171\n",
            "Epoch 31/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.5818\n",
            "Epoch 31: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6817 - accuracy: 0.5818 - val_loss: 0.6938 - val_accuracy: 0.5235\n",
            "Epoch 32/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.5613\n",
            "Epoch 32: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6996 - accuracy: 0.5613 - val_loss: 0.6901 - val_accuracy: 0.5470\n",
            "Epoch 33/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5868\n",
            "Epoch 33: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6798 - accuracy: 0.5873 - val_loss: 0.7061 - val_accuracy: 0.5043\n",
            "Epoch 34/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6659 - accuracy: 0.5886\n",
            "Epoch 34: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6659 - accuracy: 0.5886 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
            "Epoch 35/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6639 - accuracy: 0.6112\n",
            "Epoch 35: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6639 - accuracy: 0.6112 - val_loss: 0.6956 - val_accuracy: 0.4786\n",
            "Epoch 36/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.5400\n",
            "Epoch 36: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6952 - accuracy: 0.5400 - val_loss: 0.6938 - val_accuracy: 0.4957\n",
            "Epoch 37/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6718 - accuracy: 0.5886\n",
            "Epoch 37: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6718 - accuracy: 0.5886 - val_loss: 0.7022 - val_accuracy: 0.5214\n",
            "Epoch 38/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6557 - accuracy: 0.6030\n",
            "Epoch 38: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6557 - accuracy: 0.6030 - val_loss: 0.6991 - val_accuracy: 0.4915\n",
            "Epoch 39/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6536 - accuracy: 0.6222\n",
            "Epoch 39: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6536 - accuracy: 0.6222 - val_loss: 0.6973 - val_accuracy: 0.5064\n",
            "Epoch 40/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6493 - accuracy: 0.6368\n",
            "Epoch 40: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.6499 - accuracy: 0.6359 - val_loss: 0.6995 - val_accuracy: 0.5043\n",
            "Epoch 41/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.6194\n",
            "Epoch 41: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6430 - accuracy: 0.6194 - val_loss: 0.7010 - val_accuracy: 0.4979\n",
            "Epoch 42/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6325 - accuracy: 0.6366\n",
            "Epoch 42: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6325 - accuracy: 0.6366 - val_loss: 0.7034 - val_accuracy: 0.5128\n",
            "Epoch 43/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6527 - accuracy: 0.6181\n",
            "Epoch 43: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6527 - accuracy: 0.6181 - val_loss: 0.7025 - val_accuracy: 0.4808\n",
            "Epoch 44/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.5633\n",
            "Epoch 44: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6840 - accuracy: 0.5633 - val_loss: 0.6950 - val_accuracy: 0.5171\n",
            "Epoch 45/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6618 - accuracy: 0.6174\n",
            "Epoch 45: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6618 - accuracy: 0.6174 - val_loss: 0.6976 - val_accuracy: 0.5000\n",
            "Epoch 46/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.6413\n",
            "Epoch 46: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6360 - accuracy: 0.6413 - val_loss: 0.7021 - val_accuracy: 0.5150\n",
            "Epoch 47/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6836 - accuracy: 0.6194\n",
            "Epoch 47: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6836 - accuracy: 0.6194 - val_loss: 0.7121 - val_accuracy: 0.4936\n",
            "Epoch 48/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7208 - accuracy: 0.5099\n",
            "Epoch 48: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7208 - accuracy: 0.5099 - val_loss: 0.7001 - val_accuracy: 0.5085\n",
            "Epoch 49/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7176 - accuracy: 0.5181\n",
            "Epoch 49: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7176 - accuracy: 0.5181 - val_loss: 0.6946 - val_accuracy: 0.4979\n",
            "Epoch 50/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5729\n",
            "Epoch 50: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6845 - accuracy: 0.5702 - val_loss: 0.6980 - val_accuracy: 0.5064\n",
            "Epoch 51/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.5948\n",
            "Epoch 51: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6818 - accuracy: 0.5948 - val_loss: 0.7078 - val_accuracy: 0.5150\n",
            "Epoch 52/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7046 - accuracy: 0.5298\n",
            "Epoch 52: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7046 - accuracy: 0.5298 - val_loss: 0.7008 - val_accuracy: 0.4979\n",
            "Epoch 53/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6713 - accuracy: 0.5852\n",
            "Epoch 53: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6713 - accuracy: 0.5852 - val_loss: 0.6992 - val_accuracy: 0.5000\n",
            "Epoch 54/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7489 - accuracy: 0.6030\n",
            "Epoch 54: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7489 - accuracy: 0.6030 - val_loss: 0.7349 - val_accuracy: 0.4936\n",
            "Epoch 55/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6562 - accuracy: 0.6256\n",
            "Epoch 55: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6562 - accuracy: 0.6256 - val_loss: 0.6932 - val_accuracy: 0.5150\n",
            "Epoch 56/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6660 - accuracy: 0.6413\n",
            "Epoch 56: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6660 - accuracy: 0.6413 - val_loss: 0.6926 - val_accuracy: 0.5321\n",
            "Epoch 57/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7209 - accuracy: 0.5387\n",
            "Epoch 57: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7209 - accuracy: 0.5387 - val_loss: 0.7132 - val_accuracy: 0.4850\n",
            "Epoch 58/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7174 - accuracy: 0.5086\n",
            "Epoch 58: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7174 - accuracy: 0.5086 - val_loss: 0.6936 - val_accuracy: 0.4936\n",
            "Epoch 59/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.5667\n",
            "Epoch 59: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6784 - accuracy: 0.5688 - val_loss: 0.6927 - val_accuracy: 0.5107\n",
            "Epoch 60/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6263 - accuracy: 0.6456\n",
            "Epoch 60: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6285 - accuracy: 0.6448 - val_loss: 0.6993 - val_accuracy: 0.4936\n",
            "Epoch 61/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.6701\n",
            "Epoch 61: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6265 - accuracy: 0.6701 - val_loss: 0.7020 - val_accuracy: 0.4915\n",
            "Epoch 62/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6574 - accuracy: 0.6379\n",
            "Epoch 62: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6574 - accuracy: 0.6379 - val_loss: 0.7020 - val_accuracy: 0.5150\n",
            "Epoch 63/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.6407\n",
            "Epoch 63: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6364 - accuracy: 0.6407 - val_loss: 0.6930 - val_accuracy: 0.5342\n",
            "Epoch 64/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6387 - accuracy: 0.6448\n",
            "Epoch 64: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6387 - accuracy: 0.6448 - val_loss: 0.6985 - val_accuracy: 0.4957\n",
            "Epoch 65/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6253 - accuracy: 0.6598\n",
            "Epoch 65: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6253 - accuracy: 0.6598 - val_loss: 0.7063 - val_accuracy: 0.5107\n",
            "Epoch 66/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.6667\n",
            "Epoch 66: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6256 - accuracy: 0.6667 - val_loss: 0.7226 - val_accuracy: 0.5171\n",
            "Epoch 67/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6002 - accuracy: 0.6475\n",
            "Epoch 67: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6002 - accuracy: 0.6475 - val_loss: 0.7031 - val_accuracy: 0.5150\n",
            "Epoch 68/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6415 - accuracy: 0.6270\n",
            "Epoch 68: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6415 - accuracy: 0.6270 - val_loss: 0.7073 - val_accuracy: 0.4979\n",
            "Epoch 69/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.6441\n",
            "Epoch 69: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6983 - accuracy: 0.6441 - val_loss: 1.0406 - val_accuracy: 0.5150\n",
            "Epoch 70/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8060 - accuracy: 0.5017\n",
            "Epoch 70: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.8060 - accuracy: 0.5017 - val_loss: 0.7849 - val_accuracy: 0.5150\n",
            "Epoch 71/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7306 - accuracy: 0.5024\n",
            "Epoch 71: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7306 - accuracy: 0.5024 - val_loss: 0.6940 - val_accuracy: 0.4872\n",
            "Epoch 72/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.5305\n",
            "Epoch 72: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6983 - accuracy: 0.5305 - val_loss: 0.6965 - val_accuracy: 0.4872\n",
            "Epoch 73/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.5441\n",
            "Epoch 73: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6958 - accuracy: 0.5441 - val_loss: 0.6935 - val_accuracy: 0.5278\n",
            "Epoch 74/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.5558\n",
            "Epoch 74: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6908 - accuracy: 0.5558 - val_loss: 0.6958 - val_accuracy: 0.5214\n",
            "Epoch 75/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.5606\n",
            "Epoch 75: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6921 - accuracy: 0.5606 - val_loss: 0.7066 - val_accuracy: 0.5128\n",
            "Epoch 76/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6903 - accuracy: 0.5695\n",
            "Epoch 76: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6903 - accuracy: 0.5695 - val_loss: 0.6924 - val_accuracy: 0.5449\n",
            "Epoch 77/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.5688\n",
            "Epoch 77: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6834 - accuracy: 0.5688 - val_loss: 0.6977 - val_accuracy: 0.5021\n",
            "Epoch 78/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6673 - accuracy: 0.6160\n",
            "Epoch 78: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6673 - accuracy: 0.6160 - val_loss: 0.6927 - val_accuracy: 0.5171\n",
            "Epoch 79/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6665 - accuracy: 0.5882\n",
            "Epoch 79: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6673 - accuracy: 0.5866 - val_loss: 0.7022 - val_accuracy: 0.4936\n",
            "Epoch 80/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.5421\n",
            "Epoch 80: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6996 - accuracy: 0.5421 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
            "Epoch 81/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6962 - accuracy: 0.5770\n",
            "Epoch 81: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6962 - accuracy: 0.5770 - val_loss: 0.7005 - val_accuracy: 0.5128\n",
            "Epoch 82/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.5934\n",
            "Epoch 82: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6827 - accuracy: 0.5934 - val_loss: 0.7025 - val_accuracy: 0.4850\n",
            "Epoch 83/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7028 - accuracy: 0.6375\n",
            "Epoch 83: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.7026 - accuracy: 0.6366 - val_loss: 0.7064 - val_accuracy: 0.5107\n",
            "Epoch 84/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.6345\n",
            "Epoch 84: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6692 - accuracy: 0.6345 - val_loss: 0.7014 - val_accuracy: 0.4850\n",
            "Epoch 85/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6546 - accuracy: 0.6270\n",
            "Epoch 85: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6546 - accuracy: 0.6270 - val_loss: 0.7107 - val_accuracy: 0.4979\n",
            "Epoch 86/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6788 - accuracy: 0.6119\n",
            "Epoch 86: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6788 - accuracy: 0.6119 - val_loss: 0.7074 - val_accuracy: 0.5085\n",
            "Epoch 87/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.6085\n",
            "Epoch 87: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.6656 - accuracy: 0.6085 - val_loss: 0.6974 - val_accuracy: 0.4957\n",
            "Epoch 88/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8011 - accuracy: 0.5619\n",
            "Epoch 88: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.8011 - accuracy: 0.5619 - val_loss: 5.9292 - val_accuracy: 0.5150\n",
            "Epoch 89/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 5.2080 - accuracy: 0.4997\n",
            "Epoch 89: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 5.2080 - accuracy: 0.4997 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 90/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 90: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 91/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 91: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 92/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 92: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 93/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 93: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 94/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 94: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 95/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 95: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 96/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 96: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 97/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 97: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 98/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 98: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 99/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 99: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 100/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 100: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 101/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 101: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 102/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 102: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 103/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 103: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 104/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 104: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 105/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 105: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 106/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 106: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 107/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 107: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 108/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 108: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 109/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 109: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 110/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 110: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 111/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 111: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 112/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 112: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 113/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 113: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 114/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 114: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 115/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 115: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 116/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 116: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 117/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 117: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 118/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 118: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 119/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 119: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 120/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 120: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 121: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 122: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 123/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 123: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 124/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 124: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 125/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 125: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 126: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 127: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 128/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 128: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 129: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 130/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 130: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 131: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 132: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 133: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 134: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/150\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 135: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 136: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 137: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 138: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 139: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 140: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 141: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 142: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 143: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 144: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 145: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 146: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 147: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 148: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 149/150\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 149: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 150/150\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 150: val_accuracy did not improve from 0.55983\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "history5 = model5.fit(train_generator, epochs=150, validation_data=test_generator, shuffle=False, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "AjdCz1ak2HyE",
        "outputId": "a33d900b-4eaa-4c53-f3f4-efedfa5029a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plotHist(history5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK8KV4JW2HyE"
      },
      "outputs": [],
      "source": [
        "model5 = tf.keras.models.load_model(\"/content/clas_logs\\model5.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pzsz6xC2HyF",
        "outputId": "929e9bfa-9a3e-4bd1-e1c9-3ce8a0e73bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 1s 9ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions5 = model5.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "doK6kfMw2HyF",
        "outputId": "b436f8d4-dd56-47cc-ceba-ecf39499924a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pred  Actual\n",
              "0   1.0     1.0\n",
              "1   1.0     1.0\n",
              "2   1.0     0.0\n",
              "3   1.0     1.0\n",
              "4   1.0     0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8ea493b-17aa-4ac6-8bd4-6245eac9220c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8ea493b-17aa-4ac6-8bd4-6245eac9220c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8ea493b-17aa-4ac6-8bd4-6245eac9220c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8ea493b-17aa-4ac6-8bd4-6245eac9220c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "df_pred5 = pd.concat([pd.DataFrame(np.round(predictions5)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred5.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred5.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "FHACS2t52HyF",
        "outputId": "b20dce7b-d887-4153-e09c-9a6c3eeb4c43"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.5598290598290598\n",
            "MCC = 0.11886961449174695\n"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmcQWnnT2HyF"
      },
      "source": [
        "<center><h3>Model 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsoZILQb2HyF",
        "outputId": "f22e7208-ba42-4aa7-fc3a-ee4c0db4c115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_136 (Layer  (None, 30, 12)      24          ['input_11[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_68 (Multi  (None, 30, 12)      132612      ['layer_normalization_136[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_136[0][0]']\n",
            "                                                                                                  \n",
            " dropout_191 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_68[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_136 (TFOp  (None, 30, 12)      0           ['dropout_191[0][0]',            \n",
            " Lambda)                                                          'input_11[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_137 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_136[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_136 (Conv1D)            (None, 30, 6)        78          ['layer_normalization_137[0][0]']\n",
            "                                                                                                  \n",
            " dropout_192 (Dropout)          (None, 30, 6)        0           ['conv1d_136[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_137 (Conv1D)            (None, 30, 12)       84          ['dropout_192[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_137 (TFOp  (None, 30, 12)      0           ['conv1d_137[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_136[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_138 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_137[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_69 (Multi  (None, 30, 12)      132612      ['layer_normalization_138[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_138[0][0]']\n",
            "                                                                                                  \n",
            " dropout_193 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_69[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_138 (TFOp  (None, 30, 12)      0           ['dropout_193[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_137[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_139 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_138[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_138 (Conv1D)            (None, 30, 6)        78          ['layer_normalization_139[0][0]']\n",
            "                                                                                                  \n",
            " dropout_194 (Dropout)          (None, 30, 6)        0           ['conv1d_138[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_139 (Conv1D)            (None, 30, 12)       84          ['dropout_194[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_139 (TFOp  (None, 30, 12)      0           ['conv1d_139[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_138[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_140 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_139[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_70 (Multi  (None, 30, 12)      132612      ['layer_normalization_140[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_140[0][0]']\n",
            "                                                                                                  \n",
            " dropout_195 (Dropout)          (None, 30, 12)       0           ['multi_head_attention_70[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_140 (TFOp  (None, 30, 12)      0           ['dropout_195[0][0]',            \n",
            " Lambda)                                                          'tf.__operators__.add_139[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_141 (Layer  (None, 30, 12)      24          ['tf.__operators__.add_140[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " conv1d_140 (Conv1D)            (None, 30, 6)        78          ['layer_normalization_141[0][0]']\n",
            "                                                                                                  \n",
            " dropout_196 (Dropout)          (None, 30, 6)        0           ['conv1d_140[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_141 (Conv1D)            (None, 30, 12)       84          ['dropout_196[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_141 (TFOp  (None, 30, 12)      0           ['conv1d_141[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_140[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_10 (G  (None, 30)          0           ['tf.__operators__.add_141[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 900)          27900       ['global_average_pooling1d_10[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_197 (Dropout)          (None, 900)          0           ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 1)            901         ['dropout_197[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 427,267\n",
            "Trainable params: 427,267\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# input_shape = x_train.shape[1:]\n",
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model6 = build_model(\n",
        "    input_shape,\n",
        "    head_size=650,\n",
        "    num_heads=4,\n",
        "    ff_dim=6,\n",
        "    num_transformer_blocks=3,\n",
        "    mlp_units=[900],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model6.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "model6.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=11, \\\n",
        "    restore_best_weights=True)]\n",
        "\n",
        "# model.fit(\n",
        "#     train_generator,\n",
        "#     y_train,\n",
        "#     validation_split=0.2,\n",
        "#     epochs=200,\n",
        "#     batch_size=64,\n",
        "#     callbacks=callbacks,\n",
        "# )\n",
        "\n",
        "# model6.fit_generator(train_generator, epochs=150, validation_data=test_generator)\n",
        "\n",
        "# model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bePkpIQK2HyF"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model6.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model6.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYnmcLUa2HyF",
        "outputId": "011fcd8c-f406-4ce1-d2a0-72b1a3903cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.0828 - accuracy: 0.4832\n",
            "Epoch 1: val_accuracy improved from -inf to 0.48504, saving model to clas_logs\\model6.hdf5\n",
            "46/46 [==============================] - 4s 32ms/step - loss: 1.0828 - accuracy: 0.4832 - val_loss: 0.7674 - val_accuracy: 0.4850\n",
            "Epoch 2/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.8356 - accuracy: 0.5270\n",
            "Epoch 2: val_accuracy improved from 0.48504 to 0.50641, saving model to clas_logs\\model6.hdf5\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.8291 - accuracy: 0.5325 - val_loss: 0.6939 - val_accuracy: 0.5064\n",
            "Epoch 3/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7256 - accuracy: 0.4964\n",
            "Epoch 3: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7309 - accuracy: 0.4908 - val_loss: 0.8793 - val_accuracy: 0.4850\n",
            "Epoch 4/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7090 - accuracy: 0.5092\n",
            "Epoch 4: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7059 - accuracy: 0.5140 - val_loss: 0.7884 - val_accuracy: 0.4850\n",
            "Epoch 5/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7057 - accuracy: 0.5072\n",
            "Epoch 5: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7057 - accuracy: 0.5072 - val_loss: 0.7961 - val_accuracy: 0.4850\n",
            "Epoch 6/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7077 - accuracy: 0.5154\n",
            "Epoch 6: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7077 - accuracy: 0.5154 - val_loss: 0.8201 - val_accuracy: 0.4850\n",
            "Epoch 7/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7071 - accuracy: 0.5099\n",
            "Epoch 7: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7075 - accuracy: 0.5106 - val_loss: 0.7367 - val_accuracy: 0.4850\n",
            "Epoch 8/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7181 - accuracy: 0.5181\n",
            "Epoch 8: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7181 - accuracy: 0.5181 - val_loss: 0.7296 - val_accuracy: 0.4850\n",
            "Epoch 9/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7049 - accuracy: 0.5154\n",
            "Epoch 9: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7049 - accuracy: 0.5154 - val_loss: 0.7485 - val_accuracy: 0.4850\n",
            "Epoch 10/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7059 - accuracy: 0.5139\n",
            "Epoch 10: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7061 - accuracy: 0.5106 - val_loss: 0.7462 - val_accuracy: 0.4850\n",
            "Epoch 11/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7112 - accuracy: 0.5168\n",
            "Epoch 11: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7112 - accuracy: 0.5168 - val_loss: 0.6932 - val_accuracy: 0.4893\n",
            "Epoch 12/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7075 - accuracy: 0.5090\n",
            "Epoch 12: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7067 - accuracy: 0.5113 - val_loss: 0.7181 - val_accuracy: 0.4850\n",
            "Epoch 13/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7068 - accuracy: 0.5312\n",
            "Epoch 13: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7040 - accuracy: 0.5325 - val_loss: 0.7015 - val_accuracy: 0.4872\n",
            "Epoch 14/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7569 - accuracy: 0.5145\n",
            "Epoch 14: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7546 - accuracy: 0.5127 - val_loss: 0.6997 - val_accuracy: 0.5000\n",
            "Epoch 15/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7034 - accuracy: 0.5211\n",
            "Epoch 15: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7022 - accuracy: 0.5229 - val_loss: 0.7287 - val_accuracy: 0.4850\n",
            "Epoch 16/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7123 - accuracy: 0.4972\n",
            "Epoch 16: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7097 - accuracy: 0.5038 - val_loss: 0.7076 - val_accuracy: 0.4850\n",
            "Epoch 17/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7069 - accuracy: 0.5050\n",
            "Epoch 17: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7044 - accuracy: 0.5120 - val_loss: 0.7020 - val_accuracy: 0.4850\n",
            "Epoch 18/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7061 - accuracy: 0.5263\n",
            "Epoch 18: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7079 - accuracy: 0.5229 - val_loss: 0.7204 - val_accuracy: 0.4850\n",
            "Epoch 19/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7037 - accuracy: 0.5131\n",
            "Epoch 19: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7024 - accuracy: 0.5195 - val_loss: 0.7189 - val_accuracy: 0.4850\n",
            "Epoch 20/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7335 - accuracy: 0.4924\n",
            "Epoch 20: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7339 - accuracy: 0.4928 - val_loss: 0.6941 - val_accuracy: 0.4637\n",
            "Epoch 21/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7061 - accuracy: 0.5124\n",
            "Epoch 21: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7060 - accuracy: 0.5154 - val_loss: 0.7150 - val_accuracy: 0.4850\n",
            "Epoch 22/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7091 - accuracy: 0.5305\n",
            "Epoch 22: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7075 - accuracy: 0.5325 - val_loss: 0.7103 - val_accuracy: 0.4850\n",
            "Epoch 23/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6946 - accuracy: 0.5153\n",
            "Epoch 23: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6943 - accuracy: 0.5133 - val_loss: 0.7072 - val_accuracy: 0.4850\n",
            "Epoch 24/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7002 - accuracy: 0.5354\n",
            "Epoch 24: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6995 - accuracy: 0.5373 - val_loss: 0.7009 - val_accuracy: 0.4872\n",
            "Epoch 25/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6998 - accuracy: 0.5263\n",
            "Epoch 25: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6996 - accuracy: 0.5250 - val_loss: 0.7053 - val_accuracy: 0.4850\n",
            "Epoch 26/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7038 - accuracy: 0.5192\n",
            "Epoch 26: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7023 - accuracy: 0.5229 - val_loss: 0.6943 - val_accuracy: 0.4850\n",
            "Epoch 27/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7016 - accuracy: 0.5014\n",
            "Epoch 27: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7008 - accuracy: 0.5017 - val_loss: 0.7061 - val_accuracy: 0.4872\n",
            "Epoch 28/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7002 - accuracy: 0.5057\n",
            "Epoch 28: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6985 - accuracy: 0.5099 - val_loss: 0.6994 - val_accuracy: 0.4850\n",
            "Epoch 29/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7015 - accuracy: 0.5146\n",
            "Epoch 29: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7011 - accuracy: 0.5147 - val_loss: 0.7175 - val_accuracy: 0.4850\n",
            "Epoch 30/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6973 - accuracy: 0.5133\n",
            "Epoch 30: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6973 - accuracy: 0.5133 - val_loss: 0.7158 - val_accuracy: 0.4850\n",
            "Epoch 31/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7011 - accuracy: 0.5277\n",
            "Epoch 31: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6987 - accuracy: 0.5346 - val_loss: 0.6947 - val_accuracy: 0.4893\n",
            "Epoch 32/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7073 - accuracy: 0.5146\n",
            "Epoch 32: val_accuracy did not improve from 0.50641\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.7071 - accuracy: 0.5147 - val_loss: 0.7047 - val_accuracy: 0.4850\n",
            "Epoch 33/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6969 - accuracy: 0.5038\n",
            "Epoch 33: val_accuracy improved from 0.50641 to 0.51709, saving model to clas_logs\\model6.hdf5\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 0.6969 - accuracy: 0.5038 - val_loss: 0.6987 - val_accuracy: 0.5171\n",
            "Epoch 34/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.7047 - accuracy: 0.4882\n",
            "Epoch 34: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.7040 - accuracy: 0.4894 - val_loss: 0.6940 - val_accuracy: 0.5085\n",
            "Epoch 35/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7039 - accuracy: 0.4978\n",
            "Epoch 35: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7022 - accuracy: 0.5065 - val_loss: 0.6940 - val_accuracy: 0.4936\n",
            "Epoch 36/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6977 - accuracy: 0.5222\n",
            "Epoch 36: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6977 - accuracy: 0.5222 - val_loss: 0.6945 - val_accuracy: 0.5000\n",
            "Epoch 37/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6978 - accuracy: 0.5160\n",
            "Epoch 37: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6966 - accuracy: 0.5188 - val_loss: 0.7015 - val_accuracy: 0.4829\n",
            "Epoch 38/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7033 - accuracy: 0.5149\n",
            "Epoch 38: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7033 - accuracy: 0.5127 - val_loss: 0.6953 - val_accuracy: 0.5021\n",
            "Epoch 39/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.7008 - accuracy: 0.5263\n",
            "Epoch 39: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7020 - accuracy: 0.5209 - val_loss: 0.7110 - val_accuracy: 0.4850\n",
            "Epoch 40/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.5299\n",
            "Epoch 40: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6925 - accuracy: 0.5311 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
            "Epoch 41/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6981 - accuracy: 0.5229\n",
            "Epoch 41: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6982 - accuracy: 0.5222 - val_loss: 0.6939 - val_accuracy: 0.4936\n",
            "Epoch 42/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 0.7002 - accuracy: 0.4935\n",
            "Epoch 42: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6991 - accuracy: 0.4976 - val_loss: 0.6970 - val_accuracy: 0.4915\n",
            "Epoch 43/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6957 - accuracy: 0.5215\n",
            "Epoch 43: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6959 - accuracy: 0.5202 - val_loss: 0.6935 - val_accuracy: 0.5085\n",
            "Epoch 44/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.5243\n",
            "Epoch 44: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6980 - accuracy: 0.5243 - val_loss: 0.6944 - val_accuracy: 0.5021\n",
            "Epoch 45/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6899 - accuracy: 0.5368\n",
            "Epoch 45: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6901 - accuracy: 0.5352 - val_loss: 0.6965 - val_accuracy: 0.5128\n",
            "Epoch 46/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 0.6911 - accuracy: 0.5518\n",
            "Epoch 46: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6902 - accuracy: 0.5551 - val_loss: 0.7092 - val_accuracy: 0.5171\n",
            "Epoch 47/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6903 - accuracy: 0.5382\n",
            "Epoch 47: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.6902 - accuracy: 0.5394 - val_loss: 0.7012 - val_accuracy: 0.4979\n",
            "Epoch 48/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 0.6776 - accuracy: 0.5729\n",
            "Epoch 48: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6768 - accuracy: 0.5749 - val_loss: 0.7019 - val_accuracy: 0.5000\n",
            "Epoch 49/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 1.7451 - accuracy: 0.5385\n",
            "Epoch 49: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 2.1926 - accuracy: 0.5298 - val_loss: 6.4591 - val_accuracy: 0.4850\n",
            "Epoch 50/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 50: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 51/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2667 - accuracy: 0.4629\n",
            "Epoch 51: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3348 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 52/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.2288 - accuracy: 0.4600\n",
            "Epoch 52: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.2288 - accuracy: 0.4600 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 53/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 53: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 54/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 54: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 55/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 55: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 56/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 56: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 57/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 57: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 58/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 58: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 59/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 59: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 60/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 60: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 61/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 61: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 62/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 62: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 63/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 63: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 64/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 64: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 65/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 65: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 66/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 66: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 67/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 67: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 68/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 68: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 69/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 69: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 70/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 70: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 71/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 71: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 72/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 72: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 73/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 73: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 74/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 74: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 75/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 75: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 76/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 76: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 77/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 77: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 78/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 78: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 79/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 79: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 80/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 80: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 81/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 81: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 82/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 82: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 83/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 83: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 84/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 84: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 85/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 85: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 86/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 86: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 87/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 87: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 88/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 88: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 89/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 89: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 90/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 90: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 91/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 91: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 92/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 92: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 93/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 93: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 94/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 94: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 95/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 95: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 96/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 96: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 97/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 97: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 98/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 98: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 99/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 99: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 100/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 100: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 101/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 101: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 102/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 102: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 103/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 103: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 104/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 104: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 105/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 105: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 106/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 106: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 107/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 107: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 108/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 108: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 109/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 109: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 110/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 110: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 111/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 111: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 112/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 112: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 113/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 113: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 114/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 114: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 115/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 115: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 116/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 116: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 117/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 117: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 118/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 118: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 119/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 119: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 120/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 120: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 121/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 121: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 122/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 122: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 123/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 123: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 124/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 124: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 125/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 125: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 126/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 126: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 127/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 127: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 128/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 128: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 129/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 129: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 130/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 130: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 131/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 131: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 132/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 132: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 133/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 133: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 134/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 134: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 135/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 135: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 136/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 136: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 137/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 137: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 138/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 138: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 139/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 139: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 140/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 140: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 141/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 141: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 142/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 142: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 143/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 143: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 144/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 144: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 145/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 145: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 146/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 146: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 147/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 147: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 148/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 148: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 149/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 149: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 150/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 150: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 151/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 151: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 152/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 152: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 153/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 153: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 154/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 154: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 155/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 155: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 156/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 156: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 157/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 157: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 158/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 158: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 159/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 159: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 160/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 160: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 161/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 161: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 162/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 162: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 163/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 163: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 164/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 164: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 165/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 165: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 166/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 166: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 167/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 167: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 168/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 168: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 169/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 169: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 170/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 170: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 171/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 171: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 172/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 172: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 173/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 173: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 174/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 174: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 175/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 175: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 176/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 176: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 177/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 177: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 178/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 178: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 179/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 179: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 180/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 180: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 181/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 181: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 182/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 182: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 183/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 183: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 184/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 184: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 185/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 185: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 186/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 186: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 187/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 187: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 188/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 188: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 189/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 189: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 190/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 190: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 191/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 191: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 192/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 192: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 193/200\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 193: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 194/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 194: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 195/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 195: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 196/200\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 196: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 197/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 197: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 198/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 198: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 199/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 199: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 200/200\n",
            "43/46 [===========================>..] - ETA: 0s - loss: 8.2842 - accuracy: 0.4629\n",
            "Epoch 200: val_accuracy did not improve from 0.51709\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n"
          ]
        }
      ],
      "source": [
        "history6 = model6.fit(train_generator, epochs=200, validation_data=test_generator, shuffle=False, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQw9vT4T2HyF"
      },
      "outputs": [],
      "source": [
        "model6 = tf.keras.models.load_model(\"/content/clas_logs\\model6.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-4u8upU2HyG",
        "outputId": "ae1bc439-a22c-41ff-a01a-db1c13ec22a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 0s 6ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions6 = model6.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "id24j5Nn2HyG",
        "outputId": "77afd750-8dbc-46ae-a896-547502550ce1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pred  Actual\n",
              "0   1.0     1.0\n",
              "1   1.0     1.0\n",
              "2   1.0     0.0\n",
              "3   1.0     1.0\n",
              "4   1.0     0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-067b3113-d993-48f5-80da-55c8ec372e76\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-067b3113-d993-48f5-80da-55c8ec372e76')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-067b3113-d993-48f5-80da-55c8ec372e76 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-067b3113-d993-48f5-80da-55c8ec372e76');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "df_pred6 = pd.concat([pd.DataFrame(np.round(predictions6)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred6.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred6.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "1qlaGrfa2HyG",
        "outputId": "a48651cc-afb4-4f88-e502-bb18c501fd22"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.5170940170940171\n",
            "MCC = 0.04768009943174778\n"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUczrsF02HyG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CZfsX1H2HyG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9RpjF_s2HyG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3788561493fa958048bd987a33f40c184c63bad8accd2b78030fd8fa4715b6c3"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
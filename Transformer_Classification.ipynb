{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solankybasant/demo/blob/main/Transformer_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DCpQAmMxvPM3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from IPython.display import Image, display\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import seaborn as sn\n",
        "\n",
        "mpl.rcParams[\"figure.figsize\"] = (18,6)\n",
        "mpl.rcParams[\"axes.grid\"] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5-7gcvHQvPM7"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/Sentiment_Analysis_Combined.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zHbRGRFdvPM8",
        "outputId": "aaab2107-8f55-41a6-c64e-32b7a21b6410"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date     Adj Close  Label          Open          High           Low  \\\n",
              "0  2008-08-08  11734.320312      1  11432.089844  11759.959961  11388.040039   \n",
              "1  2008-08-11  11782.349609      0  11729.669922  11867.110352  11675.530273   \n",
              "2  2008-08-12  11642.469727      0  11781.700195  11782.349609  11601.519531   \n",
              "3  2008-08-13  11532.959961      1  11632.809570  11633.780273  11453.339844   \n",
              "4  2008-08-14  11615.929688      1  11532.070312  11718.280273  11450.889648   \n",
              "\n",
              "      Volume  Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0  212830000      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1  183190000      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2  173590000      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3  182550000      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4  159790000      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19c775d9-a488-428f-bbc0-b93d0dbf8e60\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-08-12</td>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-08-13</td>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-08-14</td>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19c775d9-a488-428f-bbc0-b93d0dbf8e60')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-19c775d9-a488-428f-bbc0-b93d0dbf8e60 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-19c775d9-a488-428f-bbc0-b93d0dbf8e60');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UdocQIzEvPM-",
        "outputId": "b8dc9a94-dc07-4932-a6ef-517ba4e77605"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date     Adj Close  Label          Open          High           Low  \\\n",
              "0 2008-08-08  11734.320312      1  11432.089844  11759.959961  11388.040039   \n",
              "1 2008-08-11  11782.349609      0  11729.669922  11867.110352  11675.530273   \n",
              "2 2008-08-12  11642.469727      0  11781.700195  11782.349609  11601.519531   \n",
              "3 2008-08-13  11532.959961      1  11632.809570  11633.780273  11453.339844   \n",
              "4 2008-08-14  11615.929688      1  11532.070312  11718.280273  11450.889648   \n",
              "\n",
              "      Volume  Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0  212830000      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1  183190000      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2  173590000      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3  182550000      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4  159790000      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34aeaaf3-107f-450a-a104-d952c0041cad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-08-12</td>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-08-13</td>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-08-14</td>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34aeaaf3-107f-450a-a104-d952c0041cad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34aeaaf3-107f-450a-a104-d952c0041cad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34aeaaf3-107f-450a-a104-d952c0041cad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# df = data.drop(columns = [\"Label\"])\n",
        "df = data\n",
        "df.Date = pd.to_datetime(df.Date, infer_datetime_format=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrtIqtlfvPNA",
        "outputId": "d84e6798-9213-4186-f23c-dca193efc576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1989 entries, 0 to 1988\n",
            "Data columns (total 13 columns):\n",
            " #   Column        Non-Null Count  Dtype         \n",
            "---  ------        --------------  -----         \n",
            " 0   Date          1989 non-null   datetime64[ns]\n",
            " 1   Adj Close     1989 non-null   float64       \n",
            " 2   Label         1989 non-null   int64         \n",
            " 3   Open          1989 non-null   float64       \n",
            " 4   High          1989 non-null   float64       \n",
            " 5   Low           1989 non-null   float64       \n",
            " 6   Volume        1989 non-null   int64         \n",
            " 7   Subjectivity  1989 non-null   float64       \n",
            " 8   Polarity      1989 non-null   float64       \n",
            " 9   compound      1989 non-null   float64       \n",
            " 10  neg           1989 non-null   float64       \n",
            " 11  pos           1989 non-null   float64       \n",
            " 12  neu           1989 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(10), int64(2)\n",
            "memory usage: 202.1 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "zmJgWOwZvPNB",
        "outputId": "e63b74bb-e4c6-4c5a-8ec7-b95c40889644"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc3b0941150>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df.set_index('Date')[\"Adj Close\"].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "7rJXkQG6vPNC",
        "outputId": "29f4bc13-4fe6-4bd8-fabb-5a7334db8bf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fc3b07ebf10>,\n",
              "       <matplotlib.axes._subplots.AxesSubplot object at 0x7fc3b0357410>,\n",
              "       <matplotlib.axes._subplots.AxesSubplot object at 0x7fc3b038e750>],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df.set_index('Date')[[\"Open\", \"High\", \"Low\"]].plot(subplots=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zCTfVrY1vPND",
        "outputId": "12a545c4-60c8-42da-8a16-97f9eafdd3aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Adj Close  Label          Open          High           Low     Volume  \\\n",
              "0  11734.320312      1  11432.089844  11759.959961  11388.040039  212830000   \n",
              "1  11782.349609      0  11729.669922  11867.110352  11675.530273  183190000   \n",
              "2  11642.469727      0  11781.700195  11782.349609  11601.519531  173590000   \n",
              "3  11532.959961      1  11632.809570  11633.780273  11453.339844  182550000   \n",
              "4  11615.929688      1  11532.070312  11718.280273  11450.889648  159790000   \n",
              "\n",
              "   Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cee376b7-773e-4755-bf0d-30dfc29e46d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cee376b7-773e-4755-bf0d-30dfc29e46d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cee376b7-773e-4755-bf0d-30dfc29e46d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cee376b7-773e-4755-bf0d-30dfc29e46d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_input = df.drop(columns=[\"Date\"])\n",
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "RoPgS2HNvPND",
        "outputId": "9bc75821-cfe2-42e3-af4a-81fe7beba60a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Adj Close        Label          Open          High           Low  \\\n",
              "count   1989.000000  1989.000000   1989.000000   1989.000000   1989.000000   \n",
              "mean   13463.032255     0.535445  13459.116048  13541.303173  13372.931728   \n",
              "std     3144.006996     0.498867   3143.281634   3136.271725   3150.420934   \n",
              "min     6547.049805     0.000000   6547.009766   6709.609863   6469.950195   \n",
              "25%    10913.379883     0.000000  10907.339844  11000.980469  10824.759766   \n",
              "50%    13025.580078     1.000000  13022.049805  13088.110352  12953.129883   \n",
              "75%    16478.410156     1.000000  16477.699219  16550.070312  16392.769531   \n",
              "max    18312.390625     1.000000  18315.060547  18351.359375  18272.560547   \n",
              "\n",
              "             Volume  Subjectivity     Polarity     compound          neg  \\\n",
              "count  1.989000e+03   1989.000000  1989.000000  1989.000000  1989.000000   \n",
              "mean   1.628110e+08      0.361426     0.022722    -0.957369     0.162315   \n",
              "std    9.392343e+07      0.060884     0.053687     0.199673     0.038575   \n",
              "min    8.410000e+06      0.161332    -0.225978    -0.999500     0.059000   \n",
              "25%    1.000000e+08      0.321410    -0.011461    -0.996400     0.135000   \n",
              "50%    1.351700e+08      0.361652     0.024870    -0.993200     0.159000   \n",
              "75%    1.926000e+08      0.400533     0.057980    -0.985500     0.188000   \n",
              "max    6.749200e+08      0.615242     0.195774     0.991700     0.316000   \n",
              "\n",
              "               pos          neu  \n",
              "count  1989.000000  1989.000000  \n",
              "mean      0.065675     0.772018  \n",
              "std       0.020968     0.041819  \n",
              "min       0.007000     0.588000  \n",
              "25%       0.051000     0.746000  \n",
              "50%       0.064000     0.773000  \n",
              "75%       0.079000     0.802000  \n",
              "max       0.153000     0.894000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a0c3140-c447-4815-993f-b15edc1d0e51\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1.989000e+03</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13463.032255</td>\n",
              "      <td>0.535445</td>\n",
              "      <td>13459.116048</td>\n",
              "      <td>13541.303173</td>\n",
              "      <td>13372.931728</td>\n",
              "      <td>1.628110e+08</td>\n",
              "      <td>0.361426</td>\n",
              "      <td>0.022722</td>\n",
              "      <td>-0.957369</td>\n",
              "      <td>0.162315</td>\n",
              "      <td>0.065675</td>\n",
              "      <td>0.772018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3144.006996</td>\n",
              "      <td>0.498867</td>\n",
              "      <td>3143.281634</td>\n",
              "      <td>3136.271725</td>\n",
              "      <td>3150.420934</td>\n",
              "      <td>9.392343e+07</td>\n",
              "      <td>0.060884</td>\n",
              "      <td>0.053687</td>\n",
              "      <td>0.199673</td>\n",
              "      <td>0.038575</td>\n",
              "      <td>0.020968</td>\n",
              "      <td>0.041819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6547.049805</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6547.009766</td>\n",
              "      <td>6709.609863</td>\n",
              "      <td>6469.950195</td>\n",
              "      <td>8.410000e+06</td>\n",
              "      <td>0.161332</td>\n",
              "      <td>-0.225978</td>\n",
              "      <td>-0.999500</td>\n",
              "      <td>0.059000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>10913.379883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10907.339844</td>\n",
              "      <td>11000.980469</td>\n",
              "      <td>10824.759766</td>\n",
              "      <td>1.000000e+08</td>\n",
              "      <td>0.321410</td>\n",
              "      <td>-0.011461</td>\n",
              "      <td>-0.996400</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.051000</td>\n",
              "      <td>0.746000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13025.580078</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13022.049805</td>\n",
              "      <td>13088.110352</td>\n",
              "      <td>12953.129883</td>\n",
              "      <td>1.351700e+08</td>\n",
              "      <td>0.361652</td>\n",
              "      <td>0.024870</td>\n",
              "      <td>-0.993200</td>\n",
              "      <td>0.159000</td>\n",
              "      <td>0.064000</td>\n",
              "      <td>0.773000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>16478.410156</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16477.699219</td>\n",
              "      <td>16550.070312</td>\n",
              "      <td>16392.769531</td>\n",
              "      <td>1.926000e+08</td>\n",
              "      <td>0.400533</td>\n",
              "      <td>0.057980</td>\n",
              "      <td>-0.985500</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.079000</td>\n",
              "      <td>0.802000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>18312.390625</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18315.060547</td>\n",
              "      <td>18351.359375</td>\n",
              "      <td>18272.560547</td>\n",
              "      <td>6.749200e+08</td>\n",
              "      <td>0.615242</td>\n",
              "      <td>0.195774</td>\n",
              "      <td>0.991700</td>\n",
              "      <td>0.316000</td>\n",
              "      <td>0.153000</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a0c3140-c447-4815-993f-b15edc1d0e51')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a0c3140-c447-4815-993f-b15edc1d0e51 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a0c3140-c447-4815-993f-b15edc1d0e51');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df_input.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FboEhtmXvPNF"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qj4LiprVvPNF"
      },
      "outputs": [],
      "source": [
        "features = df_scaled\n",
        "target = df_scaled[:,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IthSpTxmvPNG"
      },
      "source": [
        "<center>features is a 2D list, target is a 1D list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yGt_aaKFvPNI"
      },
      "outputs": [],
      "source": [
        "def plotHist(history : tf.keras.callbacks.History, val : str):\n",
        "    plt.plot(history.history[val])\n",
        "    plt.plot(history.history[\"val_\" + val])\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['train', 'test'], loc='upper left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rbfF1h_vPNJ",
        "outputId": "26da3563-da50-4b0a-c5ac-c743fe51b969"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1989, 12, 1989)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(features), len(features[0]), len(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Gb3S1utbvPNK",
        "outputId": "aad81e6f-8e78-439d-be2b-e4869d6fa48c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Adj Close  Label          Open          High           Low     Volume  \\\n",
              "0  11734.320312      1  11432.089844  11759.959961  11388.040039  212830000   \n",
              "1  11782.349609      0  11729.669922  11867.110352  11675.530273  183190000   \n",
              "2  11642.469727      0  11781.700195  11782.349609  11601.519531  173590000   \n",
              "3  11532.959961      1  11632.809570  11633.780273  11453.339844  182550000   \n",
              "4  11615.929688      1  11532.070312  11718.280273  11450.889648  159790000   \n",
              "\n",
              "   Subjectivity  Polarity  compound    neg    pos    neu  \n",
              "0      0.267549 -0.048568   -0.9982  0.235  0.041  0.724  \n",
              "1      0.374806  0.121956   -0.9858  0.191  0.089  0.721  \n",
              "2      0.536234 -0.044302   -0.9715  0.128  0.056  0.816  \n",
              "3      0.364021  0.011398   -0.9809  0.146  0.066  0.788  \n",
              "4      0.375099  0.040677   -0.9882  0.189  0.094  0.717  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d82bd48-2629-41f6-878b-f9531be57fcc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Label</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11734.320312</td>\n",
              "      <td>1</td>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11759.959961</td>\n",
              "      <td>11388.040039</td>\n",
              "      <td>212830000</td>\n",
              "      <td>0.267549</td>\n",
              "      <td>-0.048568</td>\n",
              "      <td>-0.9982</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11782.349609</td>\n",
              "      <td>0</td>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11867.110352</td>\n",
              "      <td>11675.530273</td>\n",
              "      <td>183190000</td>\n",
              "      <td>0.374806</td>\n",
              "      <td>0.121956</td>\n",
              "      <td>-0.9858</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11642.469727</td>\n",
              "      <td>0</td>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11782.349609</td>\n",
              "      <td>11601.519531</td>\n",
              "      <td>173590000</td>\n",
              "      <td>0.536234</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.9715</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11532.959961</td>\n",
              "      <td>1</td>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11633.780273</td>\n",
              "      <td>11453.339844</td>\n",
              "      <td>182550000</td>\n",
              "      <td>0.364021</td>\n",
              "      <td>0.011398</td>\n",
              "      <td>-0.9809</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11615.929688</td>\n",
              "      <td>1</td>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11718.280273</td>\n",
              "      <td>11450.889648</td>\n",
              "      <td>159790000</td>\n",
              "      <td>0.375099</td>\n",
              "      <td>0.040677</td>\n",
              "      <td>-0.9882</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d82bd48-2629-41f6-878b-f9531be57fcc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6d82bd48-2629-41f6-878b-f9531be57fcc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6d82bd48-2629-41f6-878b-f9531be57fcc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RapvY6CCvPNL",
        "outputId": "32d0f01b-da52-4757-b533-9a7cf4c81a57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.40894198e-01, 1.00000000e+00, 4.15113783e-01, 4.33813671e-01,\n",
              "       4.16695095e-01, 3.06702075e-01, 2.34004623e-01, 4.20649400e-01,\n",
              "       6.52872640e-04, 6.84824903e-01, 2.32876712e-01, 4.44444444e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQGtPg2AvPNM",
        "outputId": "35bb301c-af20-41ec-aa12-10b249c55066"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "target[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lOiZMJPmvPNM"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81gTm2CVvPNN",
        "outputId": "e10bdb99-f2c8-4a45-889a-0f27a423657a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1491, 12), (498, 12), (1491,), (498,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BruWh_GZvPNO"
      },
      "outputs": [],
      "source": [
        "win_len = 30\n",
        "batch_size = 32\n",
        "num_features = 12\n",
        "train_generator = TimeseriesGenerator(x_train, y_train, length=win_len, sampling_rate=1, batch_size=batch_size)\n",
        "test_generator = TimeseriesGenerator(x_test, y_test, length=win_len, sampling_rate=1, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCQdKT5MvPNP",
        "outputId": "68fda896-a6ea-48c5-e8af-aca8d0850162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of batches in training set : 46\n",
            "number of batches in testing set : 15\n",
            "batch size : 32\n",
            "window size : 30\n",
            "number of features : 12\n"
          ]
        }
      ],
      "source": [
        "print(\"number of batches in training set :\", len(train_generator))\n",
        "print(\"number of batches in testing set :\", len(test_generator))\n",
        "print(\"batch size :\", len(train_generator[0][0]))\n",
        "print(\"window size :\", len(train_generator[0][0][0]))\n",
        "print(\"number of features :\", len(train_generator[0][0][0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJW5vQmvPNQ"
      },
      "source": [
        "<center><h2>Making the Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1IWbG_TsvPNR"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = tf.keras.layers.Dense(1)(x)\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMIb8RBxvPNT"
      },
      "source": [
        "<center><h3>Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l1DsknvhvPNT"
      },
      "outputs": [],
      "source": [
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model1 = build_model(input_shape, head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], \n",
        "                    mlp_dropout=0.4,dropout=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1pvIbO6vPNU",
        "outputId": "d4ea1c0f-93a8-4d09-9f58-6af9e4b8e230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 30, 12)      24          ['input_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 30, 12)      52236       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 30, 12)       0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 30, 12)      0           ['dropout[0][0]',                \n",
            " da)                                                              'input_1[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 30, 4)        52          ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 30, 4)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 30, 12)       60          ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 30, 12)      0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 30, 12)      52236       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 30, 12)      0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 30, 4)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 30, 12)       60          ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 30, 12)      0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 30, 12)      52236       ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 30, 12)      0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 30, 4)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 30, 12)       60          ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 30, 12)      0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 30, 12)      52236       ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 30, 12)       0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 30, 12)      0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 30, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 30, 12)       60          ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 30, 12)      0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 30)          0           ['tf.__operators__.add_7[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          3968        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 213,681\n",
            "Trainable params: 213,681\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4bdzkGBDvPNU"
      },
      "outputs": [],
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model1.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model1.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjPZgn9DvPNV",
        "outputId": "122820bb-5ac7-423b-8e90-e4ec1a0251ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.5816 - accuracy: 0.5195\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51496, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 13s 37ms/step - loss: 2.5816 - accuracy: 0.5195 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 2/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 3.4910 - accuracy: 0.5114\n",
            "Epoch 2: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 3.4291 - accuracy: 0.5051 - val_loss: 1.1387 - val_accuracy: 0.4850\n",
            "Epoch 3/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.1076 - accuracy: 0.5120\n",
            "Epoch 3: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 2.1076 - accuracy: 0.5120 - val_loss: 0.8894 - val_accuracy: 0.4850\n",
            "Epoch 4/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 1.2054 - accuracy: 0.5083\n",
            "Epoch 4: val_accuracy improved from 0.51496 to 0.52137, saving model to clas_logs\\model1.hdf5\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 1.2224 - accuracy: 0.5065 - val_loss: 0.6955 - val_accuracy: 0.5214\n",
            "Epoch 5/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 1.1576 - accuracy: 0.5014\n",
            "Epoch 5: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 1.1542 - accuracy: 0.5010 - val_loss: 0.7110 - val_accuracy: 0.4850\n",
            "Epoch 6/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 5.8081 - accuracy: 0.4805\n",
            "Epoch 6: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 5.8081 - accuracy: 0.4805 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 7/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3410 - accuracy: 0.4593\n",
            "Epoch 7: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3410 - accuracy: 0.4593 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 8/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 8: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 9/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 9: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 10/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 10: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 11/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 11: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 12/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3134 - accuracy: 0.4604\n",
            "Epoch 12: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3417 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 13/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 13: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 14/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3518 - accuracy: 0.4586\n",
            "Epoch 14: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3518 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 15/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3419 - accuracy: 0.4586\n",
            "Epoch 15: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3419 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 16/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 16: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 17/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 17: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 18/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 8.3230 - accuracy: 0.4604\n",
            "Epoch 18: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 19/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2413 - accuracy: 0.4638\n",
            "Epoch 19: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3224 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 20/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3512 - accuracy: 0.4586\n",
            "Epoch 20: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 21/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2714 - accuracy: 0.4638\n",
            "Epoch 21: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3515 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 22/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 8.2712 - accuracy: 0.4638\n",
            "Epoch 22: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3512 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 23/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 8.3338 - accuracy: 0.4586\n",
            "Epoch 23: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 8.3338 - accuracy: 0.4586 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 24/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.7231 - accuracy: 0.4737\n",
            "Epoch 24: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.6517 - accuracy: 0.4757 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 25/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.7549 - accuracy: 0.5291\n",
            "Epoch 25: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 5.6460 - accuracy: 0.5291 - val_loss: 1.0194 - val_accuracy: 0.4850\n",
            "Epoch 26/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 4.0451 - accuracy: 0.5064\n",
            "Epoch 26: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 3.9964 - accuracy: 0.5003 - val_loss: 0.8394 - val_accuracy: 0.4850\n",
            "Epoch 27/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 3.0296 - accuracy: 0.5085\n",
            "Epoch 27: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 3.0148 - accuracy: 0.5086 - val_loss: 0.6976 - val_accuracy: 0.4679\n",
            "Epoch 28/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 4.7375 - accuracy: 0.5305\n",
            "Epoch 28: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 4.7375 - accuracy: 0.5305 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 29/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.7717 - accuracy: 0.5384\n",
            "Epoch 29: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.6398 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 30/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8843 - accuracy: 0.5181\n",
            "Epoch 30: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8481 - accuracy: 0.5209 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 31/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8596 - accuracy: 0.5359\n",
            "Epoch 31: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8596 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 32/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8379 - accuracy: 0.5270\n",
            "Epoch 32: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8379 - accuracy: 0.5270 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 33/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1121 - accuracy: 0.5241\n",
            "Epoch 33: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.0315 - accuracy: 0.5298 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 34/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0665 - accuracy: 0.5334\n",
            "Epoch 34: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9876 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 35/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.9813 - accuracy: 0.5398\n",
            "Epoch 35: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9054 - accuracy: 0.5448 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 36/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9387 - accuracy: 0.5400\n",
            "Epoch 36: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9387 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 37/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9395 - accuracy: 0.5414\n",
            "Epoch 37: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9395 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 38/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0122 - accuracy: 0.5396\n",
            "Epoch 38: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9845 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 39/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9727 - accuracy: 0.5403\n",
            "Epoch 39: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9353 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 40/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9897 - accuracy: 0.5368\n",
            "Epoch 40: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9519 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 41/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8219 - accuracy: 0.5400\n",
            "Epoch 41: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8219 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 42/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9316 - accuracy: 0.5414\n",
            "Epoch 42: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9316 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 43/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8715 - accuracy: 0.5448\n",
            "Epoch 43: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.8715 - accuracy: 0.5448 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 44/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9221 - accuracy: 0.5359\n",
            "Epoch 44: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9221 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 45/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.4088 - accuracy: 0.5192\n",
            "Epoch 45: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.3480 - accuracy: 0.5243 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 46/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.4913 - accuracy: 0.5327\n",
            "Epoch 46: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 5.4032 - accuracy: 0.5346 - val_loss: 0.7622 - val_accuracy: 0.4850\n",
            "Epoch 47/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.1726 - accuracy: 0.4860\n",
            "Epoch 47: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.1726 - accuracy: 0.4860 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 48/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.3606 - accuracy: 0.5340\n",
            "Epoch 48: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.3424 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 49/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.4795 - accuracy: 0.5410\n",
            "Epoch 49: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.4821 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 50/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8096 - accuracy: 0.5347\n",
            "Epoch 50: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.7851 - accuracy: 0.5366 - val_loss: 6.5764 - val_accuracy: 0.5150\n",
            "Epoch 51/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8189 - accuracy: 0.5373\n",
            "Epoch 51: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.8189 - accuracy: 0.5373 - val_loss: 1.1434 - val_accuracy: 0.4808\n",
            "Epoch 52/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.8701 - accuracy: 0.5375\n",
            "Epoch 52: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8444 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 53/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9944 - accuracy: 0.5366\n",
            "Epoch 53: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9944 - accuracy: 0.5366 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 54/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.9508 - accuracy: 0.5384\n",
            "Epoch 54: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8657 - accuracy: 0.5441 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 55/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9967 - accuracy: 0.5366\n",
            "Epoch 55: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9967 - accuracy: 0.5366 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 56/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0001 - accuracy: 0.5382\n",
            "Epoch 56: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9725 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 57/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9012 - accuracy: 0.5428\n",
            "Epoch 57: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9012 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 58/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9325 - accuracy: 0.5387\n",
            "Epoch 58: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9325 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 59/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8728 - accuracy: 0.5441\n",
            "Epoch 59: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.8728 - accuracy: 0.5441 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 60/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9176 - accuracy: 0.5410\n",
            "Epoch 60: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.8912 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 61/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0570 - accuracy: 0.5311\n",
            "Epoch 61: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0570 - accuracy: 0.5311 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 62/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9197 - accuracy: 0.5421\n",
            "Epoch 62: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9197 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 63/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9445 - accuracy: 0.5407\n",
            "Epoch 63: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9445 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 64/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9728 - accuracy: 0.5428\n",
            "Epoch 64: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9728 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 65/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9827 - accuracy: 0.5421\n",
            "Epoch 65: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 66/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0037 - accuracy: 0.5407\n",
            "Epoch 66: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 67/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0724 - accuracy: 0.5362\n",
            "Epoch 67: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 68/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9827 - accuracy: 0.5421\n",
            "Epoch 68: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 69/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 69: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 70/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9831 - accuracy: 0.5421\n",
            "Epoch 70: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9831 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 71/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0038 - accuracy: 0.5407\n",
            "Epoch 71: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0038 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 72/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 72: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 73/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9786 - accuracy: 0.5424\n",
            "Epoch 73: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9514 - accuracy: 0.5441 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 74/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9833 - accuracy: 0.5414\n",
            "Epoch 74: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9833 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 75/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0104 - accuracy: 0.5403\n",
            "Epoch 75: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 76/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0144 - accuracy: 0.5400\n",
            "Epoch 76: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.0144 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 77/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 77: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 78/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0738 - accuracy: 0.5348\n",
            "Epoch 78: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9947 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 79/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9845 - accuracy: 0.5414\n",
            "Epoch 79: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9845 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 80/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9933 - accuracy: 0.5414\n",
            "Epoch 80: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9933 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 81/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9940 - accuracy: 0.5407\n",
            "Epoch 81: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9940 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 82/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9827 - accuracy: 0.5421\n",
            "Epoch 82: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 83/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9937 - accuracy: 0.5414\n",
            "Epoch 83: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9937 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 84/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0129 - accuracy: 0.5389\n",
            "Epoch 84: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9852 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 85/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9737 - accuracy: 0.5414\n",
            "Epoch 85: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9737 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 86/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0216 - accuracy: 0.5389\n",
            "Epoch 86: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9937 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 87/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0725 - accuracy: 0.5362\n",
            "Epoch 87: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9829 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 88/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9827 - accuracy: 0.5421\n",
            "Epoch 88: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 89/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 89: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 90/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0844 - accuracy: 0.5348\n",
            "Epoch 90: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0048 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 91/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 91: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 92/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0104 - accuracy: 0.5403\n",
            "Epoch 92: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9827 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 93/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9829 - accuracy: 0.5421\n",
            "Epoch 93: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9829 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 94/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0836 - accuracy: 0.5355\n",
            "Epoch 94: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.0041 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 95/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0037 - accuracy: 0.5407\n",
            "Epoch 95: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 96/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9843 - accuracy: 0.5414\n",
            "Epoch 96: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9843 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 97/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0506 - accuracy: 0.5376\n",
            "Epoch 97: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9723 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 98/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9726 - accuracy: 0.5428\n",
            "Epoch 98: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9726 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 99/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0511 - accuracy: 0.5376\n",
            "Epoch 99: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9623 - accuracy: 0.5435 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 100/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0222 - accuracy: 0.5389\n",
            "Epoch 100: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9943 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 101/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0001 - accuracy: 0.5394\n",
            "Epoch 101: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0001 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 102/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0672 - accuracy: 0.5154\n",
            "Epoch 102: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0672 - accuracy: 0.5154 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 103/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.3525 - accuracy: 0.4979\n",
            "Epoch 103: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.3199 - accuracy: 0.5003 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 104/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0709 - accuracy: 0.5352\n",
            "Epoch 104: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 7.0709 - accuracy: 0.5352 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 105/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.7310 - accuracy: 0.5469\n",
            "Epoch 105: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.7542 - accuracy: 0.5428 - val_loss: 7.9432 - val_accuracy: 0.4850\n",
            "Epoch 106/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.8774 - accuracy: 0.5380\n",
            "Epoch 106: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.8774 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 107/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 107: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 108/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 108: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 109/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 109: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 110/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 110: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 111/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 111: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 112/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 112: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 113/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 113: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 114/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 114: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 115/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 115: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 116/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 116: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 117/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 117: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 118/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 118: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 119/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 119: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 120/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 120: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 121: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 122: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 123/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 123: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 124/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 124: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 125/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 125: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 126: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 127: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 128/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 128: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 129: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 130/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 130: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 131: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 132: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 133: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 134: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 135: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 136: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 137: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 138: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 139: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 140: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 141: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 142: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 143: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 144: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 145: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 146: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 147: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 148: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 149/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 149: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 150/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 150: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 151/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 151: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 152/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 152: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 153/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 153: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 154/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 154: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 155/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 155: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 156/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 156: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 157/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 157: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 158/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 158: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 159/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 159: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 160/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 160: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 161/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 161: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 162/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 162: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 163/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 163: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 164/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 164: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 165/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 165: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 166/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 166: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 167/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 167: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 168/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 168: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 169/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 169: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 170/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 170: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 171/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 171: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 172/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 172: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 173/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 173: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 174/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 174: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 175/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 175: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 176/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 176: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 177/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 177: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 178/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 178: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 179/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 179: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 180/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 180: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 181/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 181: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 182/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 182: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 183/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 183: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 184/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 184: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 185/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 185: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 186/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 186: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 187/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 187: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 188/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 188: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 189/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 189: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 190/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 190: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 191/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 191: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 192/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 192: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 193/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 193: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 194/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 194: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 195/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 195: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 196/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 196: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 197/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 197: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 198/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 198: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 199/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 199: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 200/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 200: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 201/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 201: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 202/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 202: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 203/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 203: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 204/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 204: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 205/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 205: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 206/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 206: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 207/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 207: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 208/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 208: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 209/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 209: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 210/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 210: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 211/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 211: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 212/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 212: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 213/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 213: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 214/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 214: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 215/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 215: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 216/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 216: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 217/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 217: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 218/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 218: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 219/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 219: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 220/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 220: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 221/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 221: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 222/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 222: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 223/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 223: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 224/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 224: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 225/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 225: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 226/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 226: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 227/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 227: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 228/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 228: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 229/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 229: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 230/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 230: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 231/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 231: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 232/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 232: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 233/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 233: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 234/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 234: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 235/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 235: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 236/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 236: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 237/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 237: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 238/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 238: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 239/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 239: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 240/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 240: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 241/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 241: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 242/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 242: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 243/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 243: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 244/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 244: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 245/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 245: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 246/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 246: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 247/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 247: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 248/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 248: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 249/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 249: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 250/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 250: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 251/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 251: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 252/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 252: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 253/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 253: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 254/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 254: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 255/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 255: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 256/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 256: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 257/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 257: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 258/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 258: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 259/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 259: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 260/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 260: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 261/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 261: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 262/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 262: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 263/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 263: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 264/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 264: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 265/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 265: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 266/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 266: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 267/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 267: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 268/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 268: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 269/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 269: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 270/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 270: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 271/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 271: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 272/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 272: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 273/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 273: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 274/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 274: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 275/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 275: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 276/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 276: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 277/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 277: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 278/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 278: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 279/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 279: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 280/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 280: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 281/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 281: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 282/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 282: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 283/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 283: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 284/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 284: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 285/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 285: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 286/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 286: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 287/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 287: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 288/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 288: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 289/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 289: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 290/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 290: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 291/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 291: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 292/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 292: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 48ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 293/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 293: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 294/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 294: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 295/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 295: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 296/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 296: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 297/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 297: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 298/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 298: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 299/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 299: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 300/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 300: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 301/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 301: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 302/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 302: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 303/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 303: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 304/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 304: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 305/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 305: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 306/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 306: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 307/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 307: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 308/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 308: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 309/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 309: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 310/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 310: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 311/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 311: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 312/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 312: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 313/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 313: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 314/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 314: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 315/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 315: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 316/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 316: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 317/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 317: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 318/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 318: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 319/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 319: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 320/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 320: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 321/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 321: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 322/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 322: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 323/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 323: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 324/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 324: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 325/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 325: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 326/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 326: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 327/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 327: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 328/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 328: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 329/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 329: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 330/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 330: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 331/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 331: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 332/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 332: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 333/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 333: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 334/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 334: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 335/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 335: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 336/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 336: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 337/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 337: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 338/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 338: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 339/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0037 - accuracy: 0.5407\n",
            "Epoch 339: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 340/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 340: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 341/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 341: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 342/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 342: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 343/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 343: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 344/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 344: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 345/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 345: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 346/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 346: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 347/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 347: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 348/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 348: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 349/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 349: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 350/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 350: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 351/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 351: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 352/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 352: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 353/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 353: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 354/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 354: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 355/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 355: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 356/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 356: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 357/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 357: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 358/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 358: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 359/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 359: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 360/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 360: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 361/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 361: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 362/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 362: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 363/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 363: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 364/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 364: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 365/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 365: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 366/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 366: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 367/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 367: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 368/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 368: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 369/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 369: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 370/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 370: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 371/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 371: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 372/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 372: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 373/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 373: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 374/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 374: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 375/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 375: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 376/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 376: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 377/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 377: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 378/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 378: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 379/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 379: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 380/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 380: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 381/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 381: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 382/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 382: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 383/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 383: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 384/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 384: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 385/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 385: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 386/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 386: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 387/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 387: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 388/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 388: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 389/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 389: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 390/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 390: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 391/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 391: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 392/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 392: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 393/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 393: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 394/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 394: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 395/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 395: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 396/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 396: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 397/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 397: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 398/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 398: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 399/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 399: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 400/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 400: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 401/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 401: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 402/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 402: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 403/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 403: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 404/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 404: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 405/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 405: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 406/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 406: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 407/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 407: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 408/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 408: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 409/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 409: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 410/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 410: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 411/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 411: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 412/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 412: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 413/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 413: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 414/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 414: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 415/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 415: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 416/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 416: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 417/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 417: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 418/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 418: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 419/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 419: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 420/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 420: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 421/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 421: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 422/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 422: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 423/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 423: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 424/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 424: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 425/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 425: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 426/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 426: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 427/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 427: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 428/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 428: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 429/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 429: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 430/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 430: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 431/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 431: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 432/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 432: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 433/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 433: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 434/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 434: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 435/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 435: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 436/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 436: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 437/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 437: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 438/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 438: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 439/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 439: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 440/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 440: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 441/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 441: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 442/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 442: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 443/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 443: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 444/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 444: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 445/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 445: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 446/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 446: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 447/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 447: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 448/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 448: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 449/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 449: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 450/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 450: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 451/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 451: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 452/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 452: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 453/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 453: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 454/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 454: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 455/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 455: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 456/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 456: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 457/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 457: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 458/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 458: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 459/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 459: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 460/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 460: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 461/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 461: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 462/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 462: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 463/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 463: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 464/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 464: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 465/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 465: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 466/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 466: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 467/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 467: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 468/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 468: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 469/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 469: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 3s 54ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 470/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 470: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 471/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 471: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 472/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 472: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 473/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 473: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 474/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 474: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 475/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 475: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 476/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 476: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 477/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 477: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 478/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 478: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 479/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 479: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 480/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 480: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 481/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 481: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 482/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 482: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 483/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 483: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 484/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 484: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 485/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 485: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 486/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 486: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 487/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 487: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 488/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 488: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 489/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 489: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 490/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 490: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 491/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 491: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 492/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 492: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 493/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 493: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 494/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 494: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 495/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 495: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 496/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 496: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 497/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 497: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 498/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 498: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 499/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 499: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 500/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 500: val_accuracy did not improve from 0.52137\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ],
      "source": [
        "# history = model1.fit()\n",
        "history = model1.fit_generator(train_generator, epochs=500, validation_data=test_generator, shuffle=False, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "EDGPbMZ7vPNW",
        "outputId": "4d9b4038-22b4-4c61-e080-118737126750"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plotHist(history, \"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "0xfxCV-9vPNX",
        "outputId": "49c39539-1068-4780-b7df-e34ea352b485"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plotHist(history, \"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CJd7-j2TvPNY"
      },
      "outputs": [],
      "source": [
        "model1 = tf.keras.models.load_model(\"/content/clas_logs\\model1.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr33w9zBvPNY",
        "outputId": "b4cbc2b4-7c6e-46c0-c28a-5afa3f764c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 1s 6ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model1.predict(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMnQcuHMvPNZ",
        "outputId": "5ac4e8cf-bf92-40d8-dee8-f1888f21d24e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49025583],\n",
              "       [0.5691159 ],\n",
              "       [0.47406   ],\n",
              "       [0.5144795 ],\n",
              "       [0.5117658 ],\n",
              "       [0.54769695],\n",
              "       [0.52417076],\n",
              "       [0.48997888],\n",
              "       [0.50442535],\n",
              "       [0.5678146 ],\n",
              "       [0.508131  ],\n",
              "       [0.5509618 ],\n",
              "       [0.5307672 ],\n",
              "       [0.47099188],\n",
              "       [0.56188166],\n",
              "       [0.5139838 ],\n",
              "       [0.5125375 ],\n",
              "       [0.5360035 ],\n",
              "       [0.54639846],\n",
              "       [0.5275662 ],\n",
              "       [0.52931356],\n",
              "       [0.5235117 ],\n",
              "       [0.5445008 ],\n",
              "       [0.47072476],\n",
              "       [0.4907159 ],\n",
              "       [0.4919728 ],\n",
              "       [0.4609087 ],\n",
              "       [0.5047648 ],\n",
              "       [0.41215336],\n",
              "       [0.4595322 ],\n",
              "       [0.49176407],\n",
              "       [0.4710005 ],\n",
              "       [0.47615796],\n",
              "       [0.49271333],\n",
              "       [0.4621929 ],\n",
              "       [0.5183671 ],\n",
              "       [0.4847251 ],\n",
              "       [0.5145487 ],\n",
              "       [0.4524294 ],\n",
              "       [0.48918787],\n",
              "       [0.47088045],\n",
              "       [0.44183606],\n",
              "       [0.49572098],\n",
              "       [0.48349988],\n",
              "       [0.48515478],\n",
              "       [0.49949846],\n",
              "       [0.4909861 ],\n",
              "       [0.50841093],\n",
              "       [0.471227  ],\n",
              "       [0.43343163],\n",
              "       [0.48105636],\n",
              "       [0.4706308 ],\n",
              "       [0.49330437],\n",
              "       [0.48821157],\n",
              "       [0.47890466],\n",
              "       [0.48380595],\n",
              "       [0.49999034],\n",
              "       [0.47614962],\n",
              "       [0.51162934],\n",
              "       [0.5011924 ],\n",
              "       [0.5278441 ],\n",
              "       [0.539327  ],\n",
              "       [0.4875705 ],\n",
              "       [0.5640831 ],\n",
              "       [0.50629103],\n",
              "       [0.48040214],\n",
              "       [0.5184918 ],\n",
              "       [0.52165425],\n",
              "       [0.50833404],\n",
              "       [0.51469076],\n",
              "       [0.46887332],\n",
              "       [0.5061791 ],\n",
              "       [0.47625947],\n",
              "       [0.548028  ],\n",
              "       [0.47382557],\n",
              "       [0.47935188],\n",
              "       [0.50587744],\n",
              "       [0.4326314 ],\n",
              "       [0.4478628 ],\n",
              "       [0.46500868],\n",
              "       [0.44853905],\n",
              "       [0.47603482],\n",
              "       [0.4644961 ],\n",
              "       [0.5023848 ],\n",
              "       [0.5189267 ],\n",
              "       [0.5042836 ],\n",
              "       [0.511239  ],\n",
              "       [0.4391733 ],\n",
              "       [0.5130458 ],\n",
              "       [0.469251  ],\n",
              "       [0.41829696],\n",
              "       [0.46324462],\n",
              "       [0.4052632 ],\n",
              "       [0.41996473],\n",
              "       [0.41477752],\n",
              "       [0.4129193 ],\n",
              "       [0.42499754],\n",
              "       [0.47236103],\n",
              "       [0.47061515],\n",
              "       [0.5046241 ],\n",
              "       [0.43216324],\n",
              "       [0.49535352],\n",
              "       [0.4613232 ],\n",
              "       [0.44820553],\n",
              "       [0.5118057 ],\n",
              "       [0.509043  ],\n",
              "       [0.49040186],\n",
              "       [0.43171608],\n",
              "       [0.42400324],\n",
              "       [0.4747743 ],\n",
              "       [0.42430556],\n",
              "       [0.44111302],\n",
              "       [0.4359442 ],\n",
              "       [0.43055546],\n",
              "       [0.50220555],\n",
              "       [0.51203173],\n",
              "       [0.4877808 ],\n",
              "       [0.47880712],\n",
              "       [0.4257512 ],\n",
              "       [0.5083858 ],\n",
              "       [0.44327673],\n",
              "       [0.48032796],\n",
              "       [0.4831094 ],\n",
              "       [0.48739132],\n",
              "       [0.5145298 ],\n",
              "       [0.42447197],\n",
              "       [0.46970126],\n",
              "       [0.50165915],\n",
              "       [0.414088  ],\n",
              "       [0.47773454],\n",
              "       [0.44020557],\n",
              "       [0.42212212],\n",
              "       [0.5103651 ],\n",
              "       [0.44916356],\n",
              "       [0.4729231 ],\n",
              "       [0.46254963],\n",
              "       [0.39785182],\n",
              "       [0.48194647],\n",
              "       [0.4303987 ],\n",
              "       [0.42474723],\n",
              "       [0.4531993 ],\n",
              "       [0.46459532],\n",
              "       [0.48048025],\n",
              "       [0.40898538],\n",
              "       [0.4494243 ],\n",
              "       [0.4570004 ],\n",
              "       [0.40754426],\n",
              "       [0.44853753],\n",
              "       [0.43183503],\n",
              "       [0.37690598],\n",
              "       [0.4145599 ],\n",
              "       [0.40958577],\n",
              "       [0.41043743],\n",
              "       [0.44239843],\n",
              "       [0.39840332],\n",
              "       [0.4705405 ],\n",
              "       [0.4553632 ],\n",
              "       [0.42459017],\n",
              "       [0.46286997],\n",
              "       [0.44890147],\n",
              "       [0.46323884],\n",
              "       [0.45364386],\n",
              "       [0.45954224],\n",
              "       [0.4699266 ],\n",
              "       [0.45811483],\n",
              "       [0.41524765],\n",
              "       [0.43134773],\n",
              "       [0.42843252],\n",
              "       [0.46455   ],\n",
              "       [0.43788135],\n",
              "       [0.47981644],\n",
              "       [0.4477476 ],\n",
              "       [0.47481507],\n",
              "       [0.48035648],\n",
              "       [0.46816754],\n",
              "       [0.4475217 ],\n",
              "       [0.4509954 ],\n",
              "       [0.46429837],\n",
              "       [0.47047073],\n",
              "       [0.46176502],\n",
              "       [0.4634427 ],\n",
              "       [0.4946766 ],\n",
              "       [0.46162227],\n",
              "       [0.40318638],\n",
              "       [0.47435862],\n",
              "       [0.51038   ],\n",
              "       [0.4739139 ],\n",
              "       [0.46767557],\n",
              "       [0.49420825],\n",
              "       [0.44777083],\n",
              "       [0.46735236],\n",
              "       [0.44472012],\n",
              "       [0.44027412],\n",
              "       [0.4175495 ],\n",
              "       [0.39641085],\n",
              "       [0.46201444],\n",
              "       [0.41921702],\n",
              "       [0.45297903],\n",
              "       [0.4868019 ],\n",
              "       [0.39444003],\n",
              "       [0.43786305],\n",
              "       [0.44608548],\n",
              "       [0.47423214],\n",
              "       [0.47458807],\n",
              "       [0.4071982 ],\n",
              "       [0.4818026 ],\n",
              "       [0.4442066 ],\n",
              "       [0.43717203],\n",
              "       [0.4819438 ],\n",
              "       [0.35734585],\n",
              "       [0.4258217 ],\n",
              "       [0.4408118 ],\n",
              "       [0.4099563 ],\n",
              "       [0.42363015],\n",
              "       [0.41134492],\n",
              "       [0.4161293 ],\n",
              "       [0.40313873],\n",
              "       [0.40050092],\n",
              "       [0.43055886],\n",
              "       [0.3993435 ],\n",
              "       [0.40817082],\n",
              "       [0.4430823 ],\n",
              "       [0.4202834 ],\n",
              "       [0.4629374 ],\n",
              "       [0.432047  ],\n",
              "       [0.4577408 ],\n",
              "       [0.4136807 ],\n",
              "       [0.46024245],\n",
              "       [0.42979017],\n",
              "       [0.45067957],\n",
              "       [0.4049615 ],\n",
              "       [0.33714387],\n",
              "       [0.40043503],\n",
              "       [0.42847353],\n",
              "       [0.39011127],\n",
              "       [0.42964408],\n",
              "       [0.45895216],\n",
              "       [0.41334754],\n",
              "       [0.43249092],\n",
              "       [0.40516922],\n",
              "       [0.4714658 ],\n",
              "       [0.43636286],\n",
              "       [0.45144856],\n",
              "       [0.4402122 ],\n",
              "       [0.40862906],\n",
              "       [0.42947224],\n",
              "       [0.40873665],\n",
              "       [0.4260269 ],\n",
              "       [0.444938  ],\n",
              "       [0.44733503],\n",
              "       [0.40538266],\n",
              "       [0.41583198],\n",
              "       [0.3907553 ],\n",
              "       [0.42081153],\n",
              "       [0.43076825],\n",
              "       [0.42024368],\n",
              "       [0.3944985 ],\n",
              "       [0.45697483],\n",
              "       [0.42376605],\n",
              "       [0.41795403],\n",
              "       [0.4565965 ],\n",
              "       [0.44914192],\n",
              "       [0.512779  ],\n",
              "       [0.43648112],\n",
              "       [0.45449767],\n",
              "       [0.46891385],\n",
              "       [0.45627937],\n",
              "       [0.49378172],\n",
              "       [0.4477097 ],\n",
              "       [0.4461953 ],\n",
              "       [0.463108  ],\n",
              "       [0.41330272],\n",
              "       [0.41154996],\n",
              "       [0.44991273],\n",
              "       [0.46262997],\n",
              "       [0.47633207],\n",
              "       [0.3981961 ],\n",
              "       [0.47695425],\n",
              "       [0.44404885],\n",
              "       [0.47313362],\n",
              "       [0.505169  ],\n",
              "       [0.42694902],\n",
              "       [0.50822616],\n",
              "       [0.50433177],\n",
              "       [0.48023325],\n",
              "       [0.49503702],\n",
              "       [0.47408032],\n",
              "       [0.4562799 ],\n",
              "       [0.48489583],\n",
              "       [0.4553424 ],\n",
              "       [0.4436735 ],\n",
              "       [0.44164348],\n",
              "       [0.46817422],\n",
              "       [0.46620068],\n",
              "       [0.43073404],\n",
              "       [0.49816185],\n",
              "       [0.48813838],\n",
              "       [0.5091421 ],\n",
              "       [0.528683  ],\n",
              "       [0.48468128],\n",
              "       [0.46401927],\n",
              "       [0.4845292 ],\n",
              "       [0.47595477],\n",
              "       [0.50186366],\n",
              "       [0.49653554],\n",
              "       [0.48732823],\n",
              "       [0.4646142 ],\n",
              "       [0.46917787],\n",
              "       [0.457601  ],\n",
              "       [0.48713535],\n",
              "       [0.45756772],\n",
              "       [0.4418791 ],\n",
              "       [0.44122007],\n",
              "       [0.47517115],\n",
              "       [0.4787529 ],\n",
              "       [0.49211732],\n",
              "       [0.42836243],\n",
              "       [0.44950983],\n",
              "       [0.47978172],\n",
              "       [0.4792688 ],\n",
              "       [0.47484902],\n",
              "       [0.43215537],\n",
              "       [0.4491043 ],\n",
              "       [0.46211022],\n",
              "       [0.4367572 ],\n",
              "       [0.45273152],\n",
              "       [0.4674652 ],\n",
              "       [0.4151019 ],\n",
              "       [0.44742614],\n",
              "       [0.42243147],\n",
              "       [0.49633807],\n",
              "       [0.4677034 ],\n",
              "       [0.45268255],\n",
              "       [0.49461463],\n",
              "       [0.44439593],\n",
              "       [0.49498135],\n",
              "       [0.46698993],\n",
              "       [0.4521623 ],\n",
              "       [0.4620621 ],\n",
              "       [0.4937976 ],\n",
              "       [0.4148729 ],\n",
              "       [0.45029607],\n",
              "       [0.4425791 ],\n",
              "       [0.4646111 ],\n",
              "       [0.4467281 ],\n",
              "       [0.4637235 ],\n",
              "       [0.46657142],\n",
              "       [0.4464511 ],\n",
              "       [0.47619316],\n",
              "       [0.47708756],\n",
              "       [0.49111915],\n",
              "       [0.49172455],\n",
              "       [0.4913707 ],\n",
              "       [0.47445065],\n",
              "       [0.4591403 ],\n",
              "       [0.49523264],\n",
              "       [0.48014942],\n",
              "       [0.47227055],\n",
              "       [0.48679608],\n",
              "       [0.4487427 ],\n",
              "       [0.46350035],\n",
              "       [0.43863875],\n",
              "       [0.446297  ],\n",
              "       [0.4786869 ],\n",
              "       [0.48005694],\n",
              "       [0.45217398],\n",
              "       [0.45894325],\n",
              "       [0.46521956],\n",
              "       [0.50703233],\n",
              "       [0.44194233],\n",
              "       [0.42576227],\n",
              "       [0.47399357],\n",
              "       [0.47578886],\n",
              "       [0.5062883 ],\n",
              "       [0.48750407],\n",
              "       [0.50763226],\n",
              "       [0.5164177 ],\n",
              "       [0.46456617],\n",
              "       [0.5079418 ],\n",
              "       [0.52974033],\n",
              "       [0.49008876],\n",
              "       [0.5024306 ],\n",
              "       [0.46575674],\n",
              "       [0.4766922 ],\n",
              "       [0.5092448 ],\n",
              "       [0.5261855 ],\n",
              "       [0.5223291 ],\n",
              "       [0.46813148],\n",
              "       [0.5153701 ],\n",
              "       [0.50076187],\n",
              "       [0.49760884],\n",
              "       [0.51695037],\n",
              "       [0.5216666 ],\n",
              "       [0.5571528 ],\n",
              "       [0.5421348 ],\n",
              "       [0.5046971 ],\n",
              "       [0.55404305],\n",
              "       [0.5171986 ],\n",
              "       [0.5153663 ],\n",
              "       [0.5178579 ],\n",
              "       [0.5084825 ],\n",
              "       [0.505268  ],\n",
              "       [0.48286307],\n",
              "       [0.516789  ],\n",
              "       [0.50934774],\n",
              "       [0.46754366],\n",
              "       [0.52510935],\n",
              "       [0.5050749 ],\n",
              "       [0.47605884],\n",
              "       [0.5031252 ],\n",
              "       [0.47689414],\n",
              "       [0.5127251 ],\n",
              "       [0.4866935 ],\n",
              "       [0.51080835],\n",
              "       [0.54568744],\n",
              "       [0.5096881 ],\n",
              "       [0.5075024 ],\n",
              "       [0.461792  ],\n",
              "       [0.4446457 ],\n",
              "       [0.50708044],\n",
              "       [0.45782745],\n",
              "       [0.44620913],\n",
              "       [0.48926505],\n",
              "       [0.40801   ],\n",
              "       [0.5124    ],\n",
              "       [0.44319943],\n",
              "       [0.3961781 ],\n",
              "       [0.4561996 ],\n",
              "       [0.4487093 ],\n",
              "       [0.46130815],\n",
              "       [0.48380244],\n",
              "       [0.49442375],\n",
              "       [0.48507148],\n",
              "       [0.48920056],\n",
              "       [0.46895182],\n",
              "       [0.49322826],\n",
              "       [0.45691946],\n",
              "       [0.49037927],\n",
              "       [0.3765013 ],\n",
              "       [0.4563841 ],\n",
              "       [0.42873836],\n",
              "       [0.4767853 ],\n",
              "       [0.4502691 ],\n",
              "       [0.45796457],\n",
              "       [0.46802968],\n",
              "       [0.4456303 ],\n",
              "       [0.47901136],\n",
              "       [0.44233507],\n",
              "       [0.4410889 ],\n",
              "       [0.49765837],\n",
              "       [0.44149864],\n",
              "       [0.4473293 ],\n",
              "       [0.44873106],\n",
              "       [0.47274956],\n",
              "       [0.4427969 ],\n",
              "       [0.3979017 ],\n",
              "       [0.4233055 ],\n",
              "       [0.47297978],\n",
              "       [0.38262057],\n",
              "       [0.51152533],\n",
              "       [0.44173393],\n",
              "       [0.4688802 ],\n",
              "       [0.52384895],\n",
              "       [0.457754  ],\n",
              "       [0.47484416],\n",
              "       [0.45033562],\n",
              "       [0.40627718],\n",
              "       [0.47056317]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fEfMMBwvPNa",
        "outputId": "1eb4a9ba-4268-4dcc-90f4-aca66287174d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(468, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJhQJWfNvPNa",
        "outputId": "2bc7ba87-53a0-4a13-e366-0d9067516970"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(468,)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "x_test[:,1][win_len:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_RkBnzZgvPNb",
        "outputId": "a4d5c883-f7e7-4c2f-b3d6-0a1c618ab119"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pred  Actual\n",
              "0   0.0     1.0\n",
              "1   1.0     1.0\n",
              "2   0.0     0.0\n",
              "3   1.0     1.0\n",
              "4   1.0     0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e5c38e2-22c0-4d9e-ac18-9bf8383d074c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e5c38e2-22c0-4d9e-ac18-9bf8383d074c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e5c38e2-22c0-4d9e-ac18-9bf8383d074c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e5c38e2-22c0-4d9e-ac18-9bf8383d074c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "df_pred = pd.concat([pd.DataFrame(np.round(predictions)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NVr4vWEhvPNb"
      },
      "outputs": [],
      "source": [
        "#function to print accuracy and MCC of the model\n",
        "def evaluation(df : pd.DataFrame):\n",
        "    conf = pd.crosstab(df[\"Actual\"], df[\"Pred\"])\n",
        "    fig = plt.figure(figsize = (6,4))\n",
        "    sn.heatmap(conf, annot = True, cmap = \"Blues\", fmt = \"g\")\n",
        "    plt.title(\"Confussion Matrix\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    TP = conf[1][1]\n",
        "    FN = conf[0][1]\n",
        "    FP = conf[1][0]\n",
        "    TN = conf[0][0]\n",
        "    Acc = (TP+TN)/(TP+TN+FN+FP)\n",
        "    Mcc = (TP*TN - FP*FN) / np.sqrt( (TP + FP)*(TP + FN)*(TN + FP)*(TN + FN) )\n",
        "    print(\"Accuracy =\",Acc)\n",
        "    print(\"MCC =\",Mcc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "4QNaOzY7vPNc",
        "outputId": "0f8ff4c9-4698-44a4-af07-a5074d3bf0cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.5213675213675214\n",
            "MCC = 0.07524606050562324\n"
          ]
        }
      ],
      "source": [
        "evaluation(df_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7UvOYGxuzAag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h3>Model 2"
      ],
      "metadata": {
        "id": "mUgTlzy10QFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (win_len, num_features)\n",
        "\n",
        "model2 = build_model(input_shape, head_size=512, num_heads=6, ff_dim=4, num_transformer_blocks=4, mlp_units=[256], \n",
        "                    mlp_dropout=0.4,dropout=0.25)"
      ],
      "metadata": {
        "id": "71grCVCf0Spb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m4Q18_i06QE",
        "outputId": "2392167c-0c84-422d-ea82-de71badf0afb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 30, 12)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 30, 12)      24          ['input_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 30, 12)      156684      ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_4[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 30, 12)      0           ['dropout_13[0][0]',             \n",
            " mbda)                                                            'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 30, 12)      24          ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 30, 4)        52          ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 30, 4)        0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 30, 12)       60          ['dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 30, 12)      0           ['conv1d_9[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_9[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 30, 12)      156684      ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 30, 12)      0           ['dropout_15[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 30, 4)        0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 30, 12)       60          ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 30, 12)      0           ['conv1d_11[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_11[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 30, 12)      156684      ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 30, 12)      0           ['dropout_17[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 30, 4)        0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 30, 12)       60          ['dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 30, 12)      0           ['conv1d_13[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 30, 12)      156684      ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 30, 12)       0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 30, 12)      0           ['dropout_19[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 30, 12)      24          ['tf.__operators__.add_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 30, 4)        52          ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 30, 4)        0           ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 30, 12)       60          ['dropout_20[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 30, 12)      0           ['conv1d_15[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 30)          0           ['tf.__operators__.add_15[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          7936        ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 256)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            257         ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 635,569\n",
            "Trainable params: 635,569\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"clas_logs\"\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = \"min\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+\"\\model2.hdf5\", monitor = \"val_accuracy\", verbose = 1, save_best_only=True, mode = \"max\")\n",
        "\n",
        "model2.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "jpr3n4Yw09Wf"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model1.fit()\n",
        "history = model2.fit_generator(train_generator, epochs=500, validation_data=test_generator, shuffle=False, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzsgBOw51HrQ",
        "outputId": "65be77cf-4dcd-4324-a902-2d13aa0743de"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45/46 [============================>.] - ETA: 0s - loss: 2.4829 - accuracy: 0.5188\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51496, saving model to clas_logs\\model2.hdf5\n",
            "46/46 [==============================] - 6s 45ms/step - loss: 2.4767 - accuracy: 0.5202 - val_loss: 0.9136 - val_accuracy: 0.5150\n",
            "Epoch 2/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 4.5413 - accuracy: 0.4709\n",
            "Epoch 2: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 4.4825 - accuracy: 0.4723 - val_loss: 1.0786 - val_accuracy: 0.4850\n",
            "Epoch 3/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 3.5009 - accuracy: 0.4701\n",
            "Epoch 3: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 3.4617 - accuracy: 0.4709 - val_loss: 1.0380 - val_accuracy: 0.4850\n",
            "Epoch 4/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.4522 - accuracy: 0.4921\n",
            "Epoch 4: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 2.4522 - accuracy: 0.4921 - val_loss: 6.8155 - val_accuracy: 0.5150\n",
            "Epoch 5/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 4.0865 - accuracy: 0.5284\n",
            "Epoch 5: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 4.0865 - accuracy: 0.5284 - val_loss: 0.8654 - val_accuracy: 0.4850\n",
            "Epoch 6/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 4.9816 - accuracy: 0.4976\n",
            "Epoch 6: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 4.9816 - accuracy: 0.4976 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 7/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 7: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 8/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 8: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 9/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 9: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 10/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 10: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 11/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 11: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 12/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 12: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 13/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 13: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 14/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 14: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 15/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 15: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 16/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 16: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 17/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 17: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 18/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 18: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 19/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 19: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 20/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 20: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 21/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 21: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 22/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 22: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 23/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 23: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 24/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9858 - accuracy: 0.5414\n",
            "Epoch 24: val_accuracy did not improve from 0.51496\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9858 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 25/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.9914 - accuracy: 0.5376\n",
            "Epoch 25: val_accuracy improved from 0.51496 to 0.51923, saving model to clas_logs\\model2.hdf5\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 5.8888 - accuracy: 0.5387 - val_loss: 0.9628 - val_accuracy: 0.5192\n",
            "Epoch 26/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.9506 - accuracy: 0.5348\n",
            "Epoch 26: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 5.8444 - accuracy: 0.5380 - val_loss: 7.3094 - val_accuracy: 0.5150\n",
            "Epoch 27/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 5.8786 - accuracy: 0.5256\n",
            "Epoch 27: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 5.8337 - accuracy: 0.5311 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 28/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0511 - accuracy: 0.5376\n",
            "Epoch 28: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9727 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 29/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9844 - accuracy: 0.5414\n",
            "Epoch 29: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9844 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 30/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9062 - accuracy: 0.5387\n",
            "Epoch 30: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9062 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 31/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0207 - accuracy: 0.5299\n",
            "Epoch 31: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9928 - accuracy: 0.5318 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 32/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 32: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 33/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0742 - accuracy: 0.5355\n",
            "Epoch 33: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9950 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 34/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 34: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 35/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0727 - accuracy: 0.5362\n",
            "Epoch 35: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9936 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 36/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9829 - accuracy: 0.5421\n",
            "Epoch 36: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 6.9829 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 37/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9932 - accuracy: 0.5414\n",
            "Epoch 37: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 38/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0010 - accuracy: 0.5403\n",
            "Epoch 38: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9735 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 39/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 39: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 40/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0618 - accuracy: 0.5369\n",
            "Epoch 40: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9831 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 41/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 41: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 42/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0109 - accuracy: 0.5396\n",
            "Epoch 42: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9832 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 43/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0728 - accuracy: 0.5355\n",
            "Epoch 43: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9939 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 44/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 44: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 45/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 45: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 46/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0724 - accuracy: 0.5362\n",
            "Epoch 46: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9933 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 47/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9934 - accuracy: 0.5414\n",
            "Epoch 47: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9934 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 48/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9935 - accuracy: 0.5414\n",
            "Epoch 48: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9935 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 49/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0622 - accuracy: 0.5362\n",
            "Epoch 49: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9834 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 50/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0832 - accuracy: 0.5355\n",
            "Epoch 50: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 7.0037 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 51/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 51: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 52/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 52: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9932 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 53/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0629 - accuracy: 0.5369\n",
            "Epoch 53: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9841 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 54/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0199 - accuracy: 0.5355\n",
            "Epoch 54: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9410 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 55/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.6736 - accuracy: 0.5319\n",
            "Epoch 55: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.6508 - accuracy: 0.5339 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 56/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.3739 - accuracy: 0.5359\n",
            "Epoch 56: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.3739 - accuracy: 0.5359 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 57/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9604 - accuracy: 0.5354\n",
            "Epoch 57: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9334 - accuracy: 0.5373 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 58/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0588 - accuracy: 0.5334\n",
            "Epoch 58: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9802 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 59/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9256 - accuracy: 0.5448\n",
            "Epoch 59: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9256 - accuracy: 0.5448 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 60/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0065 - accuracy: 0.5368\n",
            "Epoch 60: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9788 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 61/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9595 - accuracy: 0.5382\n",
            "Epoch 61: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9325 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 62/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.5118 - accuracy: 0.5410\n",
            "Epoch 62: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.4913 - accuracy: 0.5428 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 63/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 6.8937 - accuracy: 0.5284\n",
            "Epoch 63: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.8211 - accuracy: 0.5339 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 64/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.4110 - accuracy: 0.5028\n",
            "Epoch 64: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 7.3099 - accuracy: 0.5099 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 65/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1106 - accuracy: 0.5071\n",
            "Epoch 65: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 7.0996 - accuracy: 0.5072 - val_loss: 6.3712 - val_accuracy: 0.5128\n",
            "Epoch 66/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9952 - accuracy: 0.5414\n",
            "Epoch 66: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 6.9952 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 67/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.1198 - accuracy: 0.5319\n",
            "Epoch 67: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 7.0905 - accuracy: 0.5339 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 68/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0577 - accuracy: 0.5368\n",
            "Epoch 68: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0293 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 69/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1075 - accuracy: 0.5334\n",
            "Epoch 69: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 7.0271 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 70/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0337 - accuracy: 0.5382\n",
            "Epoch 70: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0057 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 71/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1059 - accuracy: 0.5348\n",
            "Epoch 71: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0256 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 72/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0658 - accuracy: 0.5368\n",
            "Epoch 72: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0373 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 73/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0858 - accuracy: 0.5354\n",
            "Epoch 73: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0570 - accuracy: 0.5373 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 74/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.1043 - accuracy: 0.5333\n",
            "Epoch 74: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0753 - accuracy: 0.5352 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 75/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 6.9202 - accuracy: 0.5437\n",
            "Epoch 75: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.8938 - accuracy: 0.5455 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 76/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0798 - accuracy: 0.5334\n",
            "Epoch 76: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0004 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 77/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.1012 - accuracy: 0.5346\n",
            "Epoch 77: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.1012 - accuracy: 0.5346 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 78/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0370 - accuracy: 0.5387\n",
            "Epoch 78: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0370 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 79/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0652 - accuracy: 0.5375\n",
            "Epoch 79: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0367 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 80/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1285 - accuracy: 0.5334\n",
            "Epoch 80: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 7.0473 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 81/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.2138 - accuracy: 0.5256\n",
            "Epoch 81: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.1296 - accuracy: 0.5311 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 82/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0820 - accuracy: 0.5327\n",
            "Epoch 82: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 7.0025 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 83/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0758 - accuracy: 0.5368\n",
            "Epoch 83: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0472 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 84/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.0484 - accuracy: 0.5380\n",
            "Epoch 84: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0484 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 85/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0464 - accuracy: 0.5368\n",
            "Epoch 85: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0181 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 86/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1629 - accuracy: 0.5298\n",
            "Epoch 86: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0805 - accuracy: 0.5352 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 87/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1418 - accuracy: 0.5320\n",
            "Epoch 87: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0602 - accuracy: 0.5373 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 88/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0556 - accuracy: 0.5375\n",
            "Epoch 88: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0273 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 89/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0553 - accuracy: 0.5375\n",
            "Epoch 89: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0269 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 90/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0569 - accuracy: 0.5375\n",
            "Epoch 90: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 7.0286 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 91/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0356 - accuracy: 0.5389\n",
            "Epoch 91: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 7.0076 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 92/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0249 - accuracy: 0.5396\n",
            "Epoch 92: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9970 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 93/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0569 - accuracy: 0.5375\n",
            "Epoch 93: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0286 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 94/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0462 - accuracy: 0.5382\n",
            "Epoch 94: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 7.0180 - accuracy: 0.5400 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 95/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0358 - accuracy: 0.5389\n",
            "Epoch 95: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0078 - accuracy: 0.5407 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 96/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.1273 - accuracy: 0.5334\n",
            "Epoch 96: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0462 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 97/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0750 - accuracy: 0.5368\n",
            "Epoch 97: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0463 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 98/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0642 - accuracy: 0.5375\n",
            "Epoch 98: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0358 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 99/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0644 - accuracy: 0.5375\n",
            "Epoch 99: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0359 - accuracy: 0.5394 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 100/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0880 - accuracy: 0.5361\n",
            "Epoch 100: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0592 - accuracy: 0.5380 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 101/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 7.1564 - accuracy: 0.5318\n",
            "Epoch 101: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.1564 - accuracy: 0.5318 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 102/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.2464 - accuracy: 0.5243\n",
            "Epoch 102: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.2153 - accuracy: 0.5264 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 103/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0645 - accuracy: 0.5368\n",
            "Epoch 103: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 7.0360 - accuracy: 0.5387 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 104/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0420 - accuracy: 0.5369\n",
            "Epoch 104: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9640 - accuracy: 0.5421 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 105/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 105: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 106/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 106: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 107/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 107: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 108/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 108: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 109/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 109: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 110/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 110: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 111/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 111: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 112/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 112: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 113/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 113: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 114/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 114: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 115/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 115: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 116/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 116: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 117/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 117: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 118/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 118: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 119/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 119: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 120/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 120: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 121/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 121: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 122/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 122: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 123/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 123: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 124/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 124: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 125/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 125: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 126/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 126: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 127/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 127: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 128/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 128: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 129/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 129: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 130/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 130: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 131/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 131: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 132/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 132: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 133/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 133: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 134/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 134: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 135/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 135: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 136/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 136: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 137/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 137: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 138/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 138: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 139/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 139: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 140/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 140: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 141/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 141: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 142/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 142: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 143/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 143: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 144/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 144: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 145/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 145: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 146/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 146: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 147/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 147: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 148/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 148: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 149/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 149: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 150/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 150: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 151/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 151: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 152/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 152: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 153/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 153: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 154/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 154: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 155/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 155: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 156/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 156: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 157/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 157: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 158/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 158: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 159/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 159: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 160/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 160: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 161/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 161: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 162/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 162: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 163/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 163: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 164/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 164: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 165/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 165: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 166/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 166: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 167/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 167: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 168/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 168: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 169/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 169: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 170/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 170: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 171/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 171: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 172/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 172: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 173/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 173: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 174/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 174: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 175/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 175: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 176/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 176: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 177/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 177: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 178/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 178: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 179/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 179: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 180/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 180: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 181/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 181: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 182/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 182: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 183/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 183: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 184/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 184: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 185/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 185: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 186/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 186: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 187/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 187: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 188/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 188: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 189/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 189: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 190/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 190: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 191/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 191: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 192/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 192: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 193/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 193: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 194/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 194: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 195/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 195: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 196/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 196: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 197/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 197: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 198/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 198: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 199/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 199: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 200/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 200: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 201/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 201: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 202/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 202: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 203/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 203: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 204/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 204: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 205/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 205: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 206/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 206: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 207/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 207: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 208/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 208: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 209/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 209: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 210/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 210: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 211/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 211: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 212/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 212: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 213/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 213: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 214/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 214: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 215/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 215: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 216/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 216: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 217/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 217: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 218/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 218: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 219/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 219: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 220/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 220: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 221/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 221: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 222/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 222: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 223/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 223: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 224/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 224: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 225/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 225: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 226/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 226: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 227/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 227: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 228/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 228: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 229/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 229: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 230/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 230: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 231/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 231: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 232/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 232: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 233/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 233: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 234/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 234: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 235/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 235: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 236/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 236: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 237/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 237: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 238/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 238: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 239/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 239: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 240/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 240: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 241/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 241: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 242/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 242: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 243/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 243: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 244/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 244: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 245/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 245: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 246/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 246: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 247/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 247: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 248/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 248: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 249/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 249: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 250/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 250: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 251/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 251: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 252/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 252: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 253/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 253: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 254/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 254: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 255/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 255: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 256/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 256: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 257/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 257: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 258/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 258: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 259/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 259: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 260/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 260: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 261/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 261: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 262/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 262: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 263/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 263: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 264/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 264: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 265/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 265: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 266/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 266: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 267/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 267: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 268/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 268: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 269/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 269: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 270/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 270: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 271/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 271: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 272/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 272: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 273/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 273: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 274/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 274: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 275/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 275: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 276/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 276: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 277/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 277: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 278/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 278: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 279/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 279: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 280/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 280: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 281/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 281: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 282/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 282: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 283/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 283: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 284/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 284: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 285/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 285: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 286/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 286: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 287/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 287: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 288/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 288: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 289/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 289: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 290/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 290: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 291/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 291: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 292/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 292: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 293/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 293: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 294/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 294: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 295/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 295: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 296/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 296: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 297/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 297: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 298/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 298: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 299/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 299: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 300/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 300: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 301/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 301: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 302/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 302: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 303/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 303: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 304/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 304: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 305/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 305: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 306/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 306: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 307/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 307: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 308/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 308: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 309/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 309: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 310/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 310: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 311/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 311: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 312/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 312: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 313/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 313: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 314/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 314: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 315/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 315: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 316/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 316: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 317/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 317: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 318/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 318: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 319/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 319: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 320/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 320: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 321/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 321: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 322/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 322: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 323/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 323: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 324/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 324: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 325/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 325: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 326/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 326: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 327/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 327: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 328/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 328: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 329/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 329: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 330/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 330: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 331/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 331: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 332/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 332: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 333/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 333: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 334/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 334: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 335/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 335: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 336/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 336: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 337/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 337: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 338/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 338: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 339/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 339: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 340/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 340: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 341/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 341: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 342/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 342: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 343/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 343: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 344/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 344: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 345/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 345: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 346/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 346: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 347/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 347: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 348/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 348: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 349/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 349: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 350/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 350: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 351/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 351: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 352/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 352: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 353/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 353: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 354/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 354: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 355/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 355: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 356/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 356: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 357/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 357: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 358/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 358: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 359/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 359: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 360/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 360: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 361/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 361: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 362/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 362: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 363/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 363: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 364/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 364: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 365/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 365: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 366/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 366: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 367/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 367: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 368/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 368: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 369/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 369: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 370/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 370: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 371/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 371: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 372/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 372: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 373/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 373: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 374/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 374: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 375/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 375: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 376/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 376: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 377/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 377: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 378/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 378: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 379/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 379: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 380/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 380: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 381/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 381: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 382/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 382: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 383/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 383: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 384/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 384: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 385/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 385: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 386/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 386: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 387/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 387: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 388/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 388: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 389/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 389: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 390/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 390: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 391/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 391: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 392/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 392: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 393/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 393: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 394/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 394: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 395/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 395: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 396/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 396: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 397/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 397: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 398/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 398: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 399/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 399: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 400/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 400: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 401/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 401: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 402/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 402: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 403/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 403: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 404/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 404: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 405/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 405: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 406/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 406: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 407/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 407: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 408/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 408: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 409/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 409: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 410/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 410: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 411/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 411: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 412/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 412: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 413/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 413: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 414/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 414: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 415/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 415: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 37ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 416/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 416: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 417/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 417: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 418/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 418: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 419/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 419: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 420/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 420: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 421/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 421: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 422/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 422: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 423/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 423: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 424/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 424: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 425/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 425: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 426/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 426: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 427/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 427: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 428/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 428: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 429/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 429: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 430/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 430: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 431/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 431: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 432/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 432: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 35ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 433/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 433: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 434/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 434: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 435/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 435: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 436/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 436: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 437/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 437: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 438/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 438: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 439/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 439: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 440/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 440: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 441/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 441: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 442/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 442: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 443/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 443: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 444/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 444: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 445/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 445: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 446/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 446: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 447/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 447: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 448/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 448: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 449/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 449: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 450/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 450: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 451/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 451: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 452/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 452: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 453/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 453: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 454/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 454: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 455/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 455: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 456/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 456: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 457/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 457: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 458/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 458: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 459/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 459: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 460/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 460: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 461/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 461: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 462/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 462: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 463/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 463: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 464/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 464: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 465/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 465: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 466/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 466: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 467/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 467: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 468/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 468: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 469/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 469: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 470/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 470: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 471/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 471: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 472/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 472: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 473/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 473: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 474/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 474: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 475/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 475: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 476/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 476: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 477/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 477: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 478/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 478: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 479/500\n",
            "44/46 [===========================>..] - ETA: 0s - loss: 7.0723 - accuracy: 0.5362\n",
            "Epoch 479: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 480/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 480: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 481/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 481: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 482/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 482: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 483/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 483: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 484/500\n",
            "46/46 [==============================] - ETA: 0s - loss: 6.9931 - accuracy: 0.5414\n",
            "Epoch 484: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 485/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 485: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 486/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 486: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 487/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 487: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 488/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 488: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 489/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 489: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 490/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 490: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 491/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 491: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 492/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 492: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 493/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 493: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 494/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 494: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 495/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 495: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 496/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 496: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 497/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 497: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 498/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 498: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 499/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 499: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 2s 36ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n",
            "Epoch 500/500\n",
            "45/46 [============================>.] - ETA: 0s - loss: 7.0210 - accuracy: 0.5396\n",
            "Epoch 500: val_accuracy did not improve from 0.51923\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 6.9931 - accuracy: 0.5414 - val_loss: 7.3965 - val_accuracy: 0.5150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotHist(history, \"loss\")"
      ],
      "metadata": {
        "id": "CHoi2w-p1LAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotHist(history, \"accuracy\")"
      ],
      "metadata": {
        "id": "-wWalIzW1QZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.models.load_model(\"clas_logs/model2.hdf5\")"
      ],
      "metadata": {
        "id": "E7uOWF8J1Sfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model2.predict(test_generator)"
      ],
      "metadata": {
        "id": "8h42jMki1WTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "WzrzTL3u1Z4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "id": "Qgz38c8Q1cfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[:,1][win_len:].shape"
      ],
      "metadata": {
        "id": "2_SVkuy01eFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred = pd.concat([pd.DataFrame(np.round(predictions)), pd.DataFrame(x_test[:,1][win_len:])], axis = 1)\n",
        "df_pred.columns = [\"Pred\", \"Actual\"]\n",
        "df_pred.head()"
      ],
      "metadata": {
        "id": "dIVWSlyV1hXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(df_pred)"
      ],
      "metadata": {
        "id": "7-E88MzM1laI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e4b444f792fbff6b064e16dd46d58b2c8126eac8e1e261c7b4c56b3fda4323e3"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}